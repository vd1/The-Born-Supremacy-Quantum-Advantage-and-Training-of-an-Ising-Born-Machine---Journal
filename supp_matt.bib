@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in Applied Probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}

@article{kubler_quantum_2019,
	title = {Quantum {Mean} {Embedding} of {Probability} {Distributions}},
	url = {http://arxiv.org/abs/1905.13526},
	abstract = {The kernel mean embedding of probability distributions is commonly used in machine learning as an injective mapping from distributions to functions in an infinite dimensional Hilbert space. It allows us, for example, to define a distance measure between probability distributions, called maximum mean discrepancy (MMD). In this work, we propose to represent probability distributions in a pure quantum state of a system that is described by an infinite dimensional Hilbert space. This enables us to work with an explicit representation of the mean embedding, whereas classically one can only work implicitly with an infinite dimensional Hilbert space through the use of the kernel trick. We show how this explicit representation can speed up methods that rely on inner products of mean embeddings and discuss the theoretical and experimental challenges that need to be solved in order to achieve these speedups.},
	urldate = {2019-07-30},
	journal = {arXiv:1905.13526 [quant-ph]},
	author = {Kübler, Jonas M. and Muandet, Krikamol and Schölkopf, Bernhard},
	month = may,
	year = {2019},
	note = {arXiv: 1905.13526},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {Comment: 7 pages, 2 figures},
	file = {arXiv\:1905.13526 PDF:/home/brian/Zotero/storage/FIKEELVQ/Kübler et al. - 2019 - Quantum Mean Embedding of Probability Distribution.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/UUHQYQPM/1905.html:text/html}
}

@article{lloyd_quantum_1999,
	title = {Quantum {Computation} over {Continuous} {Variables}},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1784},
	doi = {10.1103/PhysRevLett.82.1784},
	abstract = {This paper provides necessary and sufficient conditions for constructing a universal quantum computer over continuous variables. As an example, it is shown how a universal quantum computer for the amplitudes of the electromagnetic field might be constructed using simple linear devices such as beam splitters and phase shifters, together with squeezers and nonlinear devices such as Kerr-effect fibers and atoms in optical cavities. Such a device could in principle perform “quantum floating point” computations. Problems involving noise, finite precision, and error correction are discussed.},
	number = {8},
	urldate = {2018-07-09},
	journal = {Physical Review Letters},
	author = {Lloyd, Seth and Braunstein, Samuel L.},
	month = feb,
	year = {1999},
	pages = {1784--1787},
	file = {APS Snapshot:/home/brian/Zotero/storage/6KDCVYKK/PhysRevLett.82.html:text/html}
}

@article{bellemare_cramer_2017,
	title = {The {Cramer} {Distance} as a {Solution} to {Biased} {Wasserstein} {Gradients}},
	url = {http://arxiv.org/abs/1705.10743},
	abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram{\textbackslash}textbackslash'er distance. We show that the Cram{\textbackslash}textbackslash'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram{\textbackslash}textbackslash'er distance in practice we design a new algorithm, the Cram{\textbackslash}textbackslash'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
	urldate = {2018-07-06},
	journal = {arXiv:1705.10743 [cs, stat]},
	author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
	month = may,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1705.10743},
	file = {arXiv\:1705.10743 PDF:/home/brian/Zotero/storage/T75MVE97/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/T78RAA3R/1705.html:text/html}
}

@book{bishop_pattern_2006,
	address = {Berlin, Heidelberg},
	title = {Pattern {Recognition} and {Machine} {Learning} ({Information} {Science} and {Statistics})},
	isbn = {0-387-31073-8},
	publisher = {Springer-Verlag},
	author = {Bishop, Christopher M.},
	year = {2006}
}

@book{mackay_information_2002,
	address = {New York, NY, USA},
	title = {Information {Theory}, {Inference} \& {Learning} {Algorithms}},
	isbn = {0-521-64298-1},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	year = {2002}
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	url = {https://doi.org/10.1214/aoms/1177729694},
	doi = {10.1214/aoms/1177729694},
	number = {1},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	year = {1951},
	pages = {79--86}
}

@article{chen_equivalence_2018,
	title = {Equivalence of restricted {Boltzmann} machines and tensor network states},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104},
	doi = {10.1103/PhysRevB.97.085104},
	number = {8},
	journal = {Phys. Rev. B},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	month = feb,
	year = {2018},
	pages = {085104}
}

@article{han_unsupervised_2017,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	url = {http://arxiv.org/abs/1709.01662},
	abstract = {Generative modeling, which learns joint probability distribution from training data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning by utilizing the density matrix renormalization group method which allows dynamic adjusting dimensions of the tensors, and offers an efficient direct sampling approach, Zipper, for generative tasks. We apply our method to generative modeling of several standard datasets including the principled Bars and Stripes, random binary patterns and the MNIST handwritten digits, to illustrate ability of our model, and discuss features as well as drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work shed light on many interesting directions for future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which is of possibility of being realized by a quantum device.},
	language = {en},
	urldate = {2018-07-03},
	journal = {arXiv:1709.01662 [cond-mat, physics:quant-ph, stat]},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = sep,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics},
	annote = {arXiv: 1709.01662},
	annote = {Comment: 11 pages, 12 figures (not including the TNs) GitHub Page: https://github.com/congzlwag/UnsupGenModbyMPS},
	file = {Han et al. - 2017 - Unsupervised Generative Modeling Using Matrix Prod.pdf:/home/brian/Zotero/storage/4ZNQ23GG/Han et al. - 2017 - Unsupervised Generative Modeling Using Matrix Prod.pdf:application/pdf}
}

@article{huang_provably_2017,
	title = {Provably efficient neural network representation for image classification},
	url = {http://arxiv.org/abs/1711.04606},
	abstract = {The state-of-the-art approaches for image classification are based on neural networks. Mathematically, the task of classifying images is equivalent to finding the function that maps an image to the label it is associated with. To rigorously establish the success of neural network methods, we should first prove that the function has an efficient neural network representation, and then design provably efficient training algorithms to find such a representation. Here, we achieve the first goal based on a set of assumptions about the patterns in the images. The validity of these assumptions is very intuitive in many image classification problems, including but not limited to, recognizing handwritten digits.},
	urldate = {2018-07-03},
	journal = {arXiv:1711.04606 [cs]},
	author = {Huang, Yichen},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv: 1711.04606},
	file = {arXiv\:1711.04606 PDF:/home/brian/Zotero/storage/THX3KBPP/Huang - 2017 - Provably efficient neural network representation f.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/QJ8F369X/1711.html:text/html}
}

@article{gao_efficient_2017,
	title = {An efficient quantum algorithm for generative machine learning},
	url = {http://arxiv.org/abs/1711.02038},
	abstract = {A central task in the field of quantum computing is to find applications where quantum computer could provide exponential speedup over any classical computer. Machine learning represents an important field with broad applications where quantum computer may offer significant speedup. Several quantum algorithms for discriminative machine learning have been found based on efficient solving of linear algebraic problems, with potential exponential speedup in runtime under the assumption of effective input from a quantum random access memory. In machine learning, generative models represent another large class which is widely used for both supervised and unsupervised learning. Here, we propose an efficient quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is exponentially more powerful to represent probability distributions compared with classical generative models and has exponential speedup in training and inference at least for some instances under a reasonable assumption in computational complexity theory. Our result opens a new direction for quantum machine learning and offers a remarkable example in which a quantum algorithm shows exponential improvement over any classical algorithm in an important application field.},
	urldate = {2018-07-03},
	journal = {arXiv:1711.02038 [quant-ph, stat]},
	author = {Gao, Xun and Zhang, Zhengyu and Duan, Luming},
	month = nov,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1711.02038},
	annote = {Comment: 7+15 pages, 3+6 figures},
	file = {arXiv\:1711.02038 PDF:/home/brian/Zotero/storage/RU9QTVLI/Gao et al. - 2017 - An efficient quantum algorithm for generative mach.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/SZZJ9TAF/1711.html:text/html}
}

@article{pestun_tensor_2017,
	title = {Tensor network language model},
	url = {http://arxiv.org/abs/1710.10248},
	abstract = {We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.10248 [cond-mat, stat]},
	author = {Pestun, Vasily and Vlassopoulos, Yiannis},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {arXiv: 1710.10248},
	annote = {Comment: 21 pages},
	file = {arXiv\:1710.10248 PDF:/home/brian/Zotero/storage/AU6JEU82/Pestun and Vlassopoulos - 2017 - Tensor network language model.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/BUSELFBV/1710.html:text/html}
}

@article{zhang_entanglement_2017,
	title = {Entanglement {Entropy} of {Target} {Functions} for {Image} {Classification} and {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1710.05520},
	abstract = {The success of deep convolutional neural network (CNN) in computer vision especially image classification problems requests a new information theory for function of image, instead of image itself. In this article, after establishing a deep mathematical connection between image classification problem and quantum spin model, we propose to use entanglement entropy, a generalization of classical Boltzmann-Shannon entropy, as a powerful tool to characterize the information needed for representation of general function of image. We prove that there is a sub-volume-law bound for entanglement entropy of target functions of reasonable image classification problems. Therefore target functions of image classification only occupy a small subspace of the whole Hilbert space. As a result, a neural network with polynomial number of parameters is efficient for representation of such target functions of image. The concept of entanglement entropy can also be useful to characterize the expressive power of different neural networks. For example, we show that to maintain the same expressive power, number of channels \$D\$ in a convolutional neural network should scale with the number of convolution layers \$n\_c\$ as \$D{\textbackslash}textbackslashsim D\_0ˆ\{{\textbackslash}textbackslashfrac\{1\}\{n\_c\}\}\$. Therefore, deeper CNN with large \$n\_c\$ is more efficient than shallow ones.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.05520 [cond-mat]},
	author = {Zhang, Ya-Hui},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Strongly Correlated Electrons, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1710.05520},
	annote = {Comment: 9pages, 1 figures},
	file = {arXiv\:1710.05520 PDF:/home/brian/Zotero/storage/ACADXHH4/Zhang - 2017 - Entanglement Entropy of Target Functions for Image.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/8RPL9UTE/1710.html:text/html}
}

@article{liu_machine_2017,
	title = {Machine {Learning} by {Two}-{Dimensional} {Hierarchical} {Tensor} {Networks}: {A} {Quantum} {Information} {Theoretic} {Perspective} on {Deep} {Architectures}},
	shorttitle = {Machine {Learning} by {Two}-{Dimensional} {Hierarchical} {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1710.04833},
	abstract = {The resemblance between the tensor networks (TNs) and machine learning has drawn considerable attention. In particular, TNs and deep learning architectures bear striking similarities suggesting using quantum techniques for machine learning. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multipartite entanglement renormalization ansatz. This approach overcomes scalability issues and implies novel mathematical connections among quantum many-body physics, quantum information theory, and machine learning. The algorithm optimally encodes each image class into a TN state, so that the learning tasks as well as the image classes can be characterized by quantum properties of the state including quantum entanglement and fidelity. Furthermore, the unitary conditions of the local mappings in our algorithm make it possible to realize the machine learning by, e.g., quantum state tomography techniques or quantum computations.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.04833 [cond-mat, physics:physics, physics:quant-ph, stat]},
	author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and García, Raul Blázquez and Su, Gang and Lewenstein, Maciej},
	month = oct,
	year = {2017},
	keywords = {Quantum Physics, Statistics - Machine Learning, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics},
	annote = {arXiv: 1710.04833},
	annote = {Comment: 6 pages, 5 figures},
	file = {arXiv\:1710.04833 PDF:/home/brian/Zotero/storage/FX66Z2LN/Liu et al. - 2017 - Machine Learning by Two-Dimensional Hierarchical T.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/Y789JK5C/1710.html:text/html}
}

@article{levine_deep_2017,
	title = {Deep {Learning} and {Quantum} {Entanglement}: {Fundamental} {Connections} with {Implications} to {Network} {Design}},
	shorttitle = {Deep {Learning} and {Quantum} {Entanglement}},
	url = {http://arxiv.org/abs/1704.01552},
	abstract = {Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical observations regarding the role that the number of channels in each layer of the convolutional network fulfills in the overall inductive bias. Specifically, we show an equivalence between the function realized by a deep convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which relies on their common underlying tensorial structure. This facilitates the use of quantum entanglement measures as well-defined quantifiers of a deep network's expressive ability to model intricate correlation structures of its inputs. Most importantly, the construction of a deep ConvAC in terms of a Tensor Network is made available. This description enables us to carry a graph-theoretic analysis of a convolutional network, with which we demonstrate a direct control over the inductive bias of the deep network via its channel numbers, that are related to the min-cut in the underlying graph. This result is relevant to any practitioner designing a network for a specific task. We theoretically analyze ConvACs, and empirically validate our findings on more common ConvNets which involve ReLU activations and max pooling. Beyond the results described above, the description of a deep convolutional network in well-defined graph-theoretic tools and the formal connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work.},
	urldate = {2018-07-03},
	journal = {arXiv:1704.01552 [quant-ph]},
	author = {Levine, Yoav and Yakira, David and Cohen, Nadav and Shashua, Amnon},
	month = apr,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1704.01552},
	file = {arXiv\:1704.01552 PDF:/home/brian/Zotero/storage/4B9Z796I/Levine et al. - 2017 - Deep Learning and Quantum Entanglement Fundamenta.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/3YVCR8IU/1704.html:text/html}
}

@article{dickson_thermally_2013,
	title = {Thermally assisted quantum annealing of a 16-qubit problem},
	volume = {4},
	url = {http://dx.doi.org/10.1038/ncomms2920},
	journal = {Nature Communications},
	author = {Dickson, N G and Johnson, M W and Amin, M H and Harris, R and Altomare, F and Berkley, A J and Bunyk, P and Cai, J and Chapple, E M and Chavez, P and Cioata, F and Cirip, T and deBuen, P and Drew-Brook, M and Enderud, C and Gildert, S and Hamze, F and Hilton, J P and Hoskinson, E and Karimi, K and Ladizinsky, E and Ladizinsky, N and Lanting, T and Mahon, T and Neufeld, R and Oh, T and Perminov, I and Petroff, C and Przybysz, A and Rich, C and Spear, P and Tcaciuc, A and Thom, M C and Tolkacheva, E and Uchaikin, S and Wang, J and Wilson, A B and Merali, Z and Rose, G},
	month = may,
	year = {2013},
	pages = {1903}
}

@article{benedetti_estimation_2016,
	title = {Estimation of effective temperatures in quantum annealers for sampling applications: {A} case study with possible applications in deep learning},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.94.022308},
	doi = {10.1103/PhysRevA.94.022308},
	number = {2},
	journal = {Phys. Rev. A},
	author = {Benedetti, Marcello and Realpe-Gómez, John and Biswas, Rupak and Perdomo-Ortiz, Alejandro},
	month = aug,
	year = {2016},
	pages = {022308}
}

@article{adachi_application_2015,
	title = {Application of {Quantum} {Annealing} to {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1510.06356},
	abstract = {In Deep Learning, a well-known approach for training a Deep Neural Network starts by training a generative Deep Belief Network model, typically using Contrastive Divergence (CD), then fine-tuning the weights using backpropagation or other discriminative techniques. However, the generative training can be time-consuming due to the slow mixing of Gibbs sampling. We investigated an alternative approach that estimates model expectations of Restricted Boltzmann Machines using samples from a D-Wave quantum annealing machine. We tested this method on a coarse-grained version of the MNIST data set. In our tests we found that the quantum sampling-based training approach achieves comparable or better accuracy with significantly fewer iterations of generative training than conventional CD-based training. Further investigation is needed to determine whether similar improvements can be achieved for other data sets, and to what extent these improvements can be attributed to quantum effects.},
	urldate = {2018-07-03},
	journal = {arXiv:1510.06356 [quant-ph, stat]},
	author = {Adachi, Steven H. and Henderson, Maxwell P.},
	month = oct,
	year = {2015},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1510.06356},
	annote = {Comment: 18 pages},
	file = {arXiv\:1510.06356 PDF:/home/brian/Zotero/storage/RMJ4T7RL/Adachi and Henderson - 2015 - Application of Quantum Annealing to Training of De.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/JC3363FW/1510.html:text/html}
}

@article{kadowaki_quantum_1998,
	title = {Quantum annealing in the transverse {Ising} model},
	volume = {58},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.58.5355},
	doi = {10.1103/PhysRevE.58.5355},
	number = {5},
	journal = {Phys. Rev. E},
	author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
	month = nov,
	year = {1998},
	pages = {5355--5363}
}

@article{ackley_learning_1985,
	title = {A learning algorithm for boltzmann machines},
	volume = {9},
	issn = {0364-0213},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	doi = {10.1016/S0364-0213(85)80012-4},
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	number = {1},
	journal = {Cognitive Science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	month = jan,
	year = {1985},
	pages = {147--169}
}

@article{giovannetti_quantum_2008,
	title = {Quantum {Random} {Access} {Memory}},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.160501},
	doi = {10.1103/PhysRevLett.100.160501},
	number = {16},
	journal = {Phys. Rev. Lett.},
	author = {Giovannetti, Vittorio and Lloyd, Seth and Maccone, Lorenzo},
	month = apr,
	year = {2008},
	pages = {160501}
}

@article{aaronson_read_2015,
	title = {Read the fine print},
	volume = {11},
	url = {http://dx.doi.org/10.1038/nphys3272},
	journal = {Nature Physics},
	author = {Aaronson, Scott},
	month = apr,
	year = {2015},
	pages = {291}
}

@article{georgescu_quantum_2014,
	title = {Quantum simulation},
	volume = {86},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.86.153},
	doi = {10.1103/RevModPhys.86.153},
	number = {1},
	journal = {Rev. Mod. Phys.},
	author = {Georgescu, I. M. and Ashhab, S. and Nori, Franco},
	month = mar,
	year = {2014},
	pages = {153--185}
}

@article{feynman_simulating_1982,
	title = {Simulating {Physics} with {Computers}},
	volume = {21},
	issn = {0020-7748},
	url = {http://adsabs.harvard.edu/abs/1982IJTP...21..467F},
	doi = {10.1007/BF02650179},
	abstract = {Not Available},
	urldate = {2018-07-02},
	journal = {International Journal of Theoretical Physics},
	author = {Feynman, Richard P.},
	month = jun,
	year = {1982},
	pages = {467--488}
}

@article{fujii_impossibility_2018,
	title = {Impossibility of {Classically} {Simulating} {One}-{Clean}-{Qubit} {Model} with {Multiplicative} {Error}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.200502},
	doi = {10.1103/PhysRevLett.120.200502},
	number = {20},
	journal = {Phys. Rev. Lett.},
	author = {Fujii, Keisuke and Kobayashi, Hirotada and Morimae, Tomoyuki and Nishimura, Harumichi and Tamate, Shuhei and Tani, Seiichiro},
	month = may,
	year = {2018},
	pages = {200502}
}

@article{terhal_adaptive_2002,
	title = {Adaptive {Quantum} {Computation}, {Constant} {Depth} {Quantum} {Circuits} and {Arthur}-{Merlin} {Games}},
	url = {http://arxiv.org/abs/quant-ph/0205133},
	abstract = {We present evidence that there exist quantum computations that can be carried out in constant depth, using 2-qubit gates, that cannot be simulated classically with high accuracy. We prove that if one can simulate these circuits classically efficiently then the complexity class BQP is contained in AM.},
	urldate = {2018-07-02},
	journal = {arXiv:quant-ph/0205133},
	author = {Terhal, Barbara M. and DiVincenzo, David P.},
	month = may,
	year = {2002},
	keywords = {Quantum Physics, Computer Science - Computational Complexity},
	annote = {arXiv: quant-ph/0205133},
	annote = {Comment: Substantially revised version, 13 pages, 1 figure, to appear in QIC. Small correction in Theorem 1 added},
	file = {arXiv\:quant-ph/0205133 PDF:/home/brian/Zotero/storage/PM3GTQWA/Terhal and DiVincenzo - 2002 - Adaptive Quantum Computation, Constant Depth Quant.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/KUMNF883/0205133.html:text/html}
}

@incollection{hinton_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Training} {Restricted} {Boltzmann} {Machines}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
	language = {en},
	urldate = {2018-06-11},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hinton, Geoffrey E.},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_32},
	pages = {599--619},
	file = {Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:/home/brian/Zotero/storage/XX8EJ38M/Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:application/pdf}
}

@article{wiebe_quantum_2015,
	title = {Quantum {Inspired} {Training} for {Boltzmann} {Machines}},
	url = {http://arxiv.org/abs/1507.02642},
	abstract = {We present an efficient classical algorithm for training deep Boltzmann machines (DBMs) that uses rejection sampling in concert with variational approximations to estimate the gradients of the training objective function. Our algorithm is inspired by a recent quantum algorithm for training DBMs. We obtain rigorous bounds on the errors in the approximate gradients; in turn, we find that choosing the instrumental distribution to minimize the alpha=2 divergence with the Gibbs state minimizes the asymptotic algorithmic complexity. Our rejection sampling approach can yield more accurate gradients than low-order contrastive divergence training and the costs incurred in finding increasingly accurate gradients can be easily parallelized. Finally our algorithm can train full Boltzmann machines and scales more favorably with the number of layers in a DBM than greedy contrastive divergence training.},
	urldate = {2018-06-11},
	journal = {arXiv:1507.02642 [quant-ph]},
	author = {Wiebe, Nathan and Kapoor, Ashish and Granade, Christopher and Svore, Krysta M.},
	month = jul,
	year = {2015},
	keywords = {Quantum Physics, Computer Science - Learning},
	annote = {arXiv: 1507.02642},
	file = {arXiv\:1507.02642 PDF:/home/brian/Zotero/storage/CHMG7DGD/Wiebe et al. - 2015 - Quantum Inspired Training for Boltzmann Machines.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/XIPR5KWW/1507.html:text/html}
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.6076},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	number = {5},
	urldate = {2018-06-08},
	journal = {The Annals of Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {2263--2291},
	annote = {arXiv: 1207.6076},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/13-AOS1140 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.6076 PDF:/home/brian/Zotero/storage/AFT9XI5S/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based stati.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/959PLG6H/1207.html:text/html}
}

@article{benedetti_adversarial_2018,
	title = {Adversarial quantum circuit learning for pure state approximation},
	url = {http://arxiv.org/abs/1806.00463},
	abstract = {Adversarial learning is one of the most successful approaches to modelling high-dimensional probability distributions from data. The quantum computing community has recently begun to generalize this idea and to look for potential applications. In this work, we derive an adversarial algorithm for the problem of approximating an unknown quantum pure state. Although this could be done on error-corrected quantum computers, the adversarial formulation enables us to execute the algorithm on near-term quantum computers. Two ansatz circuits are optimized in tandem: One tries to approximate the target state, the other tries to distinguish between target and approximated state. Supported by numerical simulations, we show that resilient backpropagation algorithms perform remarkably well in optimizing the two circuits. We use the bipartite entanglement entropy to design an efficient heuristic for the stopping criteria. Our approach may find application in quantum state tomography.},
	urldate = {2018-06-08},
	journal = {arXiv:1806.00463 [quant-ph, stat]},
	author = {Benedetti, Marcello and Grant, Edward and Wossnig, Leonard and Severini, Simone},
	month = jun,
	year = {2018},
	keywords = {Quantum Physics, Statistics - Machine Learning},
	annote = {arXiv: 1806.00463},
	annote = {Comment: 14 pages, 6 figures},
	file = {arXiv\:1806.00463 PDF:/home/brian/Zotero/storage/6Q7S8UST/Benedetti et al. - 2018 - Adversarial quantum circuit learning for pure stat.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/EIGLBDN8/1806.html:text/html}
}

@article{mcclean_barren_2018,
	title = {Barren plateaus in quantum neural network training landscapes},
	url = {http://arxiv.org/abs/1803.11173},
	abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
	urldate = {2018-06-08},
	journal = {arXiv:1803.11173 [physics, physics:quant-ph]},
	author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
	month = mar,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Learning, Physics - Chemical Physics},
	annote = {arXiv: 1803.11173},
	file = {arXiv\:1803.11173 PDF:/home/brian/Zotero/storage/JVDV4FCF/McClean et al. - 2018 - Barren plateaus in quantum neural network training.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/JDRRJWDN/1803.html:text/html}
}

@inproceedings{grover_fast_1996,
	address = {New York, NY, USA},
	series = {{STOC} '96},
	title = {A {Fast} {Quantum} {Mechanical} {Algorithm} for {Database} {Search}},
	isbn = {0-89791-785-5},
	url = {http://doi.acm.org/10.1145/237814.237866},
	doi = {10.1145/237814.237866},
	booktitle = {Proceedings of the {Twenty}-eighth {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Grover, Lov K.},
	year = {1996},
	pages = {212--219}
}

@article{neill_blueprint_2018,
	title = {A blueprint for demonstrating quantum supremacy with superconducting qubits},
	volume = {360},
	url = {http://science.sciencemag.org/content/360/6385/195.abstract},
	doi = {10.1126/science.aao4309},
	abstract = {Quantum information scientists are getting closer to building a quantum computer that can perform calculations that a classical computer cannot. It has been estimated that such a computer would need around 50 qubits, but scaling up existing architectures to this number is tricky. Neill et al. explore how increasing the number of qubits from five to nine affects the quality of the output of their superconducting qubit device. If, as the number of qubits grows further, the error continues to increase at the same rate, a quantum computer with about 60 qubits and reasonable fidelity might be achievable with current technologies.Science, this issue p. 195A key step toward demonstrating a quantum system that can address difficult problems in physics and chemistry will be performing a computation beyond the capabilities of any classical computer, thus achieving so-called quantum supremacy. In this study, we used nine superconducting qubits to demonstrate a promising path toward quantum supremacy. By individually tuning the qubit parameters, we were able to generate thousands of distinct Hamiltonian evolutions and probe the output probabilities. The measured probabilities obey a universal distribution, consistent with uniformly sampling the full Hilbert space. As the number of qubits increases, the system continues to explore the exponentially growing number of states. Extending these results to a system of 50 qubits has the potential to address scientific questions that are beyond the capabilities of any classical computer.},
	number = {6385},
	journal = {Science},
	author = {Neill, C. and Roushan, P. and Kechedzhi, K. and Boixo, S. and Isakov, S. V. and Smelyanskiy, V. and Megrant, A. and Chiaro, B. and Dunsworth, A. and Arya, K. and Barends, R. and Burkett, B. and Chen, Y. and Chen, Z. and Fowler, A. and Foxen, B. and Giustina, M. and Graff, R. and Jeffrey, E. and Huang, T. and Kelly, J. and Klimov, P. and Lucero, E. and Mutus, J. and Neeley, M. and Quintana, C. and Sank, D. and Vainsencher, A. and Wenner, J. and White, T. C. and Neven, H. and Martinis, J. M.},
	month = apr,
	year = {2018},
	pages = {195}
}

@article{du_bayesian_2018,
	title = {Bayesian {Quantum} {Circuit}},
	url = {http://arxiv.org/abs/1805.11089},
	abstract = {Parameterized quantum circuits (PQCs), as one of the most promising schemes to realize quantum machine learning algorithms on near-term quantum computers, have been designed to solve machine earning tasks with quantum advantages. In this paper, we explain why PQCs with different structures can achieve generative tasks in the discrete case from the perspective of probability estimation. Although different kinds of PQCs are proposed for generative tasks, the current methods often encounter the following three hurdles: (1) the mode contraction problem; (2) unexpected data are often generated with a high proportion; (3) target data cannot be sampled directly. For the purpose of tackling the above hurdles, we devise Bayesian quantum circuit (BQC) through introducing ancillary qubits to represent prior distributions. BQC advances both generative and semi-supervised quantum circuit learning tasks, where its effectiveness is validated by numerical simulations using the Rigetti Forest platform.},
	urldate = {2018-06-08},
	journal = {arXiv:1805.11089 [quant-ph]},
	author = {Du, Yuxuan and Liu, Tongliang and Tao, Dacheng},
	month = may,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1805.11089},
	file = {arXiv\:1805.11089 PDF:/home/brian/Zotero/storage/XCRPTYRB/Du et al. - 2018 - Bayesian Quantum Circuit.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/SDAXXXCX/1805.html:text/html}
}

@article{muandet_kernel_2017,
	title = {Kernel {Mean} {Embedding} of {Distributions}: {A} {Review} and {Beyond}},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Kernel {Mean} {Embedding} of {Distributions}},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	abstract = {A Hilbert space embedding of a distribution—in short, a kernel mean embedding—has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules—which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning—in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	number = {1-2},
	urldate = {2018-06-08},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	pages = {1--141},
	annote = {arXiv: 1605.09522},
	annote = {Comment: 147 pages; this is a version of the manuscript after the review process},
	file = {arXiv\:1605.09522 PDF:/home/brian/Zotero/storage/TP673IVL/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/RGSCB2PJ/1605.html:text/html}
}

@inproceedings{w._van_dam_how_2001,
	title = {How powerful is adiabatic quantum computation?},
	doi = {10.1109/SFCS.2001.959902},
	abstract = {The authors analyze the computational power and limitations of the recently proposed 'quantum adiabatic evolution algorithm'. Adiabatic quantum computation is a novel paradigm for the design of quantum algorithms; it is truly quantum in the sense that it can be used to speed up searching by a quadratic factor over any classical algorithm. On the question of whether this new paradigm may be used to efficiently solve NP-complete problems on a quantum computer, we show that the usual query complexity arguments cannot be used to rule out a polynomial time solution. On the other hand, we argue that the adiabatic approach may be thought of as a kind of 'quantum local search'. We design a family of minimization problems that is hard for such local search heuristics, and establish an exponential lower bound for the adiabatic algorithm for these problems. This provides insights into the limitations of this approach. It remains an open question whether adiabatic quantum computation can establish an exponential speed-up over traditional computing or if there exists a classical algorithm that can simulate the quantum adiabatic process efficiently.},
	booktitle = {Proceedings 2001 {IEEE} {International} {Conference} on {Cluster} {Computing}},
	author = {{W. van Dam} and {M. Mosca} and {U. Vazirani}},
	month = oct,
	year = {2001},
	keywords = {adiabatic quantum computation, Algorithm design and analysis, computational complexity, Computational modeling, computational power, Computer science, Computer simulation, Cryptography, exponential lower bound, exponential speed-up, Laboratories, local search heuristics, minimisation, minimization problems, NP-complete problems, polynomial time solution, Polynomials, quadratic factor, quantum adiabatic evolution algorithm, quantum algorithms, quantum computer, quantum computing, Quantum computing, quantum local search, Quantum mechanics, query complexity arguments, search problems, Simulated annealing},
	pages = {279--287}
}

@article{born_beweis_1928,
	title = {Beweis des {Adiabatensatzes}},
	volume = {51},
	issn = {0044-3328},
	url = {https://link.springer.com/article/10.1007/BF01343193},
	doi = {10.1007/BF01343193},
	abstract = {Der Adiabatensatz in der neuen Quantenmechanik wird für den Fall des Punktspektrums in mathematisch strenger Weise bewiesen, wobei er sich auch bei einer vorübergehenden Entartung des mechanischen...},
	language = {de},
	number = {3-4},
	urldate = {2018-05-31},
	journal = {Zeitschrift für Physik},
	author = {Born, M. and Fock, V.},
	month = mar,
	year = {1928},
	pages = {165--180},
	file = {Full Text PDF:/home/brian/Zotero/storage/BI3UMKJI/Born and Fock - 1928 - Beweis des Adiabatensatzes.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/E33RXTJ8/10.html:text/html}
}

@inproceedings{broadbent_universal_2009,
	title = {Universal {Blind} {Quantum} {Computation}},
	doi = {10.1109/FOCS.2009.36},
	abstract = {We present a protocol which allows a client to have a server carry out a quantum computation for her such that the client's inputs, outputs and computation remain perfectly private, and where she does not require any quantum computational power or memory. The client only needs to be able to prepare single qubits randomly chosen from a finite set and send them to the server, who has the balance of the required quantum computational resources. Our protocol is interactive: after the initial preparation of quantum states, the client and server use two-way classical communication which enables the client to drive the computation, giving single-qubit measurement instructions to the server, depending on previous measurement outcomes. Our protocol works for inputs and outputs that are either classical or quantum. We give an authentication protocol that allows the client to detect an interfering server; our scheme can also be made fault-tolerant. We also generalize our result to the setting of a purely classical client who communicates classically with two non-communicating entangled servers, in order to perform a blind quantum computation. By incorporating the authentication protocol, we show that any problem in BQP has an entangled two-prover interactive proof with a purely classical verifier. Our protocol is the first universal scheme which detects a cheating server, as well as the first protocol which does not require any quantum computation whatsoever on the client's side. The novelty of our approach is in using the unique features of measurement-based quantum computing which allows us to clearly distinguish between the quantum and classical aspects of a quantum computation.},
	booktitle = {2009 50th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Broadbent, A. and Fitzsimons, J. and Kashefi, E.},
	month = oct,
	year = {2009},
	keywords = {Computer science, Quantum computing, Authentication, authentication protocol, blind quantum computation, cheating server detection, Computer aided instruction, cryptographic protocols, Fault detection, Fault tolerance, Informatics, measurement-based quantum computing, noncommunicating entangled servers, Privacy, Protocols, purely classical client setting, quantum cryptography, Quantum entanglement, quantum prover interactive proofs, single-qubit measurement, theorem proving, two-prover interactive proof, two-way classical communication},
	pages = {517--526},
	file = {IEEE Xplore Abstract Record:/home/brian/Zotero/storage/VA5ZD5IT/5438603.html:text/html}
}

@book{noauthor_ctan:_nodate,
	title = {{CTAN}: {Package} qcircuit},
	url = {https://ctan.org/pkg/qcircuit?lang=en},
	urldate = {2018-04-27},
	file = {CTAN\: Package qcircuit:/home/brian/Zotero/storage/RTJSNLRY/qcircuit.html:text/html}
}

@article{bermejo-vega_architectures_2018,
	title = {Architectures for {Quantum} {Simulation} {Showing} a {Quantum} {Speedup}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.021010},
	doi = {10.1103/PhysRevX.8.021010},
	number = {2},
	journal = {Phys. Rev. X},
	author = {Bermejo-Vega, Juan and Hangleiter, Dominik and Schwarz, Martin and Raussendorf, Robert and Eisert, Jens},
	month = apr,
	year = {2018},
	pages = {021010}
}

@article{gao_quantum_2017,
	title = {Quantum {Supremacy} for {Simulating} a {Translation}-{Invariant} {Ising} {Spin} {Model}},
	volume = {118},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.040502},
	doi = {10.1103/PhysRevLett.118.040502},
	number = {4},
	journal = {Phys. Rev. Lett.},
	author = {Gao, Xun and Wang, Sheng-Tao and Duan, L.-M.},
	month = jan,
	year = {2017},
	pages = {040502}
}

@article{kieferova_tomography_2017,
	title = {Tomography and generative training with quantum {Boltzmann} machines},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.96.062327},
	doi = {10.1103/PhysRevA.96.062327},
	number = {6},
	journal = {Phys. Rev. A},
	author = {Kieferová, Mária and Wiebe, Nathan},
	month = dec,
	year = {2017},
	pages = {062327}
}

@article{li_hybrid_2017,
	title = {Hybrid {Quantum}-{Classical} {Approach} to {Quantum} {Optimal} {Control}},
	volume = {118},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.150503},
	doi = {10.1103/PhysRevLett.118.150503},
	number = {15},
	journal = {Phys. Rev. Lett.},
	author = {Li, Jun and Yang, Xiaodong and Peng, Xinhua and Sun, Chang-Pu},
	month = apr,
	year = {2017},
	pages = {150503}
}

@article{gao_efficient_2017-1,
	title = {Efficient representation of quantum many-body states with deep neural networks},
	volume = {8},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-017-00705-2},
	doi = {10.1038/s41467-017-00705-2},
	abstract = {Part of the challenge for quantum many-body problems comes from the difficulty of representing large-scale quantum states, which in general requires an exponentially large number of parameters. Neural networks provide a powerful tool to represent quantum many-body states. An important open question is what characterizes the representational power of deep and shallow neural networks, which is of fundamental interest due to the popularity of deep learning methods. Here, we give a proof that, assuming a widely believed computational complexity conjecture, a deep neural network can efficiently represent most physical states, including the ground states of many-body Hamiltonians and states generated by quantum dynamics, while a shallow network representation with a restricted Boltzmann machine cannot efficiently represent some of those states.},
	number = {1},
	journal = {Nature Communications},
	author = {Gao, Xun and Duan, Lu-Ming},
	month = sep,
	year = {2017},
	pages = {662}
}

@article{carleo_solving_2017,
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	issn = {0036-8075},
	url = {http://science.sciencemag.org/content/355/6325/602},
	doi = {10.1126/science.aag2302},
	abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.Science, this issue p. 602; see also p. 580The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.},
	number = {6325},
	journal = {Science},
	author = {Carleo, Giuseppe and Troyer, Matthias},
	year = {2017},
	pages = {602--606}
}

@article{raussendorf_one-way_2001,
	title = {A {One}-{Way} {Quantum} {Computer}},
	volume = {86},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.86.5188},
	doi = {10.1103/PhysRevLett.86.5188},
	number = {22},
	journal = {Phys. Rev. Lett.},
	author = {Raussendorf, Robert and Briegel, Hans J.},
	month = may,
	year = {2001},
	pages = {5188--5191}
}

@book{noauthor_[1704.08482]_nodate,
	title = {[1704.08482] {On} the implausibility of classical client blind quantum computing},
	url = {https://arxiv.org/abs/1704.08482},
	urldate = {2018-04-27},
	file = {[1704.08482] On the implausibility of classical client blind quantum computing:/home/brian/Zotero/storage/GT4TXYF8/1704.html:text/html}
}

@article{gheorghiu_verification_2017,
	title = {Verification of quantum computation: {An} overview of existing approaches},
	shorttitle = {Verification of quantum computation},
	url = {https://arxiv.org/abs/1709.06984},
	language = {en},
	urldate = {2018-04-20},
	author = {Gheorghiu, Alexandru and Kapourniotis, Theodoros and Kashefi, Elham},
	month = sep,
	year = {2017},
	file = {Full Text PDF:/home/brian/Zotero/storage/9P3AQ5EH/Gheorghiu et al. - 2017 - Verification of quantum computation An overview o.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/LTD4NM7D/1709.html:text/html}
}

@book{nielsen_quantum_2010,
	address = {Cambridge},
	edition = {Tenth anniversary edition..},
	title = {Quantum computation and quantum information},
	isbn = {978-1-107-00217-3},
	abstract = {Fundamental Concepts – Introduction and overview – Introduction to quantum mechanics – Introduction to computer science – Quantum Computation – Quantum circuits – The quantum Fourier transform and its applications – Quantum search algorithms – Quantum computers: physical realisation – Quantum Information – Quantum noise, open quantum systems, and quantum operations – Distance measurement for quantum information – Quantum error-correction – Entropy and information – Quantum information theory – Notes on basic probability theory – Group theory – Approximating quantum gates: the Solovay-Kitaev theorem – Number theory – Public-key cryptography and the RSA cryptosystem – Proof of Lieb's theorem.},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Nielsen, Michael A.},
	year = {2010},
	keywords = {Quantum computers.}
}

@article{kapourniotis_nonadaptive_2017,
	title = {Nonadaptive fault-tolerant verification of quantum supremacy with noise},
	url = {http://arxiv.org/abs/1703.09568},
	abstract = {Quantum samplers are believed capable of sampling efficiently from distributions that are classically hard to sample from. We consider a sampler inspired by the Ising model. It is nonadaptive and therefore experimentally amenable. Under a plausible average-case hardness conjecture, classical sampling upto additive errors from this model is known to be hard. We present a trap-based verification scheme for quantum supremacy that only requires the verifier to prepare single-qubit states. The verification is done on the same model as the original sampler, a square lattice, with only a constant factor overhead. We next revamp our verification scheme to operate in the presence of noise by emulating a fault-tolerant procedure without correcting on-line for the errors, thus keeping the model non-adaptive, but verifying supremacy fault-tolerantly. We show that classically sampling upto additive errors is likely hard in our revamped scheme. Our results are applicable to more general sampling problems such as the Instantaneous Quantum Polynomial-time (IQP) computation model. It should also assist near-term attempts at experimentally demonstrating quantum supremacy and guide long-term ones.},
	urldate = {2018-04-12},
	journal = {arXiv:1703.09568 [quant-ph]},
	author = {Kapourniotis, Theodoros and Datta, Animesh},
	month = mar,
	year = {2017},
	keywords = {Quantum Physics},
	annote = {arXiv: 1703.09568},
	annote = {Comment: 17 pages, 10 figures, 3 theorems, 1 conjecture. Minor changes in discussions, additional explanation in the proof of Theorem 1 and updated references},
	file = {arXiv\:1703.09568 PDF:/home/brian/Zotero/storage/ANAKVV68/Kapourniotis and Datta - 2017 - Nonadaptive fault-tolerant verification of quantum.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/RYIVH3SD/1703.html:text/html}
}

@article{grant_hierarchical_2018,
	title = {Hierarchical quantum classifiers},
	url = {http://arxiv.org/abs/1804.03680},
	abstract = {Hierarchical quantum circuits have been shown to perform binary classification of classical data encoded in a quantum state. We demonstrate that more expressive circuits in the same family achieve better accuracy and can be used to classify highly entangled quantum states, for which there is no known efficient classical method. We compare performance for several different parameterizations on two classical machine learning datasets, Iris and MNIST, and on a synthetic dataset of quantum states. Finally, we demonstrate that performance is robust to noise and deploy an Iris dataset classifier on the ibmqx4 quantum computer.},
	urldate = {2018-04-12},
	journal = {arXiv:1804.03680 [quant-ph]},
	author = {Grant, Edward and Benedetti, Marcello and Cao, Shuxiang and Hallam, Andrew and Lockhart, Joshua and Stojevic, Vid and Green, Andrew G. and Severini, Simone},
	month = apr,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1804.03680},
	file = {arXiv\:1804.03680 PDF:/home/brian/Zotero/storage/THXRSTKY/Grant et al. - 2018 - Hierarchical quantum classifiers.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/BUQXW5B6/1804.html:text/html}
}

@inproceedings{fukumizu_kernel_2007,
	title = {Kernel {Measures} of {Conditional} {Dependence}},
	abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
	booktitle = {{NIPS}},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	year = {2007},
	keywords = {Cross-covariance, Experiment, Hilbert space, Kernel (operating system), Vergence},
	file = {Full Text PDF:/home/brian/Zotero/storage/5B4CHVEH/Fukumizu et al. - 2007 - Kernel Measures of Conditional Dependence.pdf:application/pdf}
}

@book{dudley_real_2002,
	edition = {2},
	series = {Cambridge {Studies} in {Advanced} {Mathematics}},
	title = {Real {Analysis} and {Probability}},
	publisher = {Cambridge University Press},
	author = {Dudley, R. M.},
	year = {2002},
	doi = {10.1017/CBO9780511755347}
}

@article{hoban_measurement-based_2014,
	title = {Measurement-{Based} {Classical} {Computation}},
	volume = {112},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.112.140505},
	doi = {10.1103/PhysRevLett.112.140505},
	number = {14},
	journal = {Phys. Rev. Lett.},
	author = {Hoban, Matty J. and Wallman, Joel J. and Anwar, Hussain and Usher, Naïri and Raussendorf, Robert and Browne, Dan E.},
	month = apr,
	year = {2014},
	pages = {140505}
}


@article{harrow_low-depth_2019,
	title = {Low-depth gradient measurements can improve convergence in variational hybrid quantum-classical algorithms},
	url = {http://arxiv.org/abs/1901.05374},
	abstract = {A broad class of hybrid quantum-classical algorithms known as "variational algorithms" have been proposed in the context of quantum simulation, machine learning, and combinatorial optimization as a means of potentially achieving a quantum speedup on a near-term quantum device for a problem of practical interest. Such algorithms use the quantum device only to prepare parameterized quantum states and make simple measurements. A classical controller uses the measurement results to perform an optimization of a classical function induced by a quantum observable which defines the problem. While most prior works have considered optimization strategies based on estimating the objective function and doing a derivative-free or finite-difference-based optimization, some recent proposals involve directly measuring observables corresponding to the gradient of the objective function. The measurement procedure needed requires coherence time barely longer than that needed to prepare a trial state. We prove that strategies based on such gradient measurements can admit substantially faster rates of convergence to the optimum in some contexts. We first introduce a natural black-box setting for variational algorithms which we prove our results with respect to. We define a simple class of problems for which a variational algorithm based on low-depth gradient measurements and stochastic gradient descent converges to the optimum substantially faster than any possible strategy based on estimating the objective function itself, and show that stochastic gradient descent is essentially optimal for this problem. Importing known results from the stochastic optimization literature, we also derive rigorous upper bounds on the cost of variational optimization in a convex region when using gradient measurements in conjunction with certain stochastic gradient descent or stochastic mirror descent algorithms.},
	urldate = {2019-02-05},
	journal = {arXiv:1901.05374 [quant-ph]},
	author = {Harrow, Aram and Napp, John},
	month = jan,
	year = {2019},
	keywords = {Quantum Physics, Mathematics - Optimization and Control},
	annote = {arXiv: 1901.05374},
	annote = {Comment: 45 pages},
	file = {arXiv\:1901.05374 PDF:/home/brian/Zotero/storage/8GXA2B7N/Harrow and Napp - 2019 - Low-depth gradient measurements can improve conver.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/DHLISVUE/1901.html:text/html}
}

@article{kerenidis_quantum_2018,
	title = {Quantum classification of the {MNIST} dataset via {Slow} {Feature} {Analysis}},
	url = {http://arxiv.org/abs/1805.08837},
	abstract = {Quantum machine learning carries the promise to revolutionize information and communication technologies. While a number of quantum algorithms with potential exponential speedups have been proposed already, it is quite difficult to provide convincing evidence that quantum computers with quantum memories will be in fact useful to solve real-world problems. Our work makes considerable progress towards this goal. We design quantum techniques for Dimensionality Reduction and for Classification, and combine them to provide an efficient and high accuracy quantum classifier that we test on the MNIST dataset. More precisely, we propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality reduction technique that maps the dataset in a lower dimensional space where we can apply a novel quantum classification procedure, the Quantum Frobenius Distance (QFD). We simulate the quantum classifier (including errors) and show that it can provide classification of the MNIST handwritten digit dataset, a widely used dataset for benchmarking classification algorithms, with \$98.5{\textbackslash}textbackslash\%\$ accuracy, similar to the classical case. The running time of the quantum classifier is polylogarithmic in the dimension and number of data points. We also provide evidence that the other parameters on which the running time depends (condition number, Frobenius norm, error threshold, etc.) scale favorably in practice, thus ascertaining the efficiency of our algorithm.},
	urldate = {2019-01-31},
	journal = {arXiv:1805.08837 [quant-ph]},
	author = {Kerenidis, Iordanis and Luongo, Alessandro},
	month = may,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {arXiv: 1805.08837},
	annote = {Comment: Results mainly unchanged. Improved running time analysis based on recent work in [CGJ19, GSLW18]},
	file = {arXiv\:1805.08837 PDF:/home/brian/Zotero/storage/YPU4PUNN/Kerenidis and Luongo - 2018 - Quantum classification of the MNIST dataset via Sl.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/V25HZTCG/1805.html:text/html}
}

@article{rebentrost_quantum_2014,
	title = {Quantum {Support} {Vector} {Machine} for {Big} {Data} {Classification}},
	volume = {113},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.113.130503},
	doi = {10.1103/PhysRevLett.113.130503},
	number = {13},
	journal = {Phys. Rev. Lett.},
	author = {Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth},
	month = sep,
	year = {2014},
	pages = {130503}
}

@article{hu_quantum_2019,
	title = {Quantum generative adversarial learning in a superconducting quantum circuit},
	volume = {5},
	url = {http://advances.sciencemag.org/content/5/1/eaav2761.abstract},
	doi = {10.1126/sciadv.aav2761},
	abstract = {Generative adversarial learning is one of the most exciting recent breakthroughs in machine learning. It has shown splendid performance in a variety of challenging tasks such as image and video generation. More recently, a quantum version of generative adversarial learning has been theoretically proposed and shown to have the potential of exhibiting an exponential advantage over its classical counterpart. Here, we report the first proof-of-principle experimental demonstration of quantum generative adversarial learning in a superconducting quantum circuit. We demonstrate that, after several rounds of adversarial learning, a quantum-state generator can be trained to replicate the statistics of the quantum data output from a quantum channel simulator, with a high fidelity (98.8\% on average) so that the discriminator cannot distinguish between the true and the generated data. Our results pave the way for experimentally exploring the intriguing long-sought-after quantum advantages in machine learning tasks with noisy intermediate–scale quantum devices.},
	number = {1},
	journal = {Science Advances},
	author = {Hu, Ling and Wu, Shu-Hao and Cai, Weizhou and Ma, Yuwei and Mu, Xianghao and Xu, Yuan and Wang, Haiyan and Song, Yipu and Deng, Dong-Ling and Zou, Chang-Ling and Sun, Luyan},
	month = jan,
	year = {2019},
	pages = {eaav2761}
}

@article{sinkhorn_relationship_1964,
	title = {A {Relationship} {Between} {Arbitrary} {Positive} {Matrices} and {Doubly} {Stochastic} {Matrices}},
	volume = {35},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177703591},
	doi = {10.1214/aoms/1177703591},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {2},
	urldate = {2019-01-15},
	journal = {The Annals of Mathematical Statistics},
	author = {Sinkhorn, Richard},
	month = jun,
	year = {1964},
	mrnumber = {MR161868},
	zmnumber = {0134.25302},
	pages = {876--879},
	file = {Full Text PDF:/home/brian/Zotero/storage/RSUCVMY5/Sinkhorn - 1964 - A Relationship Between Arbitrary Positive Matrices.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/MWAWEXEM/1177703591.html:text/html}
}

@article{peyre_computational_2018,
	title = {Computational {Optimal} {Transport}},
	url = {http://arxiv.org/abs/1803.00567},
	abstract = {Optimal Transport (OT) is a mathematical gem at the interface between probability, analysis and optimization. The goal of that theory is to define geometric tools that are useful to compare probability distributions. Earlier contributions originated from Monge's work in the 18th century, to be later rediscovered under a different formalism by Tolstoi in the 1920's, Kantorovich, Hitchcock and Koopmans in the 1940's. The problem was solved numerically by Dantzig in 1949 and others in the 1950's within the framework of linear programming, paving the way for major industrial applications in the second half of the 20th century. OT was later rediscovered under a different light by analysts in the 90's, following important work by Brenier and others, as well as in the computer vision/graphics fields under the name of earth mover's distances. Recent years have witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression,classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
	urldate = {2019-01-14},
	journal = {arXiv:1803.00567 [stat]},
	author = {Peyré, Gabriel and Cuturi, Marco},
	month = mar,
	year = {2018},
	keywords = {Statistics - Machine Learning},
	annote = {arXiv: 1803.00567},
	file = {arXiv\:1803.00567 PDF:/home/brian/Zotero/storage/BDXY2J5R/Peyré and Cuturi - 2018 - Computational Optimal Transport.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/GFT69XSB/1803.html:text/html}
}

@article{weed_sharp_2017,
	title = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in {Wasserstein} distance},
	url = {http://arxiv.org/abs/1707.00087},
	abstract = {The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from \$n\$ independent samples from \${\textbackslash}textbackslashmu\$ approaches \${\textbackslash}textbackslashmu\$ in the Wasserstein distance of any order. We prove sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces. Our finite-sample results show the existence of multi-scale behavior, where measures can exhibit radically different rates of convergence as \$n\$ grows.},
	urldate = {2019-01-10},
	journal = {arXiv:1707.00087 [math, stat]},
	author = {Weed, Jonathan and Bach, Francis},
	month = jun,
	year = {2017},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability, 60B10, 62E17},
	annote = {arXiv: 1707.00087},
	file = {arXiv\:1707.00087 PDF:/home/brian/Zotero/storage/8QYKZ6LU/Weed and Bach - 2017 - Sharp asymptotic and finite-sample rates of conver.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/CQLEHS4D/1707.html:text/html}
}

@article{nystrom_uber_1930,
	title = {Über {Die} {Praktische} {Auflösung} von {Integralgleichungen} mit {Anwendungen} auf {Randwertaufgaben}},
	volume = {54},
	url = {https://doi.org/10.1007/BF02547521},
	doi = {10.1007/BF02547521},
	journal = {Acta Mathematica},
	author = {Nyström, E. J.},
	year = {1930},
	pages = {185--204}
}

@article{cuturi_sinkhorn_2013,
	title = {Sinkhorn {Distances}: {Lightspeed} {Computation} of {Optimal} {Transportation} {Distances}},
	shorttitle = {Sinkhorn {Distances}},
	url = {http://arxiv.org/abs/1306.0895},
	abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
	urldate = {2018-12-06},
	journal = {arXiv:1306.0895 [stat]},
	author = {Cuturi, Marco},
	month = jun,
	year = {2013},
	keywords = {Statistics - Machine Learning},
	annote = {arXiv: 1306.0895},
	file = {arXiv\:1306.0895 PDF:/home/brian/Zotero/storage/896XKU6H/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/ICUSWRR8/1306.html:text/html}
}

@article{diggle_monte_1984,
	title = {Monte {Carlo} {Methods} of {Inference} for {Implicit} {Statistical} {Models}},
	volume = {46},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345504},
	abstract = {A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more fundamental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated by an example involving a model for the spatial pattern of displaced amacrine cells in the retina of a rabbit.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Diggle, Peter J. and Gratton, Richard J.},
	year = {1984},
	pages = {193--227}
}

@article{mohamed_learning_2016,
	title = {Learning in {Implicit} {Generative} {Models}},
	url = {http://arxiv.org/abs/1610.03483},
	abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning—to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models–models that only specify a stochastic procedure with which to generate data–and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	urldate = {2018-12-02},
	journal = {arXiv:1610.03483 [cs, stat]},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	annote = {arXiv: 1610.03483},
	file = {arXiv\:1610.03483 PDF:/home/brian/Zotero/storage/S72JPQMT/Mohamed and Lakshminarayanan - 2016 - Learning in Implicit Generative Models.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/YF6AABJ9/1610.html:text/html}
}

@article{sriperumbudur_hilbert_2010,
	title = {Hilbert {Space} {Embeddings} and {Metrics} on {Probability} {Measures}},
	volume = {11},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v11/sriperumbudur10a.html},
	number = {Apr},
	urldate = {2018-11-26},
	journal = {Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	year = {2010},
	pages = {1517--1561},
	file = {Full Text PDF:/home/brian/Zotero/storage/UL9QEYTB/Sriperumbudur et al. - 2010 - Hilbert Space Embeddings and Metrics on Probabilit.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/JMPCKF8F/sriperumbudur10a.html:text/html}
}

@article{sriperumbudur_universality_2011,
	title = {Universality, {Characteristic} {Kernels} and {RKHS} {Embedding} of {Measures}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/sriperumbudur11a.html},
	number = {Jul},
	urldate = {2018-11-26},
	journal = {Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert R. G.},
	year = {2011},
	pages = {2389--2410},
	file = {Full Text PDF:/home/brian/Zotero/storage/AEB7KQ5S/Sriperumbudur et al. - 2011 - Universality, Characteristic Kernels and RKHS Embe.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/J9KZ5DW4/sriperumbudur11a.html:text/html}
}

@inproceedings{sriperumbudur_injective_2008,
	title = {Injective {Hilbert} {Space} {Embeddings} of {Probability} {Measures}},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). The embedding function has been proven to be injective when the reproducing kernel is universal. In this case, the embedding induces a metric on the space of probability distributions defined on compact metric spaces. In the present work, we consider more broadly the problem of specifying characteristic kernels, defined as kernels for which the RKHS embedding of probability measures is injective. In particular, characteristic kernels can include non-universal kernels. We restrict ourselves to translation-invariant kernels on Euclidean space, and define the associated metric on probability measures in terms of the Fourier spectrum of the kernel and characteristic functions of these measures. The support of the kernel spectrum is important in finding whether a kernel is characteristic: in particular, the embedding is injective if and only if the kernel spectrum has the entire domain as its support. Characteristic kernels may nonetheless have difficulty in distinguishing certain distributions on the basis of finite samples, again due to the interaction of the kernel spectrum and the characteristic functions of the measures.},
	booktitle = {{COLT}},
	author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Lanckriet, Gert R. G. and Schölkopf, Bernhard},
	year = {2008},
	keywords = {Hilbert space, Kernel (operating system), Dimensionality reduction},
	file = {Full Text PDF:/home/brian/Zotero/storage/28ET8S7S/Sriperumbudur et al. - 2008 - Injective Hilbert Space Embeddings of Probability .pdf:application/pdf}
}

@article{situ_quantum_2018,
	title = {Quantum generative adversarial network for generating discrete data},
	url = {http://arxiv.org/abs/1807.01235},
	abstract = {Generative adversarial network (GAN) is an effective machine learning framework to train unsupervised generative models, and has drawn lots of attention in recent years. In the GAN framework, the generator is trained by an adversarial discriminator, in order to generate new samples that follows the probability distribution of a given training dataset. Classical GANs cannot generate discrete data due to the requirement of differentiability on the design of generators. Distributions of measurement outcomes of quantum circuits are continuous and differentiable, so quantum GANs can generate discrete data and complement classical GANs. In this paper, we present a quantum version of GAN for generation of discrete data, where parameterized quantum circuits are trained by a classical discriminator. Two families of quantum circuits, both composed of simple one-qubit rotation and two-qubit controlled-phase gates, are considered. The results of a small-scale proof-of-principle numerical experiment demonstrate that quantum circuits can be effectively trained in an adversarial way for generative tasks.},
	urldate = {2018-10-25},
	journal = {arXiv:1807.01235 [quant-ph]},
	author = {Situ, Haozhen and He, Zhimin and Li, Lvzhou and Zheng, Shenggen},
	month = jul,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1807.01235},
	file = {arXiv\:1807.01235 PDF:/home/brian/Zotero/storage/CZW3TY7C/Situ et al. - 2018 - Quantum generative adversarial network for generat.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/RAMU7UUV/1807.html:text/html}
}

@incollection{liu_stein_2016,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	url = {http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {2378--2386}
}

@article{killoran_strawberry_2018,
	title = {Strawberry {Fields}: {A} {Software} {Platform} for {Photonic} {Quantum} {Computing}},
	shorttitle = {Strawberry {Fields}},
	url = {http://arxiv.org/abs/1804.03159},
	abstract = {We introduce Strawberry Fields, an open-source quantum programming architecture for light-based quantum computers. Built in Python, Strawberry Fields is a full-stack library for design, simulation, optimization, and quantum machine learning of continuous-variable circuits. The platform consists of three main components: (i) an API for quantum programming based on an easy-to-use language named Blackbird; (ii) a suite of three virtual quantum computer backends, built in NumPy and Tensorflow, each targeting specialized uses; and (iii) an engine which can compile Blackbird programs on various backends, including the three built-in simulators, and – in the near future – photonic quantum information processors. The library also contains examples of several paradigmatic algorithms, including teleportation, (Gaussian) boson sampling, instantaneous quantum polynomial, Hamiltonian simulation, and variational quantum circuit optimization.},
	urldate = {2018-08-13},
	journal = {arXiv:1804.03159 [physics, physics:quant-ph]},
	author = {Killoran, Nathan and Izaac, Josh and Quesada, Nicolás and Bergholm, Ville and Amy, Matthew and Weedbrook, Christian},
	month = apr,
	year = {2018},
	keywords = {Quantum Physics, Physics - Computational Physics},
	annote = {arXiv: 1804.03159},
	annote = {Comment: Try the Strawberry Fields Interactive website, located at http://strawberryfields.ai . Source code available at https://github.com/XanaduAI/strawberryfields},
	file = {arXiv\:1804.03159 PDF:/home/brian/Zotero/storage/3JR62ZGA/Killoran et al. - 2018 - Strawberry Fields A Software Platform for Photoni.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/NKBQEKZ6/1804.html:text/html}
}

@book{noauthor_software_nodate,
	title = {Software},
	url = {https://www.xanadu.ai/software/},
	abstract = {Open-source software for photonic quantum computing. A full-stack Python library for designing, simulating, and optimizing quantum optical circuits. Includes a suite of three simulators for execution on CPU or GPU.},
	language = {en},
	urldate = {2018-08-13},
	file = {Snapshot:/home/brian/Zotero/storage/6SLDASQH/software.html:text/html}
}

@article{newell_perceptrons._1969,
	title = {Perceptrons. {An} {Introduction} to {Computational} {Geometry}. {Marvin} {Minsky} and {Seymour} {Papert}. {M}.{I}.{T}. {Press}, {Cambridge}, {Mass}., 1969. vi + 258 pp., illus. {Cloth}, \$12; paper, \$4.95},
	volume = {165},
	issn = {0036-8075},
	url = {http://science.sciencemag.org/content/165/3895/780},
	doi = {10.1126/science.165.3895.780},
	number = {3895},
	journal = {Science},
	author = {Newell, Allen},
	year = {1969},
	pages = {780--782}
}

@article{boykin_universal_1999,
	title = {On {Universal} and {Fault}-{Tolerant} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/quant-ph/9906054},
	abstract = {A novel universal and fault-tolerant basis (set of gates) for quantum computation is described. Such a set is necessary to perform quantum computation in a realistic noisy environment. The new basis consists of two single-qubit gates (Hadamard and \$\{{\textbackslash}textbackslashsigma\_z\}ˆ\{1/4\}\$), and one double-qubit gate (Controlled-NOT). Since the set consisting of Controlled-NOT and Hadamard gates is not universal, the new basis achieves universality by including only one additional elementary (in the sense that it does not include angles that are irrational multiples of \${\textbackslash}textbackslashpi\$) single-qubit gate, and hence, is potentially the simplest universal basis that one can construct. We also provide an alternative proof of universality for the only other known class of universal and fault-tolerant basis proposed by Shor and by Kitaev.},
	urldate = {2018-07-27},
	journal = {arXiv:quant-ph/9906054},
	author = {Boykin, P. Oscar and Mor, Tal and Pulver, Matthew and Roychowdhury, Vwani and Vatan, Farrokh},
	month = jun,
	year = {1999},
	keywords = {Quantum Physics},
	annote = {arXiv: quant-ph/9906054},
	annote = {Comment: 10 pages, Latex. Emails addresses \{boykin, talmo, pulver, vwani, vatan\}@ee.ucla.edu},
	file = {arXiv\:quant-ph/9906054 PDF:/home/brian/Zotero/storage/EWF5IYQ8/Boykin et al. - 1999 - On Universal and Fault-Tolerant Quantum Computing.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/DN4Q663B/9906054.html:text/html}
}

@article{sopka_introductory_1979,
	title = {Introductory {Functional} {Analysis} with {Applications} ({Erwin} {Kreyszig})},
	volume = {21},
	url = {https://doi.org/10.1137/1021075},
	doi = {10.1137/1021075},
	number = {3},
	journal = {SIAM Review},
	author = {Sopka, J.},
	year = {1979},
	pages = {412--413}
}

@article{trotter_product_1959,
	title = {On the {Product} of {Semi}-{Groups} of {Operators}},
	volume = {10},
	issn = {00029939, 10886826},
	url = {http://www.jstor.org/stable/2033649},
	number = {4},
	journal = {Proceedings of the American Mathematical Society},
	author = {Trotter, H. F.},
	year = {1959},
	pages = {545--551}
}

@article{suzuki_fractal_1990,
	title = {Fractal decomposition of exponential operators with applications to many-body theories and {Monte} {Carlo} simulations},
	volume = {146},
	issn = {0375-9601},
	url = {http://www.sciencedirect.com/science/article/pii/037596019090962N},
	doi = {https://doi.org/10.1016/0375-9601(90)90962-N},
	abstract = {A new systematic scheme of decomposition of exponential operators is presented, namely exp [x(A+B)]=Sm(x)+O(xm+1) for any positive integer m, where Sm(x)=et1Aet2Bet3Aet4B…etMA. A general scheme of construction of tj is given explicitly. The decomposition exp[x(A+B)]=[Sm(x/n)]n+O(xm+1/nm) yields a new efficient approach to quantum Monte Carlo simulations.},
	number = {6},
	journal = {Physics Letters A},
	author = {Suzuki, Masuo},
	year = {1990},
	pages = {319 -- 323}
}

@article{han_threshold_1997,
	title = {Threshold {Computation} and {Cryptographic} {Security}},
	volume = {26},
	url = {https://doi.org/10.1137/S0097539792240467},
	doi = {10.1137/S0097539792240467},
	number = {1},
	journal = {SIAM Journal on Computing},
	author = {Han, Y. and Hemaspaandra, L. and Thierauf, T.},
	year = {1997},
	pages = {59--78}
}

@article{aaronson_quantum_2005,
	title = {Quantum computing, postselection, and probabilistic polynomial-time},
	volume = {461},
	url = {http://rspa.royalsocietypublishing.org/content/461/2063/3473.abstract},
	doi = {10.1098/rspa.2005.1546},
	abstract = {I study the class of problems efficiently solvable by a quantum computer, given the ability to ‘postselect’ on the outcomes of measurements. I prove that this class coincides with a classical complexity class called PP, or probabilistic polynomial-time. Using this result, I show that several simple changes to the axioms of quantum mechanics would let us solve PP-complete problems efficiently. The result also implies, as an easy corollary, a celebrated theorem of Beigel, Reingold and Spielman that PP is closed under intersection, as well as a generalization of that theorem due to Fortnow and Reingold. This illustrates that quantum computing can yield new and simpler proofs of major results about classical computation.},
	number = {2063},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
	author = {Aaronson, Scott},
	month = nov,
	year = {2005},
	pages = {3473}
}

@article{toda_pp_1991,
	title = {{PP} is as {Hard} as the {Polynomial}-{Time} {Hierarchy}},
	volume = {20},
	url = {https://doi.org/10.1137/0220053},
	doi = {10.1137/0220053},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Toda, S.},
	year = {1991},
	pages = {865--877}
}

@article{stockmeyer_polynomial-time_1976,
	title = {The polynomial-time hierarchy},
	volume = {3},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/030439757690061X},
	doi = {https://doi.org/10.1016/0304-3975(76)90061-X},
	abstract = {The polynomial-time hierarchy is that subrecursive analog of the Kleene arithmetical hierarchy in which deterministic (nondeterministic) polynomial time plays the role of recursive (recursively enumerable) time. Known properties of the polynomial-time hierarchy are summarized. A word problem which is complete in the second stage of the hierarchy is exhibited. In the analogy between the polynomial-time hierarchy and the arithmetical hierarchy, the first order theory of equality plays the role of elementary arithmetic (as the ω-jump of the hierarchy). The problem of deciding validity in the theory of equality is shown to be complete in polynomial-space, and close upper and lower bounds on the space complexity of this problem are established.},
	number = {1},
	journal = {Theoretical Computer Science},
	author = {Stockmeyer, Larry J.},
	year = {1976},
	pages = {1 -- 22}
}

@book{noauthor_pyquil:_2018,
	title = {pyquil: {A} {Python} library for quantum programming using {Quil}},
	copyright = {Apache-2.0},
	shorttitle = {pyquil},
	url = {https://github.com/rigetticomputing/pyquil},
	urldate = {2018-07-17},
	publisher = {Rigetti Computing},
	month = jul,
	year = {2018},
	keywords = {forest, quantum, quantum-computing, quil, rigetti-forest},
	annote = {original-date: 2017-01-09T21:30:22Z}
}

@article{deutsch_quantum_1985,
	title = {Quantum theory, the {Church}–{Turing} principle and the universal quantum computer},
	volume = {400},
	url = {http://rspa.royalsocietypublishing.org/content/400/1818/97.abstract},
	doi = {10.1098/rspa.1985.0070},
	abstract = {It is argued that underlying the Church–Turing hypothesis there is an implicit physical assertion. Here, this assertion is presented explicitly as a physical principle: ‘every finitely realizible physical system can be perfectly simulated by a universal model computing machine operating by finite means’. Classical physics and the universal Turing machine, because the former is continuous and the latter discrete, do not obey the principle, at least in the strong form above. A class of model computing machines that is the quantum generalization of the class of Turing machines is described, and it is shown that quantum theory and the 'universal quantum computer’ are compatible with the principle. Computing machines resembling the universal quantum computer could, in principle, be built and would have many remarkable properties not reproducible by any Turing machine. These do not include the computation of non-recursive functions, but they do include ‘quantum parallelism’, a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it. The intuitive explanation of these properties places an intolerable strain on all interpretations of quantum theory other than Everett’s. Some of the numerous connections between the quantum theory of computation and the rest of physics are explored. Quantum complexity theory allows a physically more reasonable definition of the ‘complexity’ or ‘knowledge’ in a physical system than does classical complexity theory.},
	number = {1818},
	journal = {Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences},
	author = {Deutsch, D.},
	month = jul,
	year = {1985},
	pages = {97}
}

@article{fitzsimons_unconditionally_2017,
	title = {Unconditionally verifiable blind quantum computation},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.96.012303},
	doi = {10.1103/PhysRevA.96.012303},
	number = {1},
	journal = {Phys. Rev. A},
	author = {Fitzsimons, Joseph F. and Kashefi, Elham},
	month = jul,
	year = {2017},
	pages = {012303}
}

@book{noauthor_qcircuit:_2018,
	title = {qcircuit: {A} quantum circuit drawing application},
	copyright = {GPL-2.0},
	shorttitle = {qcircuit},
	url = {https://github.com/CQuIC/qcircuit},
	urldate = {2018-07-17},
	publisher = {CQuIC},
	month = jul,
	year = {2018},
	annote = {original-date: 2014-06-27T22:20:40Z}
}

@article{bravyi_complexity_2006,
	title = {The {Complexity} of {Stoquastic} {Local} {Hamiltonian} {Problems}},
	url = {http://arxiv.org/abs/quant-ph/0606140},
	abstract = {We study the complexity of the Local Hamiltonian Problem (denoted as LH-MIN) in the special case when a Hamiltonian obeys conditions of the Perron-Frobenius theorem: all off-diagonal matrix elements in the standard basis are real and non-positive. We will call such Hamiltonians, which are common in the natural world, stoquastic. An equivalent characterization of stoquastic Hamiltonians is that they have an entry-wise non-negative Gibbs density matrix for any temperature. We prove that LH-MIN for stoquastic Hamiltonians belongs to the complexity class AM – a probabilistic version of NP with two rounds of communication between the prover and the verifier. We also show that 2-local stoquastic LH-MIN is hard for the class MA. With the additional promise of having a polynomial spectral gap, we show that stoquastic LH-MIN belongs to the class POSTBPP=BPPpath – a generalization of BPP in which a post-selective readout is allowed. This last result also shows that any problem solved by adiabatic quantum computation using stoquastic Hamiltonians lies in PostBPP.},
	urldate = {2018-07-13},
	journal = {arXiv:quant-ph/0606140},
	author = {Bravyi, Sergey and DiVincenzo, David P. and Oliveira, Roberto I. and Terhal, Barbara M.},
	month = jun,
	year = {2006},
	keywords = {Quantum Physics},
	annote = {arXiv: quant-ph/0606140},
	annote = {Comment: 21 pages Latex, 1 figure. v2 contains several small corrections. v3 has more small corrections},
	file = {arXiv\:quant-ph/0606140 PDF:/home/brian/Zotero/storage/TPUSCLL2/Bravyi et al. - 2006 - The Complexity of Stoquastic Local Hamiltonian Pro.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/7QXR2VIX/0606140.html:text/html}
}

@article{wang_divergence_2005,
	title = {Divergence estimation of continuous distributions based on data-dependent partitions},
	volume = {51},
	issn = {0018-9448},
	doi = {10.1109/TIT.2005.853314},
	abstract = {We present a universal estimator of the divergence D(P/spl par/Q) for two arbitrary continuous distributions P and Q satisfying certain regularity conditions. This algorithm, which observes independent and identically distributed (i.i.d.) samples from both P and Q, is based on the estimation of the Radon-Nikodym derivative dP/dQ via a data-dependent partition of the observation space. Strong convergence of this estimator is proved with an empirically equivalent segmentation of the space. This basic estimator is further improved by adaptive partitioning schemes and by bias correction. The application of the algorithms to data with memory is also investigated. In the simulations, we compare our estimators with the direct plug-in estimator and estimators based on other partitioning approaches. Experimental results show that our methods achieve the best convergence performance in most of the tested cases.},
	number = {9},
	journal = {IEEE Transactions on Information Theory},
	author = {Wang, Qing and Kulkarni, S. R. and Verdu, S.},
	month = sep,
	year = {2005},
	keywords = {adaptive partitioning schemes, arbitrary continuous distribution, bias correction, Bias correction, Convergence, data-dependent partition, Density measurement, direct plug-in estimator, divergence, Entropy, Extraterrestrial measurements, information measures, information theory, Information theory, Mutual information, Partitioning algorithms, Pattern recognition, probability, Radon-Nikodym derivative, Radon–Nikodym derivative, Random variables, stationary and ergodic data, Testing, universal divergence estimator, universal estimation of information measures},
	pages = {3064--3074}
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.csail.mit.edu/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	urldate = {2018-07-09},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	month = mar,
	year = {2012},
	pages = {723−773},
	file = {Fulltext PDF:/home/brian/Zotero/storage/S5NHF9L5/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf:application/pdf}
}

@article{ron_property_2008,
	title = {Property {Testing}: {A} {Learning} {Theory} {Perspective}},
	volume = {1},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000004},
	doi = {10.1561/2200000004},
	number = {3},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ron, Dana},
	year = {2008},
	pages = {307--402}
}

@article{goldreich_property_1998,
	title = {Property {Testing} and {Its} {Connection} to {Learning} and {Approximation}},
	volume = {45},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/285055.285060},
	doi = {10.1145/285055.285060},
	number = {4},
	journal = {J. ACM},
	author = {Goldreich, Oded and Goldwasser, Shari and Ron, Dana},
	month = jul,
	year = {1998},
	keywords = {approximation algorithms, computational learning theory, graph algorithms},
	pages = {653--750}
}

@book{jeanfeydy_mmd_2018,
	title = {{MMD}, {Hausdorff} and {Sinkhorn} divergences scaled up to 1,000,000 samples.: jeanfeydy/global-divergences},
	copyright = {MIT},
	shorttitle = {{MMD}, {Hausdorff} and {Sinkhorn} divergences scaled up to 1,000,000 samples.},
	url = {https://github.com/jeanfeydy/global-divergences},
	urldate = {2019-02-06},
	author = {{jeanfeydy}},
	month = nov,
	year = {2018},
	annote = {original-date: 2018-08-15T09:29:01Z}
}

@article{grant_initialization_2019,
	title = {An initialization strategy for addressing barren plateaus in parametrized quantum circuits},
	url = {http://arxiv.org/abs/1903.05076},
	abstract = {In this technical note we propose a theoretically motivated and empirically validated initialization strategy which can resolve the barren plateau problem for practical applications. The proposed strategy allows for efficient training of parametrized quantum circuits. The technique involves randomly selecting some of the initial parameter values, then choosing the remaining values so that the final circuit is a sequence of shallow unitary blocks that each evaluates to the identity. Initializing in this way limits the effective depth of the circuits used to calculate the first parameter update so that they cannot be stuck in a barren plateau at the start of training. We show empirically that circuits initialized using this strategy can be trained using a gradient based method.},
	urldate = {2019-03-26},
	journal = {arXiv:1903.05076 [quant-ph]},
	author = {Grant, Edward and Wossnig, Leonard and Ostaszewski, Mateusz and Benedetti, Marcello},
	month = mar,
	year = {2019},
	keywords = {Quantum Physics},
	annote = {arXiv: 1903.05076},
	file = {arXiv\:1903.05076 PDF:/home/brian/Zotero/storage/RQLYRU2U/Grant et al. - 2019 - An initialization strategy for addressing barren p.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/PZFT5ZQE/1903.html:text/html}
}

@article{killoran_continuous-variable_2018,
	title = {Continuous-variable quantum neural networks},
	url = {http://arxiv.org/abs/1806.06871},
	abstract = {We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks.},
	urldate = {2019-04-01},
	journal = {arXiv:1806.06871 [quant-ph]},
	author = {Killoran, Nathan and Bromley, Thomas R. and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicolás and Lloyd, Seth},
	month = jun,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1806.06871},
	file = {arXiv\:1806.06871 PDF:/home/brian/Zotero/storage/QRGINIFE/Killoran et al. - 2018 - Continuous-variable quantum neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/CA8JDTH7/1806.html:text/html}
}

@article{rocchetto_learning_2018,
	title = {Learning hard quantum distributions with variational autoencoders},
	volume = {4},
	issn = {2056-6387},
	url = {https://doi.org/10.1038/s41534-018-0077-z},
	doi = {10.1038/s41534-018-0077-z},
	abstract = {The exact description of many-body quantum systems represents one of the major challenges in modern physics, because it requires an amount of computational resources that scales exponentially with the size of the system. Simulating the evolution of a state, or even storing its description, rapidly becomes intractable for exact classical algorithms. Recently, machine learning techniques, in the form of restricted Boltzmann machines, have been proposed as a way to efficiently represent certain quantum states with applications in state tomography and ground state estimation. Here, we introduce a practically usable deep architecture for representing and sampling from probability distributions of quantum states. Our representation is based on variational auto-encoders, a type of generative model in the form of a neural network. We show that this model is able to learn efficient representations of states that are easy to simulate classically and can compress states that are not classically tractable. Specifically, we consider the learnability of a class of quantum states introduced by Fefferman and Umans. Such states are provably hard to sample for classical computers, but not for quantum ones, under plausible computational complexity assumptions. The good level of compression achieved for hard states suggests these methods can be suitable for characterizing states of the size expected in first generation quantum hardware.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Rocchetto, Andrea and Grant, Edward and Strelchuk, Sergii and Carleo, Giuseppe and Severini, Simone},
	month = jun,
	year = {2018},
	pages = {28}
}

@book{coyle_project_2018,
	title = {Project / {Dissertation} {Submission} {Index}},
	url = {https://project-archive.inf.ed.ac.uk/msc/2018-outstanding.html},
	abstract = {In this work, we propose a generative quantum machine learning algorithm, which we conjecture is not possible to simulate efficiently by any classical means. We call the algorithm the Ising Born Machine as it generates statistics according to the Born rule of Quantum Mechanics and involves several possible quantum circuit classes which all derive from an Ising Hamiltonian. We recall the currently known proofs of the classical hardness to simulate these circuit classes up to multiplicative error, and make some slight modification such that they are compatible and more useful for the Ising Born Machine construction. Further, we invoke the use of kernel methods as part of the algorithm, and incorporate a kernel function which also claims to demonstrate a quantum classical result, thereby strengthening our claim. Finally, we also present numerical simulations using Rigetti’s Forest quantum simulation platform to investigate and test the ability of the model to learn a simple toy data distribution, with two cost functions, the Kullback Leibler Divergence and the Maximum Mean Discrepancy (MMD).},
	urldate = {2019-04-18},
	author = {Coyle, , Brian},
	month = aug,
	year = {2018},
	file = {Project / Dissertation Submission Index:/home/brian/Zotero/storage/G4CBHRA4/2018-outstanding.html:text/html}
}

@article{zhao_quantum-assisted_2019,
	title = {Quantum-assisted {Gaussian} process regression},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.99.052331},
	doi = {10.1103/PhysRevA.99.052331},
	number = {5},
	journal = {Phys. Rev. A},
	author = {Zhao, Zhikuan and Fitzsimons, Jack K. and Fitzsimons, Joseph F.},
	month = may,
	year = {2019},
	pages = {052331}
}

@article{schuld_quantum_2019,
	title = {Quantum {Machine} {Learning} in {Feature} {Hilbert} {Spaces}},
	volume = {122},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.040504},
	doi = {10.1103/PhysRevLett.122.040504},
	number = {4},
	journal = {Phys. Rev. Lett.},
	author = {Schuld, Maria and Killoran, Nathan},
	month = feb,
	year = {2019},
	pages = {040504}
}

@article{suzuki_analyzing_2019,
	title = {Analyzing feature space via {Pauli} decomposition for quantum classifier},
	url = {http://arxiv.org/abs/1906.10467},
	abstract = {A method for analyzing the feature space used in the quantum classifier on the basis of Pauli decomposition is developed. In particular, for 2-dimensional input datasets, the method boils down to a general formula that easily computes a lower bound of the exact training accuracy, which eventually helps us to see whether the feature space constructed with the selected feature map is suitable for linearly separating the dataset. The effectiveness of this formula is demonstrated, with the special type of five feature maps and four 2-dimensional non-linear datasets.},
	urldate = {2019-07-30},
	journal = {arXiv:1906.10467 [quant-ph]},
	author = {Suzuki, Yudai and Yano, Hiroshi and Gao, Qi and Uno, Shumpei and Tanaka, Tomoki and Akiyama, Manato and Yamamoto, Naoki},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10467},
	keywords = {Quantum Physics},
	annote = {Comment: 13 pages, 9 figures, 4 tables},
	file = {arXiv\:1906.10467 PDF:/home/brian/Zotero/storage/CC2Q9LMK/Suzuki et al. - 2019 - Analyzing feature space via Pauli decomposition fo.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/FVA8AE5D/1906.html:text/html}
}


@article{bouland_quantum_2018,
	title = {Quantum {Supremacy} and the {Complexity} of {Random} {Circuit} {Sampling}},
	url = {http://arxiv.org/abs/1803.04402},
	abstract = {A critical milestone on the path to useful quantum computers is quantum supremacy - a demonstration of a quantum computation that is prohibitively hard for classical computers. A leading near-term candidate, put forth by the Google/UCSB team, is sampling from the probability distributions of randomly chosen quantum circuits, which we call Random Circuit Sampling (RCS). In this paper we study both the hardness and verification of RCS. While RCS was defined with experimental realization in mind, we show complexity theoretic evidence of hardness that is on par with the strongest theoretical proposals for supremacy. Specifically, we show that RCS satisfies an average-case hardness condition - computing output probabilities of typical quantum circuits is as hard as computing them in the worst-case, and therefore \#P-hard. Our reduction exploits the polynomial structure in the output amplitudes of random quantum circuits, enabled by the Feynman path integral. In addition, it follows from known results that RCS satisfies an anti-concentration property, making it the first supremacy proposal with both average-case hardness and anti-concentration.},
	urldate = {2019-01-18},
	journal = {arXiv:1803.04402 [quant-ph]},
	author = {Bouland, Adam and Fefferman, Bill and Nirkhe, Chinmay and Vazirani, Umesh},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Computational Complexity, Quantum Physics},
	annote = {arXiv: 1803.04402},
	file = {arXiv\:1803.04402 PDF:/home/brian/Zotero/storage/GCSV4WEF/Bouland et al. - 2018 - Quantum Supremacy and the Complexity of Random Cir.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/GCY5IA2Y/1803.html:text/html}
}

@article{verdon_learning_2019,
	title = {Learning to learn with quantum neural networks via classical neural networks},
	url = {http://arxiv.org/abs/1907.05415},
	abstract = {Quantum Neural Networks (QNNs) are a promising variational learning paradigm with applications to near-term quantum processors, however they still face some significant challenges. One such challenge is finding good parameter initialization heuristics that ensure rapid and consistent convergence to local minima of the parameterized quantum circuit landscape. In this work, we train classical neural networks to assist in the quantum learning process, also know as meta-learning, to rapidly find approximate optima in the parameter landscape for several classes of quantum variational algorithms. Specifically, we train classical recurrent neural networks to find approximately optimal parameters within a small number of queries of the cost function for the Quantum Approximate Optimization Algorithm (QAOA) for MaxCut, QAOA for Sherrington-Kirkpatrick Ising model, and for a Variational Quantum Eigensolver for the Hubbard model. By initializing other optimizers at parameter values suggested by the classical neural network, we demonstrate a significant improvement in the total number of optimization iterations required to reach a given accuracy. We further demonstrate that the optimization strategies learned by the neural network generalize well across a range of problem instance sizes. This opens up the possibility of training on small, classically simulatable problem instances, in order to initialize larger, classically intractably simulatable problem instances on quantum devices, thereby significantly reducing the number of required quantum-classical optimization iterations.},
	urldate = {2019-08-02},
	journal = {arXiv:1907.05415 [quant-ph]},
	author = {Verdon, Guillaume and Broughton, Michael and McClean, Jarrod R. and Sung, Kevin J. and Babbush, Ryan and Jiang, Zhang and Neven, Hartmut and Mohseni, Masoud},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05415},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	annote = {Comment: 12 pages, 4 figures}
}