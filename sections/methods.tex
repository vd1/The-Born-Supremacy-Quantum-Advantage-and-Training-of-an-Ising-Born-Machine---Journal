% !TEX root = ../main.tex
\section*{Methods}\label{sec:methods}
In this section, we detail the methods used to train the $\IBM$ to reproduce a given probability distribution. The target distribution is the one given by \eqref{toydatadistribution}, which is used in both refs.\cite{amin_quantum_2018, verdon_quantum_2017} to train versions of the quantum Boltzmann machine:
\begin{align}
    \pi(\mathbf{y}) \coloneqq \frac{1}{T}\sum\limits_{k=1}^T 
    p^{n - d_H(s_k, \mathbf{y})}
    (1-p)^{d_H(s_k, \mathbf{y})}
\label{toydatadistribution}
\end{align}

\noindent To generate this data, $T$ %`Bernoulli' 
binary strings of length $n$, written $s_k$ and called `modes',  
are chosen randomly. 
%Each mode ($k$) is randomly chosen to be a . 
%$\mathbf{s}^k = [s^k_1, \dots, s^k_n]$, 
A sample $\mathbf{y}$ is produced with a probability which depends on its Hamming distance $d_H(s_k, \mathbf{y})$ to each mode. In all of the above, the Adam \cite{kingma_adam:_2014} optimiser was applied, using the hyperparameters suggested in that paper, i.e.\@ $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 1\times 10^{-8}$, and initial learning rate, $\eta_{\mathsf{init}}$. This was chosen since it was found to be more robust to sampling noise \cite{liu_differentiable_2018}.

In all of the numerical results, we used a $\QAOA$ structure as the underlying circuit in the $\IBM$. Specifically, the parameters in $U_f$ were chosen such that $\forall k, \Gamma_k = \pi/4, \Delta_k = 0, \Sigma_k = 0$. The Ising parameters, $\{J_{ij}, b_k\}$ were initialised randomly. 

For the Stein discrepancy, we used three Nystr\"{o}m eigenvectors to approximate the Spectral score in \figref{fig:MMDvSinkvStein3} for three qubits, and 6 eigenvectors for 4 qubits. In all cases when using the $\MMD$ with a Gaussian kernel, we chose the bandwidth parameters, $\sigma = [0.25, 10, 1000]$\cite{liu_differentiable_2018}.

