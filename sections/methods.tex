\section*{Methods}\label{sec:methods}
In this section, we detail the methods used to train the $\IBM$ to reproduce a given probability distribution. The target distribution is the one given by \eqref{toydatadistribution}, which is used in both refs.\cite{amin_quantum_2018, verdon_quantum_2017} to train versions of the quantum Boltzmann machine:
\begin{align}
    \pi(\mathbf{y}) \coloneqq \frac{1}{T}\sum\limits_{k=1}^T p^{N - d_H^{(k, \mathbf{y})}}(1-p)^{d_H^{(k, \mathbf{y})}} \label{toydatadistribution}
\end{align}

\noindent To generate this data, $T$ `Bernoulli' hidden modes are randomly chosen. Each mode ($k$) is randomly chosen to be a binary string, $\mathbf{s}^k = [s^k_1, \dots, s^k_n]$, and a given sample, $\mathbf{y}$, is produced with a probability which depends on the Hamming distance, $d_H^{(k, \mathbf{y})}$, between each mode string, $k$, and that sample. In all of the above, the Adam \cite{kingma_adam:_2014} optimiser was applied, using the hyperparameters suggested in that paper, i.e.\@ $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 1\times 10^{-8}$, and initial learning rate, $\eta_{\mathsf{init}}$. This was chosen since it was found to be more robust to sampling noise \cite{liu_differentiable_2018}.

In all of the numerical results, we used a $\QAOA$ structure as the underlying circuit in the $\IBM$. Specifically, the parameters in $U_f$ were chosen such that $\forall k, \Gamma_k = \pi/4, \Delta_k = 0, \Sigma_k = 0$. The Ising parameters, $\{J_{ij}, b_k\}$ were initialised randomly. 

For the Stein discrepancy, we used 3 Nyst\"{o}m eigenvectors to approximate the Spectral score in \figref{fig:MMDvSinkvStein3} for three qubits, and 6 eigenvectors for 4 qubits. In all cases when using the $\MMD$ with a Gaussian kernel, we chose the bandwidth parameters, $\sigma = [0.25, 10, 1000]$\cite{liu_differentiable_2018}.

