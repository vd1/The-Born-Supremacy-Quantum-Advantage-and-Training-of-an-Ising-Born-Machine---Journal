% !TEX root = ../main.tex
\section*{Introduction} \label{sec:intro}

As fault intolerant quantum devices, with $\sim 80-200$ qubits, begin to be built, we near the dawn of the Noisy Intermediate Scale Quantum (NISQ) \cite{preskill_quantum_2018} technology era. These devices will not be able to perform many of the most famous algorithms thought to demonstrate exponential speedups over classical algorithms \cite{shor_polynomial-time_1997, harrow_quantum_2009}, due to few qubits with limited connectivity. However, these devices could provide an efficient solution to other problems which cannot be solved in polynomial time by purely classical means. 
Showing this to be true is referred to as a demonstration of 
\textit{quantum computational supremacy}\cite{bremner_classical_2011, gao_quantum_2017, bremner_average-case_2016,  aaronson_computational_2013, farhi_quantum_2016, boixo_characterizing_2018} 
which involves sampling from the output distribution of a quantum computation.
While a demonstration of such an advantage is of great theoretical importance, generating random samples is not a particularly exciting application by itself. We endeavour to incorporate this sampling into a \textit{useful} application in such a way to keep the provable quantum advantage, but in a context with more practical applicability. For this, we turn to generative modelling in quantum machine learning (QML). Generative modelling is the task of training an algorithm to \textit{generalise} from a finite set of samples, $\{\mathbf{y}\}^M$, drawn from a data set, by learning the underlying probability distribution from which these samples are drawn, $\pi(\mathbf{y})$, such that the model is able to generate new samples from the target distribution itself. Generative models can range from being simple, as in Na{\"i}ve Bayes\cite{maron_automatic_1961}, to complicated neural networks, like generative adversarial networks (GANs)\cite{goodfellow_generative_2014}. However, the intrinsic randomness inherent in quantum mechanics allows for the definition of a new class of generative models, which have no classical analogue. Generally known as \textit{Born machines}\cite{cheng_information_2017, liu_differentiable_2018}, their core operating principle is the ability to produce statistics, according to Born's measurement rule. Specifically, for a state $\ket{\psi}$, a measurement results in a sample $\mathbf{x} \sim p(\mathbf{x}) = |\braket{\mathbf{x}|\psi}|^2 \label{bornrule}$. Since their inception, several works have defined variants, including Bayesian approaches\cite{du_expressive_2018}, 
adversarial training methods \cite{zeng_learning_2018}, and adaptations to continuous distributions\cite{romero_variational_2019}. Born machines form a subclass of parametrized  
quantum circuits (PQC), which have found many applications recently -see ref. \cite{benedetti_parameterized_2019} for a review. PQCs consist of a quantum circuit which carries parameters that are updated during a training process (typically a classical optimisation routine). The quantum element of a PQC is minimal; the circuit is kept as shallow as possible with few gates to be suitable for NISQ devices. The question we address here is the following. Is it possible to have a machine learning application for a PQC, which comes with a \textit{provable} superior performance over all classical alternatives on near term devices? Such provable guarantees are even more relevant given recent work in QML algorithm `dequantisations' \cite{tang_quantum-inspired_2018, tang_quantum-inspired_2018-1, andoni_solving_2018, chia_quantum-inspired_2018, gilyen_quantum-inspired_2018}. 

In this work, we take the first steps to answer this question in several ways. We define a subclass of Born machines which we dub \emph{Ising Born machines} ($\IBM$s). We improve the training of the model over previous methods (using the maximum mean discrepancy\cite{liu_differentiable_2018} ($\MMD$) with a classical kernel) by introducing new gradient-based techniques. We introduce quantum kernels in the $\MMD$, and also use new cost functions altogether: the Stein discrepancy and the Sinkhorn divergence. To do so we derive their corresponding gradients in a quantum context. We show that these three novel methods outperform the 
$\MMD$-with-classical-kernel in training by achieving a closer fit to the data relative to the total variation distance. The latter cost function is especially interesting. We use sample complexity results to derive an optimal form of it, which is as efficient to compute as possible. Next, we show that sampling from this model can not be simulated efficiently by any classical randomised algorithm, up to multiplicative error in the worst case, subject to assumptions in complexity theory (strict polynomial hierarchy). Furthermore, this holds for all circuit families encountered during training. We define a framework in which a provable outperformance could be demonstrated, which we refer to as quantum learning supremacy, based on distribution learning theory\cite{kearns_learnability_1994}. Based on the classical sampling hardness results that we show, the $\IBM$ is a good candidate for a quantum model which could demonstrate this notion of learning supremacy. Finally, we provide a novel utilisation of such generative models in quantum circuit compilation. Background information and additional results and derivations can be found in the Supplementary Material.



