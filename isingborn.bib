
@article{schuld_quantum_2019,
	title = {Quantum {Machine} {Learning} in {Feature} {Hilbert} {Spaces}},
	volume = {122},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.122.040504},
	doi = {10.1103/PhysRevLett.122.040504},
	number = {4},
	journal = {Phys. Rev. Lett.},
	author = {Schuld, Maria and Killoran, Nathan},
	month = feb,
	year = {2019},
	pages = {040504}
}

@article{lloyd_quantum_1999,
	title = {Quantum {Computation} over {Continuous} {Variables}},
	volume = {82},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.82.1784},
	doi = {10.1103/PhysRevLett.82.1784},
	abstract = {This paper provides necessary and sufficient conditions for constructing a universal quantum computer over continuous variables. As an example, it is shown how a universal quantum computer for the amplitudes of the electromagnetic field might be constructed using simple linear devices such as beam splitters and phase shifters, together with squeezers and nonlinear devices such as Kerr-effect fibers and atoms in optical cavities. Such a device could in principle perform “quantum floating point” computations. Problems involving noise, finite precision, and error correction are discussed.},
	number = {8},
	urldate = {2018-07-09},
	journal = {Physical Review Letters},
	author = {Lloyd, Seth and Braunstein, Samuel L.},
	month = feb,
	year = {1999},
	pages = {1784--1787},
	file = {APS Snapshot:/home/brian/Zotero/storage/3IIQZ2B3/PhysRevLett.82.html:text/html}
}

@article{bellemare_cramer_2017,
	title = {The {Cramer} {Distance} as a {Solution} to {Biased} {Wasserstein} {Gradients}},
	url = {http://arxiv.org/abs/1705.10743},
	abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram{\textbackslash}textbackslash'er distance. We show that the Cram{\textbackslash}textbackslash'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram{\textbackslash}textbackslash'er distance in practice we design a new algorithm, the Cram{\textbackslash}textbackslash'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
	urldate = {2018-07-06},
	journal = {arXiv:1705.10743 [cs, stat]},
	author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
	month = may,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1705.10743},
	file = {arXiv\:1705.10743 PDF:/home/brian/Zotero/storage/6A4VA4WF/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/4YN9ZT3K/1705.html:text/html}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2018-07-06},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jan,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1701.07875},
	file = {arXiv\:1701.07875 PDF:/home/brian/Zotero/storage/J9MYUFJA/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/MVEQTS73/1701.html:text/html}
}

@book{bishop_pattern_2006,
	address = {Berlin, Heidelberg},
	title = {Pattern {Recognition} and {Machine} {Learning} ({Information} {Science} and {Statistics})},
	isbn = {0-387-31073-8},
	publisher = {Springer-Verlag},
	author = {Bishop, Christopher M.},
	year = {2006}
}

@book{mackay_information_2002,
	address = {New York, NY, USA},
	title = {Information {Theory}, {Inference} \& {Learning} {Algorithms}},
	isbn = {0-521-64298-1},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	year = {2002}
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	url = {https://doi.org/10.1214/aoms/1177729694},
	doi = {10.1214/aoms/1177729694},
	number = {1},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	year = {1951},
	pages = {79--86}
}

@article{chen_equivalence_2018,
	title = {Equivalence of restricted {Boltzmann} machines and tensor network states},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.085104},
	doi = {10.1103/PhysRevB.97.085104},
	number = {8},
	journal = {Phys. Rev. B},
	author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
	month = feb,
	year = {2018},
	pages = {085104}
}

@article{han_unsupervised_2017,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	url = {http://arxiv.org/abs/1709.01662},
	abstract = {Generative modeling, which learns joint probability distribution from training data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning by utilizing the density matrix renormalization group method which allows dynamic adjusting dimensions of the tensors, and offers an efficient direct sampling approach, Zipper, for generative tasks. We apply our method to generative modeling of several standard datasets including the principled Bars and Stripes, random binary patterns and the MNIST handwritten digits, to illustrate ability of our model, and discuss features as well as drawbacks of our model over popular generative models such as Hopfield model, Boltzmann machines and generative adversarial networks. Our work shed light on many interesting directions for future exploration on the development of quantum-inspired algorithms for unsupervised machine learning, which is of possibility of being realized by a quantum device.},
	language = {en},
	urldate = {2018-07-03},
	journal = {arXiv:1709.01662 [cond-mat, physics:quant-ph, stat]},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = sep,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics},
	annote = {arXiv: 1709.01662},
	annote = {Comment: 11 pages, 12 figures (not including the TNs) GitHub Page: https://github.com/congzlwag/UnsupGenModbyMPS},
	file = {Han et al. - 2017 - Unsupervised Generative Modeling Using Matrix Prod.pdf:/home/brian/Zotero/storage/MKQ4WPPU/Han et al. - 2017 - Unsupervised Generative Modeling Using Matrix Prod.pdf:application/pdf}
}

@article{huang_provably_2017,
	title = {Provably efficient neural network representation for image classification},
	url = {http://arxiv.org/abs/1711.04606},
	abstract = {The state-of-the-art approaches for image classification are based on neural networks. Mathematically, the task of classifying images is equivalent to finding the function that maps an image to the label it is associated with. To rigorously establish the success of neural network methods, we should first prove that the function has an efficient neural network representation, and then design provably efficient training algorithms to find such a representation. Here, we achieve the first goal based on a set of assumptions about the patterns in the images. The validity of these assumptions is very intuitive in many image classification problems, including but not limited to, recognizing handwritten digits.},
	urldate = {2018-07-03},
	journal = {arXiv:1711.04606 [cs]},
	author = {Huang, Yichen},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv: 1711.04606},
	file = {arXiv\:1711.04606 PDF:/home/brian/Zotero/storage/2LM2XDSC/Huang - 2017 - Provably efficient neural network representation f.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/MM4JZIS7/1711.html:text/html}
}

@article{gao_efficient_2017,
	title = {An efficient quantum algorithm for generative machine learning},
	url = {http://arxiv.org/abs/1711.02038},
	abstract = {A central task in the field of quantum computing is to find applications where quantum computer could provide exponential speedup over any classical computer. Machine learning represents an important field with broad applications where quantum computer may offer significant speedup. Several quantum algorithms for discriminative machine learning have been found based on efficient solving of linear algebraic problems, with potential exponential speedup in runtime under the assumption of effective input from a quantum random access memory. In machine learning, generative models represent another large class which is widely used for both supervised and unsupervised learning. Here, we propose an efficient quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is exponentially more powerful to represent probability distributions compared with classical generative models and has exponential speedup in training and inference at least for some instances under a reasonable assumption in computational complexity theory. Our result opens a new direction for quantum machine learning and offers a remarkable example in which a quantum algorithm shows exponential improvement over any classical algorithm in an important application field.},
	urldate = {2018-07-03},
	journal = {arXiv:1711.02038 [quant-ph, stat]},
	author = {Gao, Xun and Zhang, Zhengyu and Duan, Luming},
	month = nov,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1711.02038},
	annote = {Comment: 7+15 pages, 3+6 figures},
	file = {arXiv\:1711.02038 PDF:/home/brian/Zotero/storage/KYNREH5Z/Gao et al. - 2017 - An efficient quantum algorithm for generative mach.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/5YIX6M6K/1711.html:text/html}
}

@article{pestun_tensor_2017,
	title = {Tensor network language model},
	url = {http://arxiv.org/abs/1710.10248},
	abstract = {We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.10248 [cond-mat, stat]},
	author = {Pestun, Vasily and Vlassopoulos, Yiannis},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {arXiv: 1710.10248},
	annote = {Comment: 21 pages},
	file = {arXiv\:1710.10248 PDF:/home/brian/Zotero/storage/BYDWJYEZ/Pestun and Vlassopoulos - 2017 - Tensor network language model.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/HHEH6ESQ/1710.html:text/html}
}

@article{zhang_entanglement_2017,
	title = {Entanglement {Entropy} of {Target} {Functions} for {Image} {Classification} and {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1710.05520},
	abstract = {The success of deep convolutional neural network (CNN) in computer vision especially image classification problems requests a new information theory for function of image, instead of image itself. In this article, after establishing a deep mathematical connection between image classification problem and quantum spin model, we propose to use entanglement entropy, a generalization of classical Boltzmann-Shannon entropy, as a powerful tool to characterize the information needed for representation of general function of image. We prove that there is a sub-volume-law bound for entanglement entropy of target functions of reasonable image classification problems. Therefore target functions of image classification only occupy a small subspace of the whole Hilbert space. As a result, a neural network with polynomial number of parameters is efficient for representation of such target functions of image. The concept of entanglement entropy can also be useful to characterize the expressive power of different neural networks. For example, we show that to maintain the same expressive power, number of channels \$D\$ in a convolutional neural network should scale with the number of convolution layers \$n\_c\$ as \$D{\textbackslash}textbackslashsim D\_0ˆ\{{\textbackslash}textbackslashfrac\{1\}\{n\_c\}\}\$. Therefore, deeper CNN with large \$n\_c\$ is more efficient than shallow ones.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.05520 [cond-mat]},
	author = {Zhang, Ya-Hui},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Strongly Correlated Electrons, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1710.05520},
	annote = {Comment: 9pages, 1 figures},
	file = {arXiv\:1710.05520 PDF:/home/brian/Zotero/storage/RY9NWS2Y/Zhang - 2017 - Entanglement Entropy of Target Functions for Image.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/DFXUTNTG/1710.html:text/html}
}

@article{liu_machine_2017,
	title = {Machine {Learning} by {Two}-{Dimensional} {Hierarchical} {Tensor} {Networks}: {A} {Quantum} {Information} {Theoretic} {Perspective} on {Deep} {Architectures}},
	shorttitle = {Machine {Learning} by {Two}-{Dimensional} {Hierarchical} {Tensor} {Networks}},
	url = {http://arxiv.org/abs/1710.04833},
	abstract = {The resemblance between the tensor networks (TNs) and machine learning has drawn considerable attention. In particular, TNs and deep learning architectures bear striking similarities suggesting using quantum techniques for machine learning. In this work, we train two-dimensional hierarchical TNs to solve image recognition problems, using a training algorithm derived from the multipartite entanglement renormalization ansatz. This approach overcomes scalability issues and implies novel mathematical connections among quantum many-body physics, quantum information theory, and machine learning. The algorithm optimally encodes each image class into a TN state, so that the learning tasks as well as the image classes can be characterized by quantum properties of the state including quantum entanglement and fidelity. Furthermore, the unitary conditions of the local mappings in our algorithm make it possible to realize the machine learning by, e.g., quantum state tomography techniques or quantum computations.},
	urldate = {2018-07-03},
	journal = {arXiv:1710.04833 [cond-mat, physics:physics, physics:quant-ph, stat]},
	author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and García, Raul Blázquez and Su, Gang and Lewenstein, Maciej},
	month = oct,
	year = {2017},
	keywords = {Quantum Physics, Statistics - Machine Learning, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics},
	annote = {arXiv: 1710.04833},
	annote = {Comment: 6 pages, 5 figures},
	file = {arXiv\:1710.04833 PDF:/home/brian/Zotero/storage/GLU62SIB/Liu et al. - 2017 - Machine Learning by Two-Dimensional Hierarchical T.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/E2H8CU79/1710.html:text/html}
}

@article{levine_deep_2017,
	title = {Deep {Learning} and {Quantum} {Entanglement}: {Fundamental} {Connections} with {Implications} to {Network} {Design}},
	shorttitle = {Deep {Learning} and {Quantum} {Entanglement}},
	url = {http://arxiv.org/abs/1704.01552},
	abstract = {Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical observations regarding the role that the number of channels in each layer of the convolutional network fulfills in the overall inductive bias. Specifically, we show an equivalence between the function realized by a deep convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which relies on their common underlying tensorial structure. This facilitates the use of quantum entanglement measures as well-defined quantifiers of a deep network's expressive ability to model intricate correlation structures of its inputs. Most importantly, the construction of a deep ConvAC in terms of a Tensor Network is made available. This description enables us to carry a graph-theoretic analysis of a convolutional network, with which we demonstrate a direct control over the inductive bias of the deep network via its channel numbers, that are related to the min-cut in the underlying graph. This result is relevant to any practitioner designing a network for a specific task. We theoretically analyze ConvACs, and empirically validate our findings on more common ConvNets which involve ReLU activations and max pooling. Beyond the results described above, the description of a deep convolutional network in well-defined graph-theoretic tools and the formal connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work.},
	urldate = {2018-07-03},
	journal = {arXiv:1704.01552 [quant-ph]},
	author = {Levine, Yoav and Yakira, David and Cohen, Nadav and Shashua, Amnon},
	month = apr,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1704.01552},
	file = {arXiv\:1704.01552 PDF:/home/brian/Zotero/storage/TVND2UN3/Levine et al. - 2017 - Deep Learning and Quantum Entanglement Fundamenta.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/52FBKJ9K/1704.html:text/html}
}

@article{dickson_thermally_2013,
	title = {Thermally assisted quantum annealing of a 16-qubit problem},
	volume = {4},
	url = {http://dx.doi.org/10.1038/ncomms2920},
	journal = {Nature Communications},
	author = {Dickson, N G and Johnson, M W and Amin, M H and Harris, R and Altomare, F and Berkley, A J and Bunyk, P and Cai, J and Chapple, E M and Chavez, P and Cioata, F and Cirip, T and deBuen, P and Drew-Brook, M and Enderud, C and Gildert, S and Hamze, F and Hilton, J P and Hoskinson, E and Karimi, K and Ladizinsky, E and Ladizinsky, N and Lanting, T and Mahon, T and Neufeld, R and Oh, T and Perminov, I and Petroff, C and Przybysz, A and Rich, C and Spear, P and Tcaciuc, A and Thom, M C and Tolkacheva, E and Uchaikin, S and Wang, J and Wilson, A B and Merali, Z and Rose, G},
	month = may,
	year = {2013},
	pages = {1903}
}

@article{benedetti_estimation_2016,
	title = {Estimation of effective temperatures in quantum annealers for sampling applications: {A} case study with possible applications in deep learning},
	volume = {94},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.94.022308},
	doi = {10.1103/PhysRevA.94.022308},
	number = {2},
	journal = {Phys. Rev. A},
	author = {Benedetti, Marcello and Realpe-Gómez, John and Biswas, Rupak and Perdomo-Ortiz, Alejandro},
	month = aug,
	year = {2016},
	pages = {022308}
}

@article{adachi_application_2015,
	title = {Application of {Quantum} {Annealing} to {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1510.06356},
	abstract = {In Deep Learning, a well-known approach for training a Deep Neural Network starts by training a generative Deep Belief Network model, typically using Contrastive Divergence (CD), then fine-tuning the weights using backpropagation or other discriminative techniques. However, the generative training can be time-consuming due to the slow mixing of Gibbs sampling. We investigated an alternative approach that estimates model expectations of Restricted Boltzmann Machines using samples from a D-Wave quantum annealing machine. We tested this method on a coarse-grained version of the MNIST data set. In our tests we found that the quantum sampling-based training approach achieves comparable or better accuracy with significantly fewer iterations of generative training than conventional CD-based training. Further investigation is needed to determine whether similar improvements can be achieved for other data sets, and to what extent these improvements can be attributed to quantum effects.},
	urldate = {2018-07-03},
	journal = {arXiv:1510.06356 [quant-ph, stat]},
	author = {Adachi, Steven H. and Henderson, Maxwell P.},
	month = oct,
	year = {2015},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1510.06356},
	annote = {Comment: 18 pages},
	file = {arXiv\:1510.06356 PDF:/home/brian/Zotero/storage/IHE4G522/Adachi and Henderson - 2015 - Application of Quantum Annealing to Training of De.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/YTGUBYR6/1510.html:text/html}
}

@article{kadowaki_quantum_1998,
	title = {Quantum annealing in the transverse {Ising} model},
	volume = {58},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.58.5355},
	doi = {10.1103/PhysRevE.58.5355},
	number = {5},
	journal = {Phys. Rev. E},
	author = {Kadowaki, Tadashi and Nishimori, Hidetoshi},
	month = nov,
	year = {1998},
	pages = {5355--5363}
}

@article{ackley_learning_1985,
	title = {A learning algorithm for boltzmann machines},
	volume = {9},
	issn = {0364-0213},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	doi = {10.1016/S0364-0213(85)80012-4},
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	number = {1},
	journal = {Cognitive Science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	month = jan,
	year = {1985},
	pages = {147--169}
}

@article{giovannetti_quantum_2008,
	title = {Quantum {Random} {Access} {Memory}},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.160501},
	doi = {10.1103/PhysRevLett.100.160501},
	number = {16},
	journal = {Phys. Rev. Lett.},
	author = {Giovannetti, Vittorio and Lloyd, Seth and Maccone, Lorenzo},
	month = apr,
	year = {2008},
	pages = {160501}
}

@article{aaronson_read_2015,
	title = {Read the fine print},
	volume = {11},
	url = {http://dx.doi.org/10.1038/nphys3272},
	journal = {Nature Physics},
	author = {Aaronson, Scott},
	month = apr,
	year = {2015},
	pages = {291}
}

@article{georgescu_quantum_2014,
	title = {Quantum simulation},
	volume = {86},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.86.153},
	doi = {10.1103/RevModPhys.86.153},
	number = {1},
	journal = {Rev. Mod. Phys.},
	author = {Georgescu, I. M. and Ashhab, S. and Nori, Franco},
	month = mar,
	year = {2014},
	pages = {153--185}
}

@article{feynman_simulating_1982,
	title = {Simulating {Physics} with {Computers}},
	volume = {21},
	issn = {0020-7748},
	url = {http://adsabs.harvard.edu/abs/1982IJTP...21..467F},
	doi = {10.1007/BF02650179},
	abstract = {Not Available},
	urldate = {2018-07-02},
	journal = {International Journal of Theoretical Physics},
	author = {Feynman, Richard P.},
	month = jun,
	year = {1982},
	pages = {467--488}
}

@article{farhi_quantum_2014,
	title = {A {Quantum} {Approximate} {Optimization} {Algorithm}},
	url = {http://arxiv.org/abs/1411.4028},
	abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
	urldate = {2018-07-02},
	journal = {arXiv:1411.4028 [quant-ph]},
	author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
	month = nov,
	year = {2014},
	keywords = {Quantum Physics},
	annote = {arXiv: 1411.4028},
	file = {arXiv\:1411.4028 PDF:/home/brian/Zotero/storage/SPU7T299/Farhi et al. - 2014 - A Quantum Approximate Optimization Algorithm.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/8KQLNAE3/1411.html:text/html}
}

@article{fujii_impossibility_2018,
	title = {Impossibility of {Classically} {Simulating} {One}-{Clean}-{Qubit} {Model} with {Multiplicative} {Error}},
	volume = {120},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.200502},
	doi = {10.1103/PhysRevLett.120.200502},
	number = {20},
	journal = {Phys. Rev. Lett.},
	author = {Fujii, Keisuke and Kobayashi, Hirotada and Morimae, Tomoyuki and Nishimura, Harumichi and Tamate, Shuhei and Tani, Seiichiro},
	month = may,
	year = {2018},
	pages = {200502}
}

@article{terhal_adaptive_2002,
	title = {Adaptive {Quantum} {Computation}, {Constant} {Depth} {Quantum} {Circuits} and {Arthur}-{Merlin} {Games}},
	url = {http://arxiv.org/abs/quant-ph/0205133},
	abstract = {We present evidence that there exist quantum computations that can be carried out in constant depth, using 2-qubit gates, that cannot be simulated classically with high accuracy. We prove that if one can simulate these circuits classically efficiently then the complexity class BQP is contained in AM.},
	urldate = {2018-07-02},
	journal = {arXiv:quant-ph/0205133},
	author = {Terhal, Barbara M. and DiVincenzo, David P.},
	month = may,
	year = {2002},
	keywords = {Quantum Physics, Computer Science - Computational Complexity},
	annote = {arXiv: quant-ph/0205133},
	annote = {Comment: Substantially revised version, 13 pages, 1 figure, to appear in QIC. Small correction in Theorem 1 added},
	file = {arXiv\:quant-ph/0205133 PDF:/home/brian/Zotero/storage/FBNH2LVT/Terhal and DiVincenzo - 2002 - Adaptive Quantum Computation, Constant Depth Quant.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/LCFFL4U6/0205133.html:text/html}
}

@article{bremner_average-case_2016,
	title = {Average-{Case} {Complexity} {Versus} {Approximate} {Simulation} of {Commuting} {Quantum} {Computations}},
	volume = {117},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.117.080501},
	doi = {10.1103/PhysRevLett.117.080501},
	number = {8},
	journal = {Phys. Rev. Lett.},
	author = {Bremner, Michael J. and Montanaro, Ashley and Shepherd, Dan J.},
	month = aug,
	year = {2016},
	pages = {080501}
}

@article{shepherd_temporally_2009,
	title = {Temporally unstructured quantum computation},
	url = {http://rspa.royalsocietypublishing.org/content/early/2009/02/18/rspa.2008.0443.abstract},
	doi = {10.1098/rspa.2008.0443},
	abstract = {We examine theoretic architectures and an abstract model for a restricted class of quantum computation, called here temporally unstructured (‘instantaneous’) quantum computation because it allows for essentially no temporal structure within the quantum dynamics. Using the theory of binary matroids, we argue that the paradigm is rich enough to enable sampling from probability distributions that cannot, classically, be sampled efficiently and accurately. This paradigm also admits simple interactive proof games that may convince a sceptic of the existence of truly quantum effects. Furthermore, these effects can be created using significantly fewer qubits than are required for running Shor's algorithm.},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
	author = {Shepherd, Dan and Bremner, Michael J},
	month = jan,
	year = {2009}
}

@incollection{hinton_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Training} {Restricted} {Boltzmann} {Machines}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
	language = {en},
	urldate = {2018-06-11},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hinton, Geoffrey E.},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_32},
	pages = {599--619},
	file = {Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:/home/brian/Zotero/storage/QTWUSDP7/Hinton - 2012 - A Practical Guide to Training Restricted Boltzmann.pdf:application/pdf}
}

@article{wiebe_quantum_2015,
	title = {Quantum {Inspired} {Training} for {Boltzmann} {Machines}},
	url = {http://arxiv.org/abs/1507.02642},
	abstract = {We present an efficient classical algorithm for training deep Boltzmann machines (DBMs) that uses rejection sampling in concert with variational approximations to estimate the gradients of the training objective function. Our algorithm is inspired by a recent quantum algorithm for training DBMs. We obtain rigorous bounds on the errors in the approximate gradients; in turn, we find that choosing the instrumental distribution to minimize the alpha=2 divergence with the Gibbs state minimizes the asymptotic algorithmic complexity. Our rejection sampling approach can yield more accurate gradients than low-order contrastive divergence training and the costs incurred in finding increasingly accurate gradients can be easily parallelized. Finally our algorithm can train full Boltzmann machines and scales more favorably with the number of layers in a DBM than greedy contrastive divergence training.},
	urldate = {2018-06-11},
	journal = {arXiv:1507.02642 [quant-ph]},
	author = {Wiebe, Nathan and Kapoor, Ashish and Granade, Christopher and Svore, Krysta M.},
	month = jul,
	year = {2015},
	keywords = {Quantum Physics, Computer Science - Learning},
	annote = {arXiv: 1507.02642},
	file = {arXiv\:1507.02642 PDF:/home/brian/Zotero/storage/IBJDVQ9J/Wiebe et al. - 2015 - Quantum Inspired Training for Boltzmann Machines.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/2Q26XWF8/1507.html:text/html}
}

@article{amin_quantum_2018,
	title = {Quantum {Boltzmann} {Machine}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.021050},
	doi = {10.1103/PhysRevX.8.021050},
	abstract = {Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, we propose a new machine-learning approach based on quantum Boltzmann distribution of a quantum Hamiltonian. Because of the noncommutative nature of quantum mechanics, the training process of the quantum Boltzmann machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors for QBM training and application.},
	number = {2},
	urldate = {2018-06-11},
	journal = {Physical Review X},
	author = {Amin, Mohammad H. and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
	month = may,
	year = {2018},
	pages = {021050},
	file = {APS Snapshot:/home/brian/Zotero/storage/CKBK89SS/PhysRevX.8.html:text/html;Full Text PDF:/home/brian/Zotero/storage/QHBZW9J9/Amin et al. - 2018 - Quantum Boltzmann Machine.pdf:application/pdf}
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.6076},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	number = {5},
	urldate = {2018-06-08},
	journal = {The Annals of Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {2263--2291},
	annote = {arXiv: 1207.6076},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/13-AOS1140 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv\:1207.6076 PDF:/home/brian/Zotero/storage/K45FBGRK/Sejdinovic et al. - 2013 - Equivalence of distance-based and RKHS-based stati.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/C9CJR2J7/1207.html:text/html}
}

@article{sriperumbudur_integral_2009,
	title = {On integral probability metrics, {\textbackslash}textbackslashphi-divergences and binary classification},
	url = {http://arxiv.org/abs/0901.2698},
	abstract = {A class of distance measures on probabilities – the integral probability metrics (IPMs) – is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. IPMs have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures defined on a metric space. Practical applications of IPMs are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of IPMs, which should contribute to making IPMs more widely used in practice, for instance in areas where \${\textbackslash}textbackslashphi\$-divergences are currently popular. First, to understand the relation between IPMs and \${\textbackslash}textbackslashphi\$-divergences, the necessary and sufficient conditions under which these classes intersect are derived: the total variation distance is shown to be the only non-trivial \${\textbackslash}textbackslashphi\$-divergence that is also an IPM. This shows that IPMs are essentially different from \${\textbackslash}textbackslashphi\$-divergences. Second, empirical estimates of several IPMs from finite i.i.d. samples are obtained, and their consistency and convergence rates are analyzed. These estimators are shown to be easily computable, with better rates of convergence than estimators of \${\textbackslash}textbackslashphi\$-divergences. Third, a novel interpretation is provided for IPMs by relating them to binary classification, where it is shown that the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier. In addition, the smoothness of an appropriate binary classifier is proved to be inversely related to the distance between the class-conditional distributions, measured in terms of an IPM.},
	urldate = {2018-06-08},
	journal = {arXiv:0901.2698 [cs, math]},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	month = jan,
	year = {2009},
	keywords = {Computer Science - Information Theory},
	annote = {arXiv: 0901.2698},
	annote = {Comment: 18 pages},
	file = {arXiv\:0901.2698 PDF:/home/brian/Zotero/storage/TSCHRZWQ/Sriperumbudur et al. - 2009 - On integral probability metrics, phi-divergences .pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/PT8BXWE4/0901.html:text/html}
}

@article{benedetti_adversarial_2018,
	title = {Adversarial quantum circuit learning for pure state approximation},
	url = {http://arxiv.org/abs/1806.00463},
	abstract = {Adversarial learning is one of the most successful approaches to modelling high-dimensional probability distributions from data. The quantum computing community has recently begun to generalize this idea and to look for potential applications. In this work, we derive an adversarial algorithm for the problem of approximating an unknown quantum pure state. Although this could be done on error-corrected quantum computers, the adversarial formulation enables us to execute the algorithm on near-term quantum computers. Two ansatz circuits are optimized in tandem: One tries to approximate the target state, the other tries to distinguish between target and approximated state. Supported by numerical simulations, we show that resilient backpropagation algorithms perform remarkably well in optimizing the two circuits. We use the bipartite entanglement entropy to design an efficient heuristic for the stopping criteria. Our approach may find application in quantum state tomography.},
	urldate = {2018-06-08},
	journal = {arXiv:1806.00463 [quant-ph, stat]},
	author = {Benedetti, Marcello and Grant, Edward and Wossnig, Leonard and Severini, Simone},
	month = jun,
	year = {2018},
	keywords = {Quantum Physics, Statistics - Machine Learning},
	annote = {arXiv: 1806.00463},
	annote = {Comment: 14 pages, 6 figures},
	file = {arXiv\:1806.00463 PDF:/home/brian/Zotero/storage/JIYY489F/Benedetti et al. - 2018 - Adversarial quantum circuit learning for pure stat.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/YIYTFDFQ/1806.html:text/html}
}

@article{mcclean_barren_2018,
	title = {Barren plateaus in quantum neural network training landscapes},
	url = {http://arxiv.org/abs/1803.11173},
	abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
	urldate = {2018-06-08},
	journal = {arXiv:1803.11173 [physics, physics:quant-ph]},
	author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
	month = mar,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Learning, Physics - Chemical Physics},
	annote = {arXiv: 1803.11173},
	file = {arXiv\:1803.11173 PDF:/home/brian/Zotero/storage/L8664CKM/McClean et al. - 2018 - Barren plateaus in quantum neural network training.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/L32UKF2T/1803.html:text/html}
}

@article{cheng_information_2017,
	title = {Information {Perspective} to {Probabilistic} {Modeling}: {Boltzmann} {Machines} versus {Born} {Machines}},
	shorttitle = {Information {Perspective} to {Probabilistic} {Modeling}},
	url = {http://arxiv.org/abs/1712.04144},
	abstract = {We compare and contrast the statistical physics and quantum physics inspired approaches for unsupervised generative modeling of classical data. The two approaches represent probabilities of observed data using energy-based models and quantum states respectively.Classical and quantum information patterns of the target datasets therefore provide principled guidelines for structural design and learning in these two approaches. Taking the restricted Boltzmann machines (RBM) as an example, we analyze the information theoretical bounds of the two approaches. We verify our reasonings by comparing the performance of RBMs of various architectures on the standard MNIST datasets.},
	urldate = {2018-06-08},
	journal = {arXiv:1712.04144 [cond-mat, physics:physics, physics:quant-ph, stat]},
	author = {Cheng, Song and Chen, Jing and Wang, Lei},
	month = dec,
	year = {2017},
	keywords = {Quantum Physics, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability},
	annote = {arXiv: 1712.04144},
	annote = {Comment: 7 pages, 4 figures},
	file = {arXiv\:1712.04144 PDF:/home/brian/Zotero/storage/SGSJQGW4/Cheng et al. - 2017 - Information Perspective to Probabilistic Modeling.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/ITTY6MZY/1712.html:text/html}
}

@inproceedings{grover_fast_1996,
	address = {New York, NY, USA},
	series = {{STOC} '96},
	title = {A {Fast} {Quantum} {Mechanical} {Algorithm} for {Database} {Search}},
	isbn = {0-89791-785-5},
	url = {http://doi.acm.org/10.1145/237814.237866},
	doi = {10.1145/237814.237866},
	booktitle = {Proceedings of the {Twenty}-eighth {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Grover, Lov K.},
	year = {1996},
	pages = {212--219}
}

@article{shor_polynomial-time_1997,
	title = {Polynomial-{Time} {Algorithms} for {Prime} {Factorization} and {Discrete} {Logarithms} on a {Quantum} {Computer}},
	volume = {26},
	url = {https://doi.org/10.1137/S0097539795293172},
	doi = {10.1137/S0097539795293172},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Shor, P.},
	year = {1997},
	pages = {1484--1509}
}

@article{preskill_quantum_2018,
	title = {Quantum {Computing} in the {NISQ} era and beyond},
	url = {http://arxiv.org/abs/1801.00862},
	abstract = {Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. NISQ devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away — we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing.},
	urldate = {2018-06-08},
	journal = {arXiv:1801.00862 [cond-mat, physics:quant-ph]},
	author = {Preskill, John},
	month = jan,
	year = {2018},
	keywords = {Quantum Physics, Condensed Matter - Strongly Correlated Electrons},
	annote = {arXiv: 1801.00862},
	annote = {Comment: 22 pages. Based on a Keynote Address at Quantum Computing for Business, 5 December 2017. (v2) Minor corrections},
	file = {arXiv\:1801.00862 PDF:/home/brian/Zotero/storage/JSYWJ2T2/Preskill - 2018 - Quantum Computing in the NISQ era and beyond.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/FSMD4ZMV/1801.html:text/html}
}

@article{neill_blueprint_2018,
	title = {A blueprint for demonstrating quantum supremacy with superconducting qubits},
	volume = {360},
	url = {http://science.sciencemag.org/content/360/6385/195.abstract},
	doi = {10.1126/science.aao4309},
	abstract = {Quantum information scientists are getting closer to building a quantum computer that can perform calculations that a classical computer cannot. It has been estimated that such a computer would need around 50 qubits, but scaling up existing architectures to this number is tricky. Neill et al. explore how increasing the number of qubits from five to nine affects the quality of the output of their superconducting qubit device. If, as the number of qubits grows further, the error continues to increase at the same rate, a quantum computer with about 60 qubits and reasonable fidelity might be achievable with current technologies.Science, this issue p. 195A key step toward demonstrating a quantum system that can address difficult problems in physics and chemistry will be performing a computation beyond the capabilities of any classical computer, thus achieving so-called quantum supremacy. In this study, we used nine superconducting qubits to demonstrate a promising path toward quantum supremacy. By individually tuning the qubit parameters, we were able to generate thousands of distinct Hamiltonian evolutions and probe the output probabilities. The measured probabilities obey a universal distribution, consistent with uniformly sampling the full Hilbert space. As the number of qubits increases, the system continues to explore the exponentially growing number of states. Extending these results to a system of 50 qubits has the potential to address scientific questions that are beyond the capabilities of any classical computer.},
	number = {6385},
	journal = {Science},
	author = {Neill, C. and Roushan, P. and Kechedzhi, K. and Boixo, S. and Isakov, S. V. and Smelyanskiy, V. and Megrant, A. and Chiaro, B. and Dunsworth, A. and Arya, K. and Barends, R. and Burkett, B. and Chen, Y. and Chen, Z. and Fowler, A. and Foxen, B. and Giustina, M. and Graff, R. and Jeffrey, E. and Huang, T. and Kelly, J. and Klimov, P. and Lucero, E. and Mutus, J. and Neeley, M. and Quintana, C. and Sank, D. and Vainsencher, A. and Wenner, J. and White, T. C. and Neven, H. and Martinis, J. M.},
	month = apr,
	year = {2018},
	pages = {195}
}

@article{aaronson_computational_2013,
	title = {The {Computational} {Complexity} of {Linear} {Optics}},
	volume = {9},
	issn = {1557-2862},
	url = {http://www.theoryofcomputing.org/articles/v009a004},
	doi = {10.4086/toc.2013.v009a004},
	language = {EN},
	number = {1},
	urldate = {2018-06-08},
	journal = {Theory of Computing},
	author = {Aaronson, Scott and Arkhipov, Alex},
	month = feb,
	year = {2013},
	pages = {143--252},
	file = {Full Text PDF:/home/brian/Zotero/storage/UUZBVTVD/Aaronson and Arkhipov - 2013 - The Computational Complexity of Linear Optics.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/ZX96EPR4/v009a004.html:text/html}
}

@article{benedetti_generative_2019,
	title = {A generative modeling approach for benchmarking and training shallow quantum circuits},
	volume = {5},
	copyright = {2019 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0157-8},
	doi = {10.1038/s41534-019-0157-8},
	abstract = {Hybrid quantum-classical algorithms provide ways to use noisy intermediate-scale quantum computers for practical applications. Expanding the portfolio of such techniques, we propose a quantum circuit learning algorithm that can be used to assist the characterization of quantum devices and to train shallow circuits for generative tasks. The procedure leverages quantum hardware capabilities to its fullest extent by using native gates and their qubit connectivity. We demonstrate that our approach can learn an optimal preparation of the Greenberger-Horne-Zeilinger states, also known as “cat states”. We further demonstrate that our approach can efficiently prepare approximate representations of coherent thermal states, wave functions that encode Boltzmann probabilities in their amplitudes. Finally, complementing proposals to characterize the power or usefulness of near-term quantum devices, such as IBM’s quantum volume, we provide a new hardware-independent metric called the qBAS score. It is based on the performance yield in a specific sampling task on one of the canonical machine learning data sets known as Bars and Stripes. We show how entanglement is a key ingredient in encoding the patterns of this data set; an ideal benchmark for testing hardware starting at four qubits and up. We provide experimental results and evaluation of this metric to probe the trade off between several architectural circuit designs and circuit depths on an ion-trap quantum computer.},
	language = {En},
	number = {1},
	urldate = {2019-07-30},
	journal = {npj Quantum Information},
	author = {Benedetti, Marcello and Garcia-Pintos, Delfina and Perdomo, Oscar and Leyton-Ortega, Vicente and Nam, Yunseong and Perdomo-Ortiz, Alejandro},
	month = may,
	year = {2019},
	pages = {45},
	file = {Full Text PDF:/home/brian/Zotero/storage/PBQZ9Q7W/Benedetti et al. - 2019 - A generative modeling approach for benchmarking an.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/I597QW44/s41534-019-0157-8.html:text/html}
}

@article{du_bayesian_2018,
	title = {Bayesian {Quantum} {Circuit}},
	url = {http://arxiv.org/abs/1805.11089},
	abstract = {Parameterized quantum circuits (PQCs), as one of the most promising schemes to realize quantum machine learning algorithms on near-term quantum computers, have been designed to solve machine earning tasks with quantum advantages. In this paper, we explain why PQCs with different structures can achieve generative tasks in the discrete case from the perspective of probability estimation. Although different kinds of PQCs are proposed for generative tasks, the current methods often encounter the following three hurdles: (1) the mode contraction problem; (2) unexpected data are often generated with a high proportion; (3) target data cannot be sampled directly. For the purpose of tackling the above hurdles, we devise Bayesian quantum circuit (BQC) through introducing ancillary qubits to represent prior distributions. BQC advances both generative and semi-supervised quantum circuit learning tasks, where its effectiveness is validated by numerical simulations using the Rigetti Forest platform.},
	urldate = {2018-06-08},
	journal = {arXiv:1805.11089 [quant-ph]},
	author = {Du, Yuxuan and Liu, Tongliang and Tao, Dacheng},
	month = may,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1805.11089},
	file = {arXiv\:1805.11089 PDF:/home/brian/Zotero/storage/ATQ3U4HV/Du et al. - 2018 - Bayesian Quantum Circuit.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/KXDWPNQZ/1805.html:text/html}
}

@article{muandet_kernel_2017,
	title = {Kernel {Mean} {Embedding} of {Distributions}: {A} {Review} and {Beyond}},
	volume = {10},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Kernel {Mean} {Embedding} of {Distributions}},
	url = {http://arxiv.org/abs/1605.09522},
	doi = {10.1561/2200000060},
	abstract = {A Hilbert space embedding of a distribution—in short, a kernel mean embedding—has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules—which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning—in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	number = {1-2},
	urldate = {2018-06-08},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	year = {2017},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	pages = {1--141},
	annote = {arXiv: 1605.09522},
	annote = {Comment: 147 pages; this is a version of the manuscript after the review process},
	file = {arXiv\:1605.09522 PDF:/home/brian/Zotero/storage/3MRX84JS/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/F4RRXWRP/1605.html:text/html}
}

@inproceedings{w._van_dam_how_2001,
	title = {How powerful is adiabatic quantum computation?},
	doi = {10.1109/SFCS.2001.959902},
	abstract = {The authors analyze the computational power and limitations of the recently proposed 'quantum adiabatic evolution algorithm'. Adiabatic quantum computation is a novel paradigm for the design of quantum algorithms; it is truly quantum in the sense that it can be used to speed up searching by a quadratic factor over any classical algorithm. On the question of whether this new paradigm may be used to efficiently solve NP-complete problems on a quantum computer, we show that the usual query complexity arguments cannot be used to rule out a polynomial time solution. On the other hand, we argue that the adiabatic approach may be thought of as a kind of 'quantum local search'. We design a family of minimization problems that is hard for such local search heuristics, and establish an exponential lower bound for the adiabatic algorithm for these problems. This provides insights into the limitations of this approach. It remains an open question whether adiabatic quantum computation can establish an exponential speed-up over traditional computing or if there exists a classical algorithm that can simulate the quantum adiabatic process efficiently.},
	booktitle = {Proceedings 2001 {IEEE} {International} {Conference} on {Cluster} {Computing}},
	author = {{W. van Dam} and {M. Mosca} and {U. Vazirani}},
	month = oct,
	year = {2001},
	keywords = {adiabatic quantum computation, Algorithm design and analysis, computational complexity, Computational modeling, computational power, Computer science, Computer simulation, Cryptography, exponential lower bound, exponential speed-up, Laboratories, local search heuristics, minimisation, minimization problems, NP-complete problems, polynomial time solution, Polynomials, quadratic factor, quantum adiabatic evolution algorithm, quantum algorithms, quantum computer, quantum computing, Quantum computing, quantum local search, Quantum mechanics, query complexity arguments, search problems, Simulated annealing},
	pages = {279--287}
}

@article{born_beweis_1928,
	title = {Beweis des {Adiabatensatzes}},
	volume = {51},
	issn = {0044-3328},
	url = {https://link.springer.com/article/10.1007/BF01343193},
	doi = {10.1007/BF01343193},
	abstract = {Der Adiabatensatz in der neuen Quantenmechanik wird für den Fall des Punktspektrums in mathematisch strenger Weise bewiesen, wobei er sich auch bei einer vorübergehenden Entartung des mechanischen...},
	language = {de},
	number = {3-4},
	urldate = {2018-05-31},
	journal = {Zeitschrift für Physik},
	author = {Born, M. and Fock, V.},
	month = mar,
	year = {1928},
	pages = {165--180},
	file = {Full Text PDF:/home/brian/Zotero/storage/78J9WSRT/Born and Fock - 1928 - Beweis des Adiabatensatzes.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/V63CB7X7/10.html:text/html}
}

@article{schuld_circuit-centric_2018,
	title = {Circuit-centric quantum classifiers},
	url = {http://arxiv.org/abs/1804.00633},
	abstract = {The current generation of quantum computing technologies call for quantum algorithms that require a limited number of qubits and quantum gates, and which are robust against errors. A suitable design approach are variational circuits where the parameters of gates are learnt, an approach that is particularly fruitful for applications in machine learning. In this paper, we propose a low-depth variational quantum algorithm for supervised learning. The input feature vectors are encoded into the amplitudes of a quantum system, and a quantum circuit of parametrised single and two-qubit gates together with a single-qubit measurement is used to classify the inputs. This circuit architecture ensures that the number of learnable parameters is poly-logarithmic in the input dimension. We propose a quantum-classical training scheme where the analytical gradients of the model can be estimated by running several slightly adapted versions of the variational circuit. We show with simulations that the circuit-centric quantum classifier performs well on standard classical benchmark datasets while requiring dramatically fewer parameters than other methods. We also evaluate sensitivity of the classification to state preparation and parameter noise, introduce a quantum version of dropout regularisation and provide a graphical representation of quantum gates as highly symmetric linear layers of a neural network.},
	urldate = {2018-04-30},
	journal = {arXiv:1804.00633 [quant-ph]},
	author = {Schuld, Maria and Bocharov, Alex and Svore, Krysta and Wiebe, Nathan},
	month = apr,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1804.00633},
	annote = {Comment: 17 pages, 9 Figures, 5 Tables},
	file = {arXiv\:1804.00633 PDF:/home/brian/Zotero/storage/YNKMAWJV/Schuld et al. - 2018 - Circuit-centric quantum classifiers.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/EQHR6PA5/1804.html:text/html}
}

@inproceedings{broadbent_universal_2009,
	title = {Universal {Blind} {Quantum} {Computation}},
	doi = {10.1109/FOCS.2009.36},
	abstract = {We present a protocol which allows a client to have a server carry out a quantum computation for her such that the client's inputs, outputs and computation remain perfectly private, and where she does not require any quantum computational power or memory. The client only needs to be able to prepare single qubits randomly chosen from a finite set and send them to the server, who has the balance of the required quantum computational resources. Our protocol is interactive: after the initial preparation of quantum states, the client and server use two-way classical communication which enables the client to drive the computation, giving single-qubit measurement instructions to the server, depending on previous measurement outcomes. Our protocol works for inputs and outputs that are either classical or quantum. We give an authentication protocol that allows the client to detect an interfering server; our scheme can also be made fault-tolerant. We also generalize our result to the setting of a purely classical client who communicates classically with two non-communicating entangled servers, in order to perform a blind quantum computation. By incorporating the authentication protocol, we show that any problem in BQP has an entangled two-prover interactive proof with a purely classical verifier. Our protocol is the first universal scheme which detects a cheating server, as well as the first protocol which does not require any quantum computation whatsoever on the client's side. The novelty of our approach is in using the unique features of measurement-based quantum computing which allows us to clearly distinguish between the quantum and classical aspects of a quantum computation.},
	booktitle = {2009 50th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Broadbent, A. and Fitzsimons, J. and Kashefi, E.},
	month = oct,
	year = {2009},
	keywords = {Computer science, Quantum computing, Authentication, authentication protocol, blind quantum computation, cheating server detection, Computer aided instruction, cryptographic protocols, Fault detection, Fault tolerance, Informatics, measurement-based quantum computing, noncommunicating entangled servers, Privacy, Protocols, purely classical client setting, quantum cryptography, Quantum entanglement, quantum prover interactive proofs, single-qubit measurement, theorem proving, two-prover interactive proof, two-way classical communication},
	pages = {517--526},
	file = {IEEE Xplore Abstract Record:/home/brian/Zotero/storage/WJ4DVCUM/5438603.html:text/html}
}

@article{lloyd_quantum_2014,
	title = {Quantum principal component analysis},
	volume = {10},
	url = {http://dx.doi.org/10.1038/nphys3029},
	journal = {Nature Physics},
	author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
	month = jul,
	year = {2014},
	pages = {631}
}

@article{harrow_quantum_2009,
	title = {Quantum {Algorithm} for {Linear} {Systems} of {Equations}},
	volume = {103},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.103.150502},
	doi = {10.1103/PhysRevLett.103.150502},
	number = {15},
	journal = {Phys. Rev. Lett.},
	author = {Harrow, Aram W. and Hassidim, Avinatan and Lloyd, Seth},
	month = oct,
	year = {2009},
	pages = {150502}
}

@article{dallaire-demers_quantum_2018,
	title = {Quantum generative adversarial networks},
	url = {http://arxiv.org/abs/1804.08641},
	abstract = {Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits. Furthermore, we also show how to compute gradients – a key element in generative adversarial network training – using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.},
	urldate = {2018-04-29},
	journal = {arXiv:1804.08641 [quant-ph]},
	author = {Dallaire-Demers, Pierre-Luc and Killoran, Nathan},
	month = apr,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1804.08641},
	annote = {Comment: 23 pages, 11 figures},
	file = {arXiv\:1804.08641 PDF:/home/brian/Zotero/storage/EGJ7PNJL/Dallaire-Demers and Killoran - 2018 - Quantum generative adversarial networks.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/R5EMUC5I/1804.html:text/html}
}

@book{noauthor_ctan:_nodate,
	title = {{CTAN}: {Package} qcircuit},
	url = {https://ctan.org/pkg/qcircuit?lang=en},
	urldate = {2018-04-27},
	file = {CTAN\: Package qcircuit:/home/brian/Zotero/storage/EAL2GNQJ/qcircuit.html:text/html}
}

@article{bermejo-vega_architectures_2018,
	title = {Architectures for {Quantum} {Simulation} {Showing} a {Quantum} {Speedup}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.021010},
	doi = {10.1103/PhysRevX.8.021010},
	number = {2},
	journal = {Phys. Rev. X},
	author = {Bermejo-Vega, Juan and Hangleiter, Dominik and Schwarz, Martin and Raussendorf, Robert and Eisert, Jens},
	month = apr,
	year = {2018},
	pages = {021010}
}

@article{gao_quantum_2017,
	title = {Quantum {Supremacy} for {Simulating} a {Translation}-{Invariant} {Ising} {Spin} {Model}},
	volume = {118},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.040502},
	doi = {10.1103/PhysRevLett.118.040502},
	number = {4},
	journal = {Phys. Rev. Lett.},
	author = {Gao, Xun and Wang, Sheng-Tao and Duan, L.-M.},
	month = jan,
	year = {2017},
	pages = {040502}
}

@article{kieferova_tomography_2017,
	title = {Tomography and generative training with quantum {Boltzmann} machines},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.96.062327},
	doi = {10.1103/PhysRevA.96.062327},
	number = {6},
	journal = {Phys. Rev. A},
	author = {Kieferová, Mária and Wiebe, Nathan},
	month = dec,
	year = {2017},
	pages = {062327}
}

@article{bremner_classical_2011,
	title = {Classical simulation of commuting quantum computations implies collapse of the polynomial hierarchy},
	volume = {467},
	issn = {1364-5021},
	url = {http://rspa.royalsocietypublishing.org/content/467/2126/459},
	doi = {10.1098/rspa.2010.0301},
	abstract = {We consider quantum computations comprising only commuting gates, known as IQP computations, and provide compelling evidence that the task of sampling their output probability distributions is unlikely to be achievable by any efficient classical means. More specifically, we introduce the class post-IQP of languages decided with bounded error by uniform families of IQP circuits with post-selection, and prove first that post-IQP equals the classical class PP. Using this result we show that if the output distributions of uniform IQP circuit families could be classically efficiently sampled, either exactly in total variation distance or even approximately up to 41 per cent multiplicative error in the probabilities, then the infinite tower of classical complexity classes known as the polynomial hierarchy would collapse to its third level. We mention some further results on the classical simulation properties of IQP circuit families, in particular showing that if the output distribution results from measurements on only lines then it may, in fact, be classically efficiently sampled.},
	number = {2126},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Bremner, Michael J. and Jozsa, Richard and Shepherd, Dan J.},
	year = {2011},
	pages = {459--472}
}

@article{li_hybrid_2017,
	title = {Hybrid {Quantum}-{Classical} {Approach} to {Quantum} {Optimal} {Control}},
	volume = {118},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.150503},
	doi = {10.1103/PhysRevLett.118.150503},
	number = {15},
	journal = {Phys. Rev. Lett.},
	author = {Li, Jun and Yang, Xiaodong and Peng, Xinhua and Sun, Chang-Pu},
	month = apr,
	year = {2017},
	pages = {150503}
}

@article{gao_efficient_2017-1,
	title = {Efficient representation of quantum many-body states with deep neural networks},
	volume = {8},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/s41467-017-00705-2},
	doi = {10.1038/s41467-017-00705-2},
	abstract = {Part of the challenge for quantum many-body problems comes from the difficulty of representing large-scale quantum states, which in general requires an exponentially large number of parameters. Neural networks provide a powerful tool to represent quantum many-body states. An important open question is what characterizes the representational power of deep and shallow neural networks, which is of fundamental interest due to the popularity of deep learning methods. Here, we give a proof that, assuming a widely believed computational complexity conjecture, a deep neural network can efficiently represent most physical states, including the ground states of many-body Hamiltonians and states generated by quantum dynamics, while a shallow network representation with a restricted Boltzmann machine cannot efficiently represent some of those states.},
	number = {1},
	journal = {Nature Communications},
	author = {Gao, Xun and Duan, Lu-Ming},
	month = sep,
	year = {2017},
	pages = {662}
}

@article{carleo_solving_2017,
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	issn = {0036-8075},
	url = {http://science.sciencemag.org/content/355/6325/602},
	doi = {10.1126/science.aag2302},
	abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.Science, this issue p. 602; see also p. 580The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.},
	number = {6325},
	journal = {Science},
	author = {Carleo, Giuseppe and Troyer, Matthias},
	year = {2017},
	pages = {602--606}
}

@article{raussendorf_one-way_2001,
	title = {A {One}-{Way} {Quantum} {Computer}},
	volume = {86},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.86.5188},
	doi = {10.1103/PhysRevLett.86.5188},
	number = {22},
	journal = {Phys. Rev. Lett.},
	author = {Raussendorf, Robert and Briegel, Hans J.},
	month = may,
	year = {2001},
	pages = {5188--5191}
}

@book{noauthor_[1704.08482]_nodate,
	title = {[1704.08482] {On} the implausibility of classical client blind quantum computing},
	url = {https://arxiv.org/abs/1704.08482},
	urldate = {2018-04-27},
	file = {[1704.08482] On the implausibility of classical client blind quantum computing:/home/brian/Zotero/storage/MU8NPYQ5/1704.html:text/html}
}

@article{steinbrecher_quantum_2019,
	title = {Quantum optical neural networks},
	volume = {5},
	copyright = {2019 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0174-7},
	doi = {10.1038/s41534-019-0174-7},
	abstract = {Physically motivated quantum algorithms for specific near-term quantum hardware will likely be the next frontier in quantum information science. Here, we show how many of the features of neural networks for machine learning can naturally be mapped into the quantum optical domain by introducing the quantum optical neural network (QONN). Through numerical simulation and analysis we train the QONN to perform a range of quantum information processing tasks, including newly developed protocols for quantum optical state compression, reinforcement learning, black-box quantum simulation, and one-way quantum repeaters. We consistently demonstrate that our system can generalize from only a small set of training data onto inputs for which it has not been trained. Our results indicate that QONNs are a powerful design tool for quantum optical systems and, leveraging advances in integrated quantum photonics, a promising architecture for next-generation quantum processors.},
	language = {En},
	number = {1},
	urldate = {2019-07-30},
	journal = {npj Quantum Information},
	author = {Steinbrecher, Gregory R. and Olson, Jonathan P. and Englund, Dirk and Carolan, Jacques},
	month = jul,
	year = {2019},
	pages = {60},
	file = {Full Text PDF:/home/brian/Zotero/storage/M3Z7K7BJ/Steinbrecher et al. - 2019 - Quantum optical neural networks.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/QTAI2LVN/s41534-019-0174-7.html:text/html}
}

@article{gheorghiu_verification_2017,
	title = {Verification of quantum computation: {An} overview of existing approaches},
	shorttitle = {Verification of quantum computation},
	url = {https://arxiv.org/abs/1709.06984},
	language = {en},
	urldate = {2018-04-20},
	author = {Gheorghiu, Alexandru and Kapourniotis, Theodoros and Kashefi, Elham},
	month = sep,
	year = {2017},
	file = {Full Text PDF:/home/brian/Zotero/storage/K2YR46PD/Gheorghiu et al. - 2017 - Verification of quantum computation An overview o.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/642724YQ/1709.html:text/html}
}

@book{nielsen_quantum_2010,
	address = {Cambridge},
	edition = {Tenth anniversary edition..},
	title = {Quantum computation and quantum information},
	isbn = {978-1-107-00217-3},
	abstract = {Fundamental Concepts – Introduction and overview – Introduction to quantum mechanics – Introduction to computer science – Quantum Computation – Quantum circuits – The quantum Fourier transform and its applications – Quantum search algorithms – Quantum computers: physical realisation – Quantum Information – Quantum noise, open quantum systems, and quantum operations – Distance measurement for quantum information – Quantum error-correction – Entropy and information – Quantum information theory – Notes on basic probability theory – Group theory – Approximating quantum gates: the Solovay-Kitaev theorem – Number theory – Public-key cryptography and the RSA cryptosystem – Proof of Lieb's theorem.},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Nielsen, Michael A.},
	year = {2010},
	keywords = {Quantum computers.}
}

@article{fujii_commuting_2017,
	title = {Commuting quantum circuits and complexity of {Ising} partition functions},
	volume = {19},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/19/i=3/a=033003?key=crossref.cefbe34cf11242886552ceea447a4526},
	doi = {10.1088/1367-2630/aa5fdb},
	language = {en},
	number = {3},
	urldate = {2018-04-20},
	journal = {New Journal of Physics},
	author = {Fujii, Keisuke and Morimae, Tomoyuki},
	month = mar,
	year = {2017},
	pages = {033003},
	file = {pdf.pdf:/home/brian/Zotero/storage/2RVX5NSP/pdf.pdf:application/pdf}
}

@article{farhi_classification_2018,
	title = {Classification with {Quantum} {Neural} {Networks} on {Near} {Term} {Processors}},
	url = {https://arxiv.org/abs/1802.06002},
	language = {en},
	urldate = {2018-04-20},
	journal = {arXiv:1802.06002},
	author = {Farhi, Edward and Neven, Hartmut},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/brian/Zotero/storage/6FX3UYN9/Farhi and Neven - 2018 - Classification with Quantum Neural Networks on Nea.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/6TUVQBLX/1802.html:text/html}
}

@article{farhi_quantum_2016,
	title = {Quantum {Supremacy} through the {Quantum} {Approximate} {Optimization} {Algorithm}},
	url = {http://arxiv.org/abs/1602.07674},
	abstract = {The Quantum Approximate Optimization Algorithm (QAOA) is designed to run on a gate model quantum computer and has shallow depth. It takes as input a combinatorial optimization problem and outputs a string that satisfies a high fraction of the maximum number of clauses that can be satisfied. For certain problems the lowest depth version of the QAOA has provable performance guarantees although there exist classical algorithms that have better guarantees. Here we argue that beyond its possible computational value the QAOA can exhibit a form of Quantum Supremacy in that, based on reasonable complexity theoretic assumptions, the output distribution of even the lowest depth version cannot be efficiently simulated on any classical device. We contrast this with the case of sampling from the output of a quantum computer running the Quantum Adiabatic Algorithm (QADI) with the restriction that the Hamiltonian that governs the evolution is gapped and stoquastic. Here we show that there is an oracle that would allow sampling from the QADI but even with this oracle, if one could efficiently classically sample from the output of the QAOA, the Polynomial Hierarchy would collapse. This suggests that the QAOA is an excellent candidate to run on near term quantum computers not only because it may be of use for optimization but also because of its potential as a route to establishing quantum supremacy.},
	urldate = {2018-04-12},
	journal = {arXiv:1602.07674 [quant-ph]},
	author = {Farhi, Edward and Harrow, Aram W.},
	month = feb,
	year = {2016},
	keywords = {Quantum Physics},
	annote = {arXiv: 1602.07674},
	annote = {Comment: 22 pages},
	file = {arXiv\:1602.07674 PDF:/home/brian/Zotero/storage/FBYZV4Z6/Farhi and Harrow - 2016 - Quantum Supremacy through the Quantum Approximate .pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/HFC6FJZ5/1602.html:text/html}
}

@article{verdon_quantum_2017,
	title = {A quantum algorithm to train neural networks using low-depth circuits},
	url = {http://arxiv.org/abs/1712.05304},
	abstract = {The question has remained open if near-term gate model quantum computers will offer a quantum advantage for practical applications in the pre-fault tolerance noise regime. A class of algorithms which have shown some promise in this regard are the so-called classical-quantum hybrid variational algorithms. Here we develop a low-depth quantum algorithm to train quantum Boltzmann machine neural networks using such variational methods. We introduce a method which employs the quantum approximate optimization algorithm as a subroutine in order to approximately sample from Gibbs states of Ising Hamiltonians. We use this approximate Gibbs sampling to train neural networks for which we demonstrate training convergence for numerically simulated noisy circuits with depolarizing errors of rates of up to 4\%.},
	urldate = {2018-04-12},
	journal = {arXiv:1712.05304 [cond-mat, physics:quant-ph]},
	author = {Verdon, Guillaume and Broughton, Michael and Biamonte, Jacob},
	month = dec,
	year = {2017},
	keywords = {Quantum Physics, Condensed Matter - Disordered Systems and Neural Networks},
	annote = {arXiv: 1712.05304},
	annote = {Comment: 8 pages, 2 figures},
	file = {arXiv\:1712.05304 PDF:/home/brian/Zotero/storage/JXBALX5W/Verdon et al. - 2017 - A quantum algorithm to train neural networks using.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/A5VBV2PF/1712.html:text/html}
}

@article{kapourniotis_nonadaptive_2017,
	title = {Nonadaptive fault-tolerant verification of quantum supremacy with noise},
	url = {http://arxiv.org/abs/1703.09568},
	abstract = {Quantum samplers are believed capable of sampling efficiently from distributions that are classically hard to sample from. We consider a sampler inspired by the Ising model. It is nonadaptive and therefore experimentally amenable. Under a plausible average-case hardness conjecture, classical sampling upto additive errors from this model is known to be hard. We present a trap-based verification scheme for quantum supremacy that only requires the verifier to prepare single-qubit states. The verification is done on the same model as the original sampler, a square lattice, with only a constant factor overhead. We next revamp our verification scheme to operate in the presence of noise by emulating a fault-tolerant procedure without correcting on-line for the errors, thus keeping the model non-adaptive, but verifying supremacy fault-tolerantly. We show that classically sampling upto additive errors is likely hard in our revamped scheme. Our results are applicable to more general sampling problems such as the Instantaneous Quantum Polynomial-time (IQP) computation model. It should also assist near-term attempts at experimentally demonstrating quantum supremacy and guide long-term ones.},
	urldate = {2018-04-12},
	journal = {arXiv:1703.09568 [quant-ph]},
	author = {Kapourniotis, Theodoros and Datta, Animesh},
	month = mar,
	year = {2017},
	keywords = {Quantum Physics},
	annote = {arXiv: 1703.09568},
	annote = {Comment: 17 pages, 10 figures, 3 theorems, 1 conjecture. Minor changes in discussions, additional explanation in the proof of Theorem 1 and updated references},
	file = {arXiv\:1703.09568 PDF:/home/brian/Zotero/storage/K9PAZ3GH/Kapourniotis and Datta - 2017 - Nonadaptive fault-tolerant verification of quantum.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/5A7DIFBZ/1703.html:text/html}
}

@inproceedings{fukumizu_kernel_2007,
	title = {Kernel {Measures} of {Conditional} {Dependence}},
	abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of infinite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
	booktitle = {{NIPS}},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	year = {2007},
	keywords = {Cross-covariance, Experiment, Hilbert space, Kernel (operating system), Vergence},
	file = {Full Text PDF:/home/brian/Zotero/storage/MNB2IVP6/Fukumizu et al. - 2007 - Kernel Measures of Conditional Dependence.pdf:application/pdf}
}

@book{dudley_real_2002,
	edition = {2},
	series = {Cambridge {Studies} in {Advanced} {Mathematics}},
	title = {Real {Analysis} and {Probability}},
	publisher = {Cambridge University Press},
	author = {Dudley, R. M.},
	year = {2002},
	doi = {10.1017/CBO9780511755347}
}

@article{hoban_measurement-based_2014,
	title = {Measurement-{Based} {Classical} {Computation}},
	volume = {112},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.112.140505},
	doi = {10.1103/PhysRevLett.112.140505},
	number = {14},
	journal = {Phys. Rev. Lett.},
	author = {Hoban, Matty J. and Wallman, Joel J. and Anwar, Hussain and Usher, Naïri and Raussendorf, Robert and Browne, Dan E.},
	month = apr,
	year = {2014},
	pages = {140505}
}

@article{leyton-ortega_robust_2019,
	title = {Robust {Implementation} of {Generative} {Modeling} with {Parametrized} {Quantum} {Circuits}},
	url = {http://arxiv.org/abs/1901.08047},
	abstract = {Although the performance of hybrid quantum-classical algorithms is highly dependent on the selection of the classical optimizer and the circuit ansatz, a robust and thorough assessment on-hardware of such features has been missing to date. From the optimizer perspective, the primary challenge lies in the solver's stochastic nature, and their significant variance over the random initialization. Therefore, a robust comparison requires that one perform several training curves for each solver before one can reach conclusions about their typical performance. Since each of the training curves requires the execution of thousands of quantum circuits in the quantum computer, such a robust study remained a steep challenge for most hybrid platforms available today. Here, we leverage on Rigetti's Quantum Cloud Services to overcome this implementation barrier, and we study the on-hardware performance of the data-driven quantum circuit learning (DDQCL) for three different state-of-the-art classical solvers, and on two-different circuit ansatze associated to different entangling connectivity graphs for the same task. Additionally, we assess the gains in performance from varying circuit depths. To evaluate the typical performance associated with each of these settings in this benchmark study, we use at least five independent runs of DDQCL towards the generation of quantum generative models capable of capturing the patterns of the canonical Bars and Stripes data set.},
	urldate = {2019-02-05},
	journal = {arXiv:1901.08047 [quant-ph]},
	author = {Leyton-Ortega, Vicente and Perdomo-Ortiz, Alejandro and Perdomo, Oscar},
	month = jan,
	year = {2019},
	keywords = {Quantum Physics},
	annote = {arXiv: 1901.08047},
	annote = {Comment: 8 pages, 5 figures, 2 tables},
	file = {arXiv\:1901.08047 PDF:/home/brian/Zotero/storage/RIVNHSAH/Leyton-Ortega et al. - 2019 - Robust Implementation of Generative Modeling with .pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/EJF2SD2W/1901.html:text/html}
}

@article{schuld_evaluating_2018,
	title = {Evaluating analytic gradients on quantum hardware},
	url = {http://arxiv.org/abs/1811.11184},
	abstract = {An important application for near-term quantum computing lies in optimization tasks, with applications ranging from quantum chemistry and drug discovery to machine learning. In many settings — most prominently in so-called parametrized or variational algorithms — the objective function is a result of hybrid quantum-classical processing. To optimize the objective, it is useful to have access to exact gradients of quantum circuits with respect to gate parameters. This paper shows how gradients of expectation values of quantum measurements can be estimated using the same, or almost the same, architecture that executes the original circuit. It generalizes previous results for qubit-based platforms, and proposes recipes for the computation of gradients of continuous-variable circuits. Interestingly, in many important instances it is sufficient to run the original quantum circuit twice while shifting a single gate parameter to obtain the corresponding component of the gradient. More general cases can be solved by conditioning a single gate on an ancilla.},
	urldate = {2019-02-05},
	journal = {arXiv:1811.11184 [quant-ph]},
	author = {Schuld, Maria and Bergholm, Ville and Gogolin, Christian and Izaac, Josh and Killoran, Nathan},
	month = nov,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1811.11184},
	file = {arXiv\:1811.11184 PDF:/home/brian/Zotero/storage/CIE3A2IZ/Schuld et al. - 2018 - Evaluating analytic gradients on quantum hardware.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/QZ9XZL9B/1811.html:text/html}
}

@article{harrow_low-depth_2019,
	title = {Low-depth gradient measurements can improve convergence in variational hybrid quantum-classical algorithms},
	url = {http://arxiv.org/abs/1901.05374},
	abstract = {A broad class of hybrid quantum-classical algorithms known as "variational algorithms" have been proposed in the context of quantum simulation, machine learning, and combinatorial optimization as a means of potentially achieving a quantum speedup on a near-term quantum device for a problem of practical interest. Such algorithms use the quantum device only to prepare parameterized quantum states and make simple measurements. A classical controller uses the measurement results to perform an optimization of a classical function induced by a quantum observable which defines the problem. While most prior works have considered optimization strategies based on estimating the objective function and doing a derivative-free or finite-difference-based optimization, some recent proposals involve directly measuring observables corresponding to the gradient of the objective function. The measurement procedure needed requires coherence time barely longer than that needed to prepare a trial state. We prove that strategies based on such gradient measurements can admit substantially faster rates of convergence to the optimum in some contexts. We first introduce a natural black-box setting for variational algorithms which we prove our results with respect to. We define a simple class of problems for which a variational algorithm based on low-depth gradient measurements and stochastic gradient descent converges to the optimum substantially faster than any possible strategy based on estimating the objective function itself, and show that stochastic gradient descent is essentially optimal for this problem. Importing known results from the stochastic optimization literature, we also derive rigorous upper bounds on the cost of variational optimization in a convex region when using gradient measurements in conjunction with certain stochastic gradient descent or stochastic mirror descent algorithms.},
	urldate = {2019-02-05},
	journal = {arXiv:1901.05374 [quant-ph]},
	author = {Harrow, Aram and Napp, John},
	month = jan,
	year = {2019},
	keywords = {Quantum Physics, Mathematics - Optimization and Control},
	annote = {arXiv: 1901.05374},
	annote = {Comment: 45 pages},
	file = {arXiv\:1901.05374 PDF:/home/brian/Zotero/storage/D82MYCNB/Harrow and Napp - 2019 - Low-depth gradient measurements can improve conver.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/59PSNCIJ/1901.html:text/html}
}

@article{kerenidis_quantum_2018,
	title = {Quantum classification of the {MNIST} dataset via {Slow} {Feature} {Analysis}},
	url = {http://arxiv.org/abs/1805.08837},
	abstract = {Quantum machine learning carries the promise to revolutionize information and communication technologies. While a number of quantum algorithms with potential exponential speedups have been proposed already, it is quite difficult to provide convincing evidence that quantum computers with quantum memories will be in fact useful to solve real-world problems. Our work makes considerable progress towards this goal. We design quantum techniques for Dimensionality Reduction and for Classification, and combine them to provide an efficient and high accuracy quantum classifier that we test on the MNIST dataset. More precisely, we propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality reduction technique that maps the dataset in a lower dimensional space where we can apply a novel quantum classification procedure, the Quantum Frobenius Distance (QFD). We simulate the quantum classifier (including errors) and show that it can provide classification of the MNIST handwritten digit dataset, a widely used dataset for benchmarking classification algorithms, with \$98.5{\textbackslash}textbackslash\%\$ accuracy, similar to the classical case. The running time of the quantum classifier is polylogarithmic in the dimension and number of data points. We also provide evidence that the other parameters on which the running time depends (condition number, Frobenius norm, error threshold, etc.) scale favorably in practice, thus ascertaining the efficiency of our algorithm.},
	urldate = {2019-01-31},
	journal = {arXiv:1805.08837 [quant-ph]},
	author = {Kerenidis, Iordanis and Luongo, Alessandro},
	month = may,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {arXiv: 1805.08837},
	annote = {Comment: Results mainly unchanged. Improved running time analysis based on recent work in [CGJ19, GSLW18]},
	file = {arXiv\:1805.08837 PDF:/home/brian/Zotero/storage/PU7MTV3E/Kerenidis and Luongo - 2018 - Quantum classification of the MNIST dataset via Sl.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/9RKSLQ7A/1805.html:text/html}
}

@article{rebentrost_quantum_2014,
	title = {Quantum {Support} {Vector} {Machine} for {Big} {Data} {Classification}},
	volume = {113},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.113.130503},
	doi = {10.1103/PhysRevLett.113.130503},
	number = {13},
	journal = {Phys. Rev. Lett.},
	author = {Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth},
	month = sep,
	year = {2014},
	pages = {130503}
}

@article{hu_quantum_2019,
	title = {Quantum generative adversarial learning in a superconducting quantum circuit},
	volume = {5},
	url = {http://advances.sciencemag.org/content/5/1/eaav2761.abstract},
	doi = {10.1126/sciadv.aav2761},
	abstract = {Generative adversarial learning is one of the most exciting recent breakthroughs in machine learning. It has shown splendid performance in a variety of challenging tasks such as image and video generation. More recently, a quantum version of generative adversarial learning has been theoretically proposed and shown to have the potential of exhibiting an exponential advantage over its classical counterpart. Here, we report the first proof-of-principle experimental demonstration of quantum generative adversarial learning in a superconducting quantum circuit. We demonstrate that, after several rounds of adversarial learning, a quantum-state generator can be trained to replicate the statistics of the quantum data output from a quantum channel simulator, with a high fidelity (98.8\% on average) so that the discriminator cannot distinguish between the true and the generated data. Our results pave the way for experimentally exploring the intriguing long-sought-after quantum advantages in machine learning tasks with noisy intermediate–scale quantum devices.},
	number = {1},
	journal = {Science Advances},
	author = {Hu, Ling and Wu, Shu-Hao and Cai, Weizhou and Ma, Yuwei and Mu, Xianghao and Xu, Yuan and Wang, Haiyan and Song, Yipu and Deng, Dong-Ling and Zou, Chang-Ling and Sun, Luyan},
	month = jan,
	year = {2019},
	pages = {eaav2761}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-01-18},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv: 1412.6980},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:/home/brian/Zotero/storage/TS233BK6/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/9K3FLZHX/1412.html:text/html}
}

@article{boixo_characterizing_2018,
	title = {Characterizing quantum supremacy in near-term devices},
	volume = {14},
	issn = {1745-2481},
	url = {https://doi.org/10.1038/s41567-018-0124-x},
	doi = {10.1038/s41567-018-0124-x},
	abstract = {A critical question for quantum computing in the near future is whether quantum devices without error correction can perform a well-defined computational task beyond the capabilities of supercomputers. Such a demonstration of what is referred to as quantum supremacy requires a reliable evaluation of the resources required to solve tasks with classical approaches. Here, we propose the task of sampling from the output distribution of random quantum circuits as a demonstration of quantum supremacy. We extend previous results in computational complexity to argue that this sampling task must take exponential time in a classical computer. We introduce cross-entropy benchmarking to obtain the experimental fidelity of complex multiqubit dynamics. This can be estimated and extrapolated to give a success metric for a quantum supremacy demonstration. We study the computational cost of relevant classical algorithms and conclude that quantum supremacy can be achieved with circuits in a two-dimensional lattice of 7 × 7 qubits and around 40 clock cycles. This requires an error rate of around 0.5\% for two-qubit gates (0.05\% for one-qubit gates), and it would demonstrate the basic building blocks for a fault-tolerant quantum computer.},
	number = {6},
	journal = {Nature Physics},
	author = {Boixo, Sergio and Isakov, Sergei V. and Smelyanskiy, Vadim N. and Babbush, Ryan and Ding, Nan and Jiang, Zhang and Bremner, Michael J. and Martinis, John M. and Neven, Hartmut},
	month = jun,
	year = {2018},
	pages = {595--600}
}

@article{bouland_quantum_2018,
	title = {Quantum {Supremacy} and the {Complexity} of {Random} {Circuit} {Sampling}},
	url = {http://arxiv.org/abs/1803.04402},
	abstract = {A critical milestone on the path to useful quantum computers is quantum supremacy - a demonstration of a quantum computation that is prohibitively hard for classical computers. A leading near-term candidate, put forth by the Google/UCSB team, is sampling from the probability distributions of randomly chosen quantum circuits, which we call Random Circuit Sampling (RCS). In this paper we study both the hardness and verification of RCS. While RCS was defined with experimental realization in mind, we show complexity theoretic evidence of hardness that is on par with the strongest theoretical proposals for supremacy. Specifically, we show that RCS satisfies an average-case hardness condition - computing output probabilities of typical quantum circuits is as hard as computing them in the worst-case, and therefore \#P-hard. Our reduction exploits the polynomial structure in the output amplitudes of random quantum circuits, enabled by the Feynman path integral. In addition, it follows from known results that RCS satisfies an anti-concentration property, making it the first supremacy proposal with both average-case hardness and anti-concentration.},
	urldate = {2019-01-18},
	journal = {arXiv:1803.04402 [quant-ph]},
	author = {Bouland, Adam and Fefferman, Bill and Nirkhe, Chinmay and Vazirani, Umesh},
	month = mar,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Computational Complexity},
	annote = {arXiv: 1803.04402},
	file = {arXiv\:1803.04402 PDF:/home/brian/Zotero/storage/PWQNWT8S/Bouland et al. - 2018 - Quantum Supremacy and the Complexity of Random Cir.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/I2UMWEP8/1803.html:text/html}
}

@article{aaronson_complexity-theoretic_2016,
	title = {Complexity-{Theoretic} {Foundations} of {Quantum} {Supremacy} {Experiments}},
	url = {http://arxiv.org/abs/1612.05903},
	abstract = {In the near future, there will likely be special-purpose quantum computers with 40-50 high-quality qubits. This paper lays general theoretical foundations for how to use such devices to demonstrate "quantum supremacy": that is, a clear quantum speedup for some task, motivated by the goal of overturning the Extended Church-Turing Thesis as confidently as possible. First, we study the hardness of sampling the output distribution of a random quantum circuit, along the lines of a recent proposal by the the Quantum AI group at Google. We show that there's a natural hardness assumption, which has nothing to do with sampling, yet implies that no efficient classical algorithm can pass a statistical test that the quantum sampling procedure's outputs do pass. Compared to previous work, the central advantage is that we can now talk directly about the observed outputs, rather than about the distribution being sampled. Second, in an attempt to refute our hardness assumption, we give a new algorithm, for simulating a general quantum circuit with n qubits and m gates in polynomial space and mˆO(n) time. We then discuss why this and other known algorithms fail to refute our assumption. Third, resolving an open problem of Aaronson and Arkhipov, we show that any strong quantum supremacy theorem–of the form "if approximate quantum sampling is classically easy, then PH collapses"–must be non-relativizing. Fourth, refuting a conjecture by Aaronson and Ambainis, we show that the Fourier Sampling problem achieves a constant versus linear separation between quantum and randomized query complexities. Fifth, we study quantum supremacy relative to oracles in P/poly. Previous work implies that, if OWFs exist, then quantum supremacy is possible relative to such oracles. We show that some assumption is needed: if SampBPP=SampBQP and NP is in BPP, then quantum supremacy is impossible relative to such oracles.},
	urldate = {2019-01-18},
	journal = {arXiv:1612.05903 [quant-ph]},
	author = {Aaronson, Scott and Chen, Lijie},
	month = dec,
	year = {2016},
	keywords = {Quantum Physics, Computer Science - Computational Complexity},
	annote = {arXiv: 1612.05903},
	annote = {Comment: abstract shortened to meet the constraint},
	file = {arXiv\:1612.05903 PDF:/home/brian/Zotero/storage/8EAXHGSU/Aaronson and Chen - 2016 - Complexity-Theoretic Foundations of Quantum Suprem.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/338JY88A/1612.html:text/html}
}

@article{mills_information_2018,
	title = {Information {Theoretically} {Secure} {Hypothesis} {Test} for {Temporally} {Unstructured} {Quantum} {Computation} ({Extended} {Abstract})},
	volume = {266},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1803.00706},
	doi = {10.4204/EPTCS.266.14},
	abstract = {The efficient certification of classically intractable quantum devices has been a central research question for some time. However, to observe a "quantum advantage", it is believed that one does not need to build a large scale universal quantum computer, a task which has proven extremely challenging. Intermediate quantum models that are easier to implement, but which also exhibit this quantum advantage over classical computers, have been proposed. In this work, we present a certification technique for such a sub-universal quantum server which only performs commuting gates and requires very limited quantum memory. By allowing a verifying client to manipulate single qubits, we exploit properties of measurement based blind quantum computing to give them the tools to test the "quantum superiority" of the server.},
	urldate = {2019-01-18},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Mills, Daniel and Pappa, Anna and Kapourniotis, Theodoros and Kashefi, Elham},
	month = feb,
	year = {2018},
	keywords = {Quantum Physics},
	pages = {209--221},
	annote = {arXiv: 1803.00706},
	annote = {Comment: In Proceedings QPL 2017, arXiv:1802.09737},
	file = {arXiv\:1803.00706 PDF:/home/brian/Zotero/storage/9GN2QS8E/Mills et al. - 2018 - Information Theoretically Secure Hypothesis Test f.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/CNFA5RML/1803.html:text/html}
}

@book{villani_optimal_2009,
	address = {Berlin Heidelberg},
	series = {Grundlehren der mathematischen {Wissenschaften}},
	title = {Optimal {Transport}: {Old} and {New}},
	isbn = {978-3-540-71049-3},
	shorttitle = {Optimal {Transport}},
	url = {//www.springer.com/gb/book/9783540710493},
	abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results. PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book’s value as a most welcome reference text on this subject.},
	language = {en},
	urldate = {2019-01-16},
	publisher = {Springer-Verlag},
	author = {Villani, Cédric},
	year = {2009},
	file = {Snapshot:/home/brian/Zotero/storage/CDAWKVTQ/9783540710493.html:text/html}
}

@article{sinkhorn_relationship_1964,
	title = {A {Relationship} {Between} {Arbitrary} {Positive} {Matrices} and {Doubly} {Stochastic} {Matrices}},
	volume = {35},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177703591},
	doi = {10.1214/aoms/1177703591},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {2},
	urldate = {2019-01-15},
	journal = {The Annals of Mathematical Statistics},
	author = {Sinkhorn, Richard},
	month = jun,
	year = {1964},
	mrnumber = {MR161868},
	zmnumber = {0134.25302},
	pages = {876--879},
	file = {Full Text PDF:/home/brian/Zotero/storage/CNDXT73K/Sinkhorn - 1964 - A Relationship Between Arbitrary Positive Matrices.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/IK445YBC/1177703591.html:text/html}
}

@article{peyre_computational_2018,
	title = {Computational {Optimal} {Transport}},
	url = {http://arxiv.org/abs/1803.00567},
	abstract = {Optimal Transport (OT) is a mathematical gem at the interface between probability, analysis and optimization. The goal of that theory is to define geometric tools that are useful to compare probability distributions. Earlier contributions originated from Monge's work in the 18th century, to be later rediscovered under a different formalism by Tolstoi in the 1920's, Kantorovich, Hitchcock and Koopmans in the 1940's. The problem was solved numerically by Dantzig in 1949 and others in the 1950's within the framework of linear programming, paving the way for major industrial applications in the second half of the 20th century. OT was later rediscovered under a different light by analysts in the 90's, following important work by Brenier and others, as well as in the computer vision/graphics fields under the name of earth mover's distances. Recent years have witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression,classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
	urldate = {2019-01-14},
	journal = {arXiv:1803.00567 [stat]},
	author = {Peyré, Gabriel and Cuturi, Marco},
	month = mar,
	year = {2018},
	keywords = {Statistics - Machine Learning},
	annote = {arXiv: 1803.00567},
	file = {arXiv\:1803.00567 PDF:/home/brian/Zotero/storage/WET76ZEY/Peyré and Cuturi - 2018 - Computational Optimal Transport.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/IA8PI5TW/1803.html:text/html}
}

@article{weed_sharp_2017,
	title = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in {Wasserstein} distance},
	url = {http://arxiv.org/abs/1707.00087},
	abstract = {The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from \$n\$ independent samples from \${\textbackslash}textbackslashmu\$ approaches \${\textbackslash}textbackslashmu\$ in the Wasserstein distance of any order. We prove sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces. Our finite-sample results show the existence of multi-scale behavior, where measures can exhibit radically different rates of convergence as \$n\$ grows.},
	urldate = {2019-01-10},
	journal = {arXiv:1707.00087 [math, stat]},
	author = {Weed, Jonathan and Bach, Francis},
	month = jun,
	year = {2017},
	keywords = {Mathematics - Statistics Theory, Mathematics - Probability, 60B10, 62E17},
	annote = {arXiv: 1707.00087},
	file = {arXiv\:1707.00087 PDF:/home/brian/Zotero/storage/KADFPBF6/Weed and Bach - 2017 - Sharp asymptotic and finite-sample rates of conver.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/C55U8PC4/1707.html:text/html}
}

@article{gibbs_choosing_2002,
	title = {On {Choosing} and {Bounding} {Probability} {Metrics}},
	volume = {70},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2002.tb00178.x},
	doi = {10.1111/j.1751-5823.2002.tb00178.x},
	abstract = {Summary When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
	number = {3},
	journal = {International Statistical Review},
	author = {Gibbs, Alison L. and Su, Francis Edward},
	year = {2002},
	keywords = {Discrepancy, Hellinger distance, Probability metrics, Prokhorov metric, Rates of convergence, Relative entropy, Wasserstein distance},
	pages = {419--435}
}

@article{hangleiter_sample_2018,
	title = {Sample complexity of device-independently certified "quantum supremacy"},
	url = {http://arxiv.org/abs/1812.01023},
	abstract = {Results on the hardness of approximate sampling are seen as important stepping stones towards a convincing demonstration of the superior computational power of quantum devices. The most prominent suggestions for such experiments include boson sampling, IQP circuit sampling, and universal random circuit sampling. A key challenge for any such demonstration is to certify the correct implementation. For all these examples, and in fact for all sufficiently flat distributions, we show that any non-interactive certification from classical samples and a description of the target distribution requires exponentially many uses of the device. Our proofs rely on the same property that is a central ingredient for the approximate hardness results: namely, that the sampling distributions, as random variables depending on the random unitaries defining the problem instances, have small second moments.},
	language = {en},
	urldate = {2019-01-07},
	journal = {arXiv:1812.01023 [quant-ph]},
	author = {Hangleiter, Dominik and Kliesch, Martin and Eisert, Jens and Gogolin, Christian},
	month = dec,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1812.01023},
	annote = {Comment: 17 pages},
	file = {Hangleiter et al. - 2018 - Sample complexity of device-independently certifie.pdf:/home/brian/Zotero/storage/HDZZBK7I/Hangleiter et al. - 2018 - Sample complexity of device-independently certifie.pdf:application/pdf}
}

@article{romero_variational_2019,
	title = {Variational quantum generators: {Generative} adversarial quantum machine learning for continuous distributions},
	shorttitle = {Variational quantum generators},
	url = {http://arxiv.org/abs/1901.00848},
	abstract = {We propose a hybrid quantum-classical approach to model continuous classical probability distributions using a variational quantum circuit. The architecture of the variational circuit consists of two parts: a quantum circuit employed to encode a classical random variable into a quantum state, called the quantum encoder, and a variational circuit whose parameters are optimized to mimic a target probability distribution. Samples are generated by measuring the expectation values of a set of operators chosen at the beginning of the calculation. Our quantum generator can be complemented with a classical function, such as a neural network, as part of the classical post-processing. We demonstrate the application of the quantum variational generator using a generative adversarial learning approach, where the quantum generator is trained via its interaction with a discriminator model that compares the generated samples with those coming from the real data distribution. We show that our quantum generator is able to learn target probability distributions using either a classical neural network or a variational quantum circuit as the discriminator. Our implementation takes advantage of automatic differentiation tools to perform the optimization of the variational circuits employed. The framework presented here for the design and implementation of variational quantum generators can serve as a blueprint for designing hybrid quantum-classical architectures for other machine learning tasks on near-term quantum devices.},
	urldate = {2019-01-07},
	journal = {arXiv:1901.00848 [quant-ph]},
	author = {Romero, Jonathan and Aspuru-Guzik, Alan},
	month = jan,
	year = {2019},
	keywords = {Quantum Physics},
	annote = {arXiv: 1901.00848},
	annote = {Comment: 15 pages, 9 figures},
	file = {arXiv\:1901.00848 PDF:/home/brian/Zotero/storage/GLR93WWA/Romero and Aspuru-Guzik - 2019 - Variational quantum generators Generative adversa.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/DSZYXPHV/1901.html:text/html}
}

@article{zeng_learning_2018,
	title = {Learning and {Inference} on {Generative} {Adversarial} {Quantum} {Circuits}},
	url = {http://arxiv.org/abs/1808.03425},
	abstract = {Quantum mechanics is inherently probabilistic in light of Born's rule. Using quantum circuits as probabilistic generative models for classical data exploits their superior expressibility and efficient direct sampling ability. However, training of quantum circuits can be more challenging compared to classical neural networks due to lack of efficient differentiable learning algorithm. We devise an adversarial quantum-classical hybrid training scheme via coupling a quantum circuit generator and a classical neural network discriminator together. After training, the quantum circuit generative model can infer missing data with quadratic speed up via amplitude amplification. We numerically simulate the learning and inference of generative adversarial quantum circuit using the prototypical Bars-and-Stripes dataset. Generative adversarial quantum circuits is a fresh approach to machine learning which may enjoy the practically useful quantum advantage on near-term quantum devices.},
	urldate = {2019-01-07},
	journal = {arXiv:1808.03425 [quant-ph, stat]},
	author = {Zeng, Jinfeng and Wu, Yufeng and Liu, Jin-Guo and Wang, Lei and Hu, Jiangping},
	month = aug,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1808.03425},
	annote = {Comment: 7 pages, 6 figures},
	file = {arXiv\:1808.03425 PDF:/home/brian/Zotero/storage/3DEMFVFI/Zeng et al. - 2018 - Learning and Inference on Generative Adversarial Q.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/549E6ZCH/1808.html:text/html}
}

@article{nystrom_uber_1930,
	title = {Über {Die} {Praktische} {Auflösung} von {Integralgleichungen} mit {Anwendungen} auf {Randwertaufgaben}},
	volume = {54},
	url = {https://doi.org/10.1007/BF02547521},
	doi = {10.1007/BF02547521},
	journal = {Acta Mathematica},
	author = {Nyström, E. J.},
	year = {1930},
	pages = {185--204}
}

@article{genevay_learning_2017,
	title = {Learning {Generative} {Models} with {Sinkhorn} {Divergences}},
	url = {http://arxiv.org/abs/1706.00292},
	abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
	urldate = {2018-12-11},
	journal = {arXiv:1706.00292 [stat]},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	month = jun,
	year = {2017},
	keywords = {Statistics - Machine Learning},
	annote = {arXiv: 1706.00292},
	file = {arXiv\:1706.00292 PDF:/home/brian/Zotero/storage/B4GQNUAK/Genevay et al. - 2017 - Learning Generative Models with Sinkhorn Divergenc.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/JNZQG2T5/1706.html:text/html}
}

@article{genevay_sample_2018,
	title = {Sample {Complexity} of {Sinkhorn} divergences},
	url = {http://arxiv.org/abs/1810.02733},
	abstract = {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on {\textbackslash}textbackslashemph\{Sinkhorn divergences\} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength \${\textbackslash}textbackslashvarepsilon\$, between OT (\${\textbackslash}textbackslashvarepsilon=0\$) and MMD (\${\textbackslash}textbackslashvarepsilon={\textbackslash}textbackslashinfty\$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively \$O(nˆ3{\textbackslash}textbackslashlog n)\$, \$O(nˆ2)\$ and \$nˆ2\$ operations given a sample size \$n\$), much less is known in terms of their {\textbackslash}textbackslashemph\{sample complexity\}, namely the gap between these quantities, when evaluated using finite samples {\textbackslash}textbackslashemph\{vs.\} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, \$1/nˆ\{1/d\}\$ for OT in dimension \$d\$ and \$1/{\textbackslash}textbackslashsqrt\{n\}\$ for MMD, that for SDs has only been studied empirically. In this paper, we {\textbackslash}textbackslashemph\{(i)\} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer \${\textbackslash}textbackslashvarepsilon\$, {\textbackslash}textbackslashemph\{(ii)\} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and {\textbackslash}textbackslashemph\{(iii)\} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in \$1/{\textbackslash}textbackslashsqrt\{n\}\$ (as in MMD), with a constant that depends however on \${\textbackslash}textbackslashvarepsilon\$, making the bridge between OT and MMD complete.},
	urldate = {2018-12-11},
	journal = {arXiv:1810.02733 [math, stat]},
	author = {Genevay, Aude and Chizat, Lénaic and Bach, Francis and Cuturi, Marco and Peyré, Gabriel},
	month = oct,
	year = {2018},
	keywords = {Mathematics - Statistics Theory},
	annote = {arXiv: 1810.02733},
	file = {arXiv\:1810.02733 PDF:/home/brian/Zotero/storage/HPGLVN54/Genevay et al. - 2018 - Sample Complexity of Sinkhorn divergences.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/43EUGA94/1810.html:text/html}
}

@article{feydy_interpolating_2018,
	title = {Interpolating between {Optimal} {Transport} and {MMD} using {Sinkhorn} {Divergences}},
	url = {https://arxiv.org/abs/1810.08278},
	language = {en},
	urldate = {2018-12-11},
	journal = {arXiv: 1810.08278},
	author = {Feydy, Jean and Séjourné, Thibault and Vialard, François-Xavier and Amari, Shun-ichi and Trouvé, Alain and Peyré, Gabriel},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/home/brian/Zotero/storage/8AXSKHX8/Feydy et al. - 2018 - Interpolating between Optimal Transport and MMD us.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/55LNR6AA/1810.html:text/html}
}

@article{ramdas_wasserstein_2015,
	title = {On {Wasserstein} {Two} {Sample} {Testing} and {Related} {Families} of {Nonparametric} {Tests}},
	url = {http://arxiv.org/abs/1509.02237},
	abstract = {Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a {\textbackslash}textbackslashtextit\{smoothed\} Wasserstein distance, and others through the pursuit of a "distribution-free" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others.},
	urldate = {2018-12-06},
	journal = {arXiv:1509.02237 [math, stat]},
	author = {Ramdas, Aaditya and Garcia, Nicolas and Cuturi, Marco},
	month = sep,
	year = {2015},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	annote = {arXiv: 1509.02237},
	annote = {Comment: 18 pages},
	file = {arXiv\:1509.02237 PDF:/home/brian/Zotero/storage/2CLM2JWC/Ramdas et al. - 2015 - On Wasserstein Two Sample Testing and Related Fami.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/N68Q9I23/1509.html:text/html}
}

@article{cuturi_sinkhorn_2013,
	title = {Sinkhorn {Distances}: {Lightspeed} {Computation} of {Optimal} {Transportation} {Distances}},
	shorttitle = {Sinkhorn {Distances}},
	url = {http://arxiv.org/abs/1306.0895},
	abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
	urldate = {2018-12-06},
	journal = {arXiv:1306.0895 [stat]},
	author = {Cuturi, Marco},
	month = jun,
	year = {2013},
	keywords = {Statistics - Machine Learning},
	annote = {arXiv: 1306.0895},
	file = {arXiv\:1306.0895 PDF:/home/brian/Zotero/storage/W5W3BRHB/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/PHQGAPJG/1306.html:text/html}
}

@article{du_expressive_2018,
	title = {The {Expressive} {Power} of {Parameterized} {Quantum} {Circuits}},
	url = {http://arxiv.org/abs/1810.11922},
	abstract = {Parameterized quantum circuits (PQCs) have been broadly used as a hybrid quantum-classical machine learning scheme to accomplish generative tasks. However, whether PQCs have better expressive power than classical generative neural networks, such as restricted or deep Boltzmann machines, remains an open issue. In this paper, we prove that PQCs with a simple structure already outperform any classical neural network for generative tasks, unless the polynomial hierarchy collapses. Our proof builds on known results from tensor networks and quantum circuits (in particular, instantaneous quantum polynomial circuits). In addition, PQCs equipped with ancillary qubits for post-selection have even stronger expressive power than those without post-selection. We employ them as an application for Bayesian learning, since it is possible to learn prior probabilities rather than assuming they are known. We expect that it will find many more applications in semi-supervised learning where prior distributions are normally assumed to be unknown. Lastly, we conduct several numerical experiments using the Rigetti Forest platform to demonstrate the performance of the proposed Bayesian quantum circuit.},
	urldate = {2018-12-02},
	journal = {arXiv:1810.11922 [quant-ph]},
	author = {Du, Yuxuan and Hsieh, Min-Hsiu and Liu, Tongliang and Tao, Dacheng},
	month = oct,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {arXiv: 1810.11922},
	annote = {Comment: Comments welcomed!},
	file = {arXiv\:1810.11922 PDF:/home/brian/Zotero/storage/NPCNNMCL/Du et al. - 2018 - The Expressive Power of Parameterized Quantum Circ.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/8MK3DKBU/1810.html:text/html}
}

@article{diggle_monte_1984,
	title = {Monte {Carlo} {Methods} of {Inference} for {Implicit} {Statistical} {Models}},
	volume = {46},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2345504},
	abstract = {A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more fundamental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated by an example involving a model for the spatial pattern of displaced amacrine cells in the retina of a rabbit.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Diggle, Peter J. and Gratton, Richard J.},
	year = {1984},
	pages = {193--227}
}

@article{mohamed_learning_2016,
	title = {Learning in {Implicit} {Generative} {Models}},
	url = {http://arxiv.org/abs/1610.03483},
	abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning—to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models–models that only specify a stochastic procedure with which to generate data–and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	urldate = {2018-12-02},
	journal = {arXiv:1610.03483 [cs, stat]},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	annote = {arXiv: 1610.03483},
	file = {arXiv\:1610.03483 PDF:/home/brian/Zotero/storage/578SPZSX/Mohamed and Lakshminarayanan - 2016 - Learning in Implicit Generative Models.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/F4NXDB4E/1610.html:text/html}
}

@article{sriperumbudur_hilbert_2010,
	title = {Hilbert {Space} {Embeddings} and {Metrics} on {Probability} {Measures}},
	volume = {11},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v11/sriperumbudur10a.html},
	number = {Apr},
	urldate = {2018-11-26},
	journal = {Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	year = {2010},
	pages = {1517--1561},
	file = {Full Text PDF:/home/brian/Zotero/storage/Q5FK5ERM/Sriperumbudur et al. - 2010 - Hilbert Space Embeddings and Metrics on Probabilit.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/MX3WQD4T/sriperumbudur10a.html:text/html}
}

@article{sriperumbudur_universality_2011,
	title = {Universality, {Characteristic} {Kernels} and {RKHS} {Embedding} of {Measures}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/sriperumbudur11a.html},
	number = {Jul},
	urldate = {2018-11-26},
	journal = {Journal of Machine Learning Research},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert R. G.},
	year = {2011},
	pages = {2389--2410},
	file = {Full Text PDF:/home/brian/Zotero/storage/Y2DSMXNA/Sriperumbudur et al. - 2011 - Universality, Characteristic Kernels and RKHS Embe.pdf:application/pdf;Snapshot:/home/brian/Zotero/storage/WKN2XJ8R/sriperumbudur11a.html:text/html}
}

@incollection{gretton_kernel_2007,
	title = {A {Kernel} {Method} for the {Two}-{Sample}-{Problem}},
	url = {http://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf},
	urldate = {2018-11-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte and Schölkopf, Bernhard and Smola, Alex J.},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {513--520},
	file = {NIPS Full Text PDF:/home/brian/Zotero/storage/T75V9JA9/Gretton et al. - 2007 - A Kernel Method for the Two-Sample-Problem.pdf:application/pdf;NIPS Snapshot:/home/brian/Zotero/storage/J6HHHHUX/3110-a-kernel-method-for-the-two-sample-problem.html:text/html}
}

@article{andoni_solving_2018,
	title = {On {Solving} {Linear} {Systems} in {Sublinear} {Time}},
	url = {http://arxiv.org/abs/1809.02995},
	abstract = {We study {\textbackslash}textbackslashemph\{sublinear\} algorithms that solve linear systems locally. In the classical version of this problem the input is a matrix \$S{\textbackslash}textbackslashin {\textbackslash}textbackslashmathbb\{R\}ˆ\{n{\textbackslash}textbackslashtimes n\}\$ and a vector \$b{\textbackslash}textbackslashin{\textbackslash}textbackslashmathbb\{R\}ˆn\$ in the range of \$S\$, and the goal is to output \$x{\textbackslash}textbackslashin {\textbackslash}textbackslashmathbb\{R\}ˆn\$ satisfying \$Sx=b\$. For the case when the matrix \$S\$ is symmetric diagonally dominant (SDD), the breakthrough algorithm of Spielman and Teng [STOC 2004] approximately solves this problem in near-linear time (in the input size which is the number of non-zeros in \$S\$), and subsequent papers have further simplified, improved, and generalized the algorithms for this setting. Here we focus on computing one (or a few) coordinates of \$x\$, which potentially allows for sublinear algorithms. Formally, given an index \$u{\textbackslash}textbackslashin [n]\$ together with \$S\$ and \$b\$ as above, the goal is to output an approximation \${\textbackslash}textbackslashhat\{x\}\_u\$ for \$xˆ*\_u\$, where \$xˆ*\$ is a fixed solution to \$Sx=b\$. Our results show that there is a qualitative gap between SDD matrices and the more general class of positive semidefinite (PSD) matrices. For SDD matrices, we develop an algorithm that approximates a single coordinate \$x\_\{u\}\$ in time that is polylogarithmic in \$n\$, provided that \$S\$ is sparse and has a small condition number (e.g., Laplacian of an expander graph). The approximation guarantee is additive \${\textbackslash}textbar {\textbackslash}textbackslashhat\{x\}\_u-xˆ*\_u {\textbackslash}textbar {\textbackslash}textbackslashle {\textbackslash}textbackslashepsilon {\textbackslash}textbackslash{\textbackslash}textbar xˆ* {\textbackslash}textbackslash{\textbackslash}textbar\_{\textbackslash}textbackslashinfty\$ for accuracy parameter \${\textbackslash}textbackslashepsilon{\textbackslash}textgreater0\$. We further prove that the condition-number assumption is necessary and tight. In contrast to the SDD matrices, we prove that for certain PSD matrices \$S\$, the running time must be at least polynomial in \$n\$. This holds even when one wants to obtain the same additive approximation, and \$S\$ has bounded sparsity and condition number.},
	urldate = {2018-11-26},
	journal = {arXiv:1809.02995 [cs]},
	author = {Andoni, Alexandr and Krauthgamer, Robert and Pogrow, Yosef},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Data Structures and Algorithms},
	annote = {arXiv: 1809.02995},
	file = {arXiv\:1809.02995 PDF:/home/brian/Zotero/storage/6M62HHZU/Andoni et al. - 2018 - On Solving Linear Systems in Sublinear Time.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/4JIEFUI5/1809.html:text/html}
}

@article{chia_quantum-inspired_2018,
	title = {Quantum-inspired sublinear classical algorithms for solving low-rank linear systems},
	url = {http://arxiv.org/abs/1811.04852},
	abstract = {We present classical sublinear-time algorithms for solving low-rank linear systems of equations. Our algorithms are inspired by the HHL quantum algorithm for solving linear systems and the recent breakthrough by Tang of dequantizing the quantum algorithm for recommendation systems. Let \$A {\textbackslash}textbackslashin {\textbackslash}textbackslashmathbb\{C\}ˆ\{m {\textbackslash}textbackslashtimes n\}\$ be a rank-\$k\$ matrix, and \$b {\textbackslash}textbackslashin {\textbackslash}textbackslashmathbb\{C\}ˆm\$ be a vector. We present two algorithms: a "sampling" algorithm that provides a sample from \$Aˆ\{-1\}b\$ and a "query" algorithm that outputs an estimate of an entry of \$Aˆ\{-1\}b\$, where \$Aˆ\{-1\}\$ denotes the Moore-Penrose pseudo-inverse. Both of our algorithms have query and time complexity \$O({\textbackslash}textbackslashmathrm\{poly\}(k, {\textbackslash}textbackslashkappa, {\textbackslash}textbackslash{\textbackslash}textbarA{\textbackslash}textbackslash{\textbackslash}textbar\_F, 1/{\textbackslash}textbackslashepsilon){\textbackslash}textbackslash,{\textbackslash}textbackslashmathrm\{polylog\}(m, n))\$, where \${\textbackslash}textbackslashkappa\$ is the condition number of \$A\$ and \${\textbackslash}textbackslashepsilon\$ is the precision parameter. Note that the algorithms we consider are sublinear time, so they cannot write and read the whole matrix or vectors. In this paper, we assume that \$A\$ and \$b\$ come with well-known low-overhead data structures such that entries of \$A\$ and \$b\$ can be sampled according to some natural probability distributions. Alternatively, when \$A\$ is positive semidefinite, our algorithms can be adapted so that the sampling assumption on \$b\$ is not required.},
	urldate = {2018-11-26},
	journal = {arXiv:1811.04852 [quant-ph]},
	author = {Chia, Nai-Hui and Lin, Han-Hsuan and Wang, Chunhao},
	month = nov,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval},
	annote = {arXiv: 1811.04852},
	file = {arXiv\:1811.04852 PDF:/home/brian/Zotero/storage/39VCAM7D/Chia et al. - 2018 - Quantum-inspired sublinear classical algorithms fo.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/MWB6A74T/1811.html:text/html}
}

@article{gilyen_quantum-inspired_2018,
	title = {Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension},
	url = {http://arxiv.org/abs/1811.04909},
	abstract = {We construct an efficient classical analogue of the quantum matrix inversion algorithm (HHL) for low-rank matrices. Inspired by recent work of Tang, assuming length-square sampling access to input data, we implement the pseudoinverse of a low-rank matrix and sample from the solution to the problem \$Ax=b\$ using fast sampling techniques. We implement the pseudo-inverse by finding an approximate singular value decomposition of \$A\$ via subsampling, then inverting the singular values. In principle, the approach can also be used to apply any desired "smooth" function to the singular values. Since many quantum algorithms can be expressed as a singular value transformation problem, our result suggests that more low-rank quantum algorithms can be effectively "dequantised" into classical length-square sampling algorithms.},
	urldate = {2018-11-26},
	journal = {arXiv:1811.04909 [quant-ph]},
	author = {Gilyén, András and Lloyd, Seth and Tang, Ewin},
	month = nov,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Data Structures and Algorithms},
	annote = {arXiv: 1811.04909},
	annote = {Comment: 10 pages},
	file = {arXiv\:1811.04909 PDF:/home/brian/Zotero/storage/8UZFVL6A/Gilyén et al. - 2018 - Quantum-inspired low-rank stochastic regression wi.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/K2FDSIF2/1811.html:text/html}
}

@article{tang_quantum-inspired_2018,
	title = {Quantum-inspired classical algorithms for principal component analysis and supervised clustering},
	url = {http://arxiv.org/abs/1811.00414},
	abstract = {We describe classical analogues to quantum algorithms for principal component analysis and nearest-centroid clustering. Given sampling assumptions, our classical algorithms run in time polylogarithmic in input, matching the runtime of the quantum algorithms with only polynomial slowdown. These algorithms are evidence that their corresponding problems do not yield exponential quantum speedups. To build our classical algorithms, we use the same techniques as applied in our previous work dequantizing a quantum recommendation systems algorithm. Thus, we provide further evidence for the strength of classical \${\textbackslash}textbackslashellˆ2\$-norm sampling assumptions when replacing quantum state preparation assumptions, in the machine learning domain.},
	urldate = {2018-11-26},
	journal = {arXiv:1811.00414 [quant-ph]},
	author = {Tang, Ewin},
	month = oct,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval},
	annote = {arXiv: 1811.00414},
	annote = {Comment: 5 pages},
	file = {arXiv\:1811.00414 PDF:/home/brian/Zotero/storage/ETGG2EAL/Tang - 2018 - Quantum-inspired classical algorithms for principa.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/A54397LJ/1811.html:text/html}
}

@inproceedings{liu_kernelized_2016,
	address = {New York, NY, USA},
	series = {{ICML}'16},
	title = {A {Kernelized} {Stein} {Discrepancy} for {Goodness}-of-fit {Tests}},
	url = {http://dl.acm.org/citation.cfm?id=3045390.3045421},
	abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
	urldate = {2018-11-21},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 48},
	publisher = {JMLR.org},
	author = {Liu, Qiang and Lee, Jason D. and Jordan, Michael},
	year = {2016},
	pages = {276--284}
}

@incollection{gorham_measuring_2015,
	title = {Measuring {Sample} {Quality} with {Stein}{\textbackslash}textbackslashtextquotesingle s {Method}},
	url = {http://papers.nips.cc/paper/5768-measuring-sample-quality-with-steins-method.pdf},
	urldate = {2018-11-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Gorham, Jackson and Mackey, Lester},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {226--234},
	file = {NIPS Snapshot:/home/brian/Zotero/storage/CAGV5DMQ/5768-measuring-sample-quality-with-steins-method..html:text/html}
}

@inproceedings{sriperumbudur_injective_2008,
	title = {Injective {Hilbert} {Space} {Embeddings} of {Probability} {Measures}},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). The embedding function has been proven to be injective when the reproducing kernel is universal. In this case, the embedding induces a metric on the space of probability distributions defined on compact metric spaces. In the present work, we consider more broadly the problem of specifying characteristic kernels, defined as kernels for which the RKHS embedding of probability measures is injective. In particular, characteristic kernels can include non-universal kernels. We restrict ourselves to translation-invariant kernels on Euclidean space, and define the associated metric on probability measures in terms of the Fourier spectrum of the kernel and characteristic functions of these measures. The support of the kernel spectrum is important in finding whether a kernel is characteristic: in particular, the embedding is injective if and only if the kernel spectrum has the entire domain as its support. Characteristic kernels may nonetheless have difficulty in distinguishing certain distributions on the basis of finite samples, again due to the interaction of the kernel spectrum and the characteristic functions of the measures.},
	booktitle = {{COLT}},
	author = {Sriperumbudur, Bharath K. and Gretton, Arthur and Fukumizu, Kenji and Lanckriet, Gert R. G. and Schölkopf, Bernhard},
	year = {2008},
	keywords = {Hilbert space, Kernel (operating system), Dimensionality reduction},
	file = {Full Text PDF:/home/brian/Zotero/storage/69IGZZEJ/Sriperumbudur et al. - 2008 - Injective Hilbert Space Embeddings of Probability .pdf:application/pdf}
}

@article{situ_quantum_2018,
	title = {Quantum generative adversarial network for generating discrete data},
	url = {http://arxiv.org/abs/1807.01235},
	abstract = {Generative adversarial network (GAN) is an effective machine learning framework to train unsupervised generative models, and has drawn lots of attention in recent years. In the GAN framework, the generator is trained by an adversarial discriminator, in order to generate new samples that follows the probability distribution of a given training dataset. Classical GANs cannot generate discrete data due to the requirement of differentiability on the design of generators. Distributions of measurement outcomes of quantum circuits are continuous and differentiable, so quantum GANs can generate discrete data and complement classical GANs. In this paper, we present a quantum version of GAN for generation of discrete data, where parameterized quantum circuits are trained by a classical discriminator. Two families of quantum circuits, both composed of simple one-qubit rotation and two-qubit controlled-phase gates, are considered. The results of a small-scale proof-of-principle numerical experiment demonstrate that quantum circuits can be effectively trained in an adversarial way for generative tasks.},
	urldate = {2018-10-25},
	journal = {arXiv:1807.01235 [quant-ph]},
	author = {Situ, Haozhen and He, Zhimin and Li, Lvzhou and Zheng, Shenggen},
	month = jul,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1807.01235},
	file = {arXiv\:1807.01235 PDF:/home/brian/Zotero/storage/A2DXJMVL/Situ et al. - 2018 - Quantum generative adversarial network for generat.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/PSXSR45T/1807.html:text/html}
}

@article{shi_spectral_2018,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1806.02925},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\textbackslash}textbackslash"om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\textbackslash}textbackslash"om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	urldate = {2018-09-17},
	journal = {arXiv:1806.02925 [cs, stat]},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1806.02925},
	annote = {Comment: To appear at ICML 2018},
	file = {arXiv\:1806.02925 PDF:/home/brian/Zotero/storage/4TVK5HSX/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/WKS2R2H2/1806.html:text/html}
}

@incollection{liu_stein_2016,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	url = {http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {2378--2386}
}

@inproceedings{li_gradient_2018,
	title = {Gradient {Estimators} for {Implicit} {Models}},
	url = {https://openreview.net/forum?id=SJi9WOeRb},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Yingzhen and Turner, Richard E.},
	year = {2018}
}

@inproceedings{yang_goodness--fit_2018,
	address = {Stockholmsmässan, Stockholm Sweden},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Goodness-of-{Fit} {Testing} for {Discrete} {Distributions} via {Stein} {Discrepancy}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/yang18c.html},
	abstract = {Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Jiasen and Liu, Qiang and Rao, Vinayak and Neville, Jennifer},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {5561--5570}
}

@article{killoran_strawberry_2018,
	title = {Strawberry {Fields}: {A} {Software} {Platform} for {Photonic} {Quantum} {Computing}},
	shorttitle = {Strawberry {Fields}},
	url = {http://arxiv.org/abs/1804.03159},
	abstract = {We introduce Strawberry Fields, an open-source quantum programming architecture for light-based quantum computers. Built in Python, Strawberry Fields is a full-stack library for design, simulation, optimization, and quantum machine learning of continuous-variable circuits. The platform consists of three main components: (i) an API for quantum programming based on an easy-to-use language named Blackbird; (ii) a suite of three virtual quantum computer backends, built in NumPy and Tensorflow, each targeting specialized uses; and (iii) an engine which can compile Blackbird programs on various backends, including the three built-in simulators, and – in the near future – photonic quantum information processors. The library also contains examples of several paradigmatic algorithms, including teleportation, (Gaussian) boson sampling, instantaneous quantum polynomial, Hamiltonian simulation, and variational quantum circuit optimization.},
	urldate = {2018-08-13},
	journal = {arXiv:1804.03159 [physics, physics:quant-ph]},
	author = {Killoran, Nathan and Izaac, Josh and Quesada, Nicolás and Bergholm, Ville and Amy, Matthew and Weedbrook, Christian},
	month = apr,
	year = {2018},
	keywords = {Quantum Physics, Physics - Computational Physics},
	annote = {arXiv: 1804.03159},
	annote = {Comment: Try the Strawberry Fields Interactive website, located at http://strawberryfields.ai . Source code available at https://github.com/XanaduAI/strawberryfields},
	file = {arXiv\:1804.03159 PDF:/home/brian/Zotero/storage/URW6ILB9/Killoran et al. - 2018 - Strawberry Fields A Software Platform for Photoni.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/I6T3R6CY/1804.html:text/html}
}

@book{noauthor_software_nodate,
	title = {Software},
	url = {https://www.xanadu.ai/software/},
	abstract = {Open-source software for photonic quantum computing. A full-stack Python library for designing, simulating, and optimizing quantum optical circuits. Includes a suite of three simulators for execution on CPU or GPU.},
	language = {en},
	urldate = {2018-08-13},
	file = {Snapshot:/home/brian/Zotero/storage/SSZZXPCB/software.html:text/html}
}

@article{newell_perceptrons._1969,
	title = {Perceptrons. {An} {Introduction} to {Computational} {Geometry}. {Marvin} {Minsky} and {Seymour} {Papert}. {M}.{I}.{T}. {Press}, {Cambridge}, {Mass}., 1969. vi + 258 pp., illus. {Cloth}, \$12; paper, \$4.95},
	volume = {165},
	issn = {0036-8075},
	url = {http://science.sciencemag.org/content/165/3895/780},
	doi = {10.1126/science.165.3895.780},
	number = {3895},
	journal = {Science},
	author = {Newell, Allen},
	year = {1969},
	pages = {780--782}
}

@article{boykin_universal_1999,
	title = {On {Universal} and {Fault}-{Tolerant} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/quant-ph/9906054},
	abstract = {A novel universal and fault-tolerant basis (set of gates) for quantum computation is described. Such a set is necessary to perform quantum computation in a realistic noisy environment. The new basis consists of two single-qubit gates (Hadamard and \$\{{\textbackslash}textbackslashsigma\_z\}ˆ\{1/4\}\$), and one double-qubit gate (Controlled-NOT). Since the set consisting of Controlled-NOT and Hadamard gates is not universal, the new basis achieves universality by including only one additional elementary (in the sense that it does not include angles that are irrational multiples of \${\textbackslash}textbackslashpi\$) single-qubit gate, and hence, is potentially the simplest universal basis that one can construct. We also provide an alternative proof of universality for the only other known class of universal and fault-tolerant basis proposed by Shor and by Kitaev.},
	urldate = {2018-07-27},
	journal = {arXiv:quant-ph/9906054},
	author = {Boykin, P. Oscar and Mor, Tal and Pulver, Matthew and Roychowdhury, Vwani and Vatan, Farrokh},
	month = jun,
	year = {1999},
	keywords = {Quantum Physics},
	annote = {arXiv: quant-ph/9906054},
	annote = {Comment: 10 pages, Latex. Emails addresses \{boykin, talmo, pulver, vwani, vatan\}@ee.ucla.edu},
	file = {arXiv\:quant-ph/9906054 PDF:/home/brian/Zotero/storage/ASHTEBML/Boykin et al. - 1999 - On Universal and Fault-Tolerant Quantum Computing.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/M79IYN3G/9906054.html:text/html}
}

@article{sopka_introductory_1979,
	title = {Introductory {Functional} {Analysis} with {Applications} ({Erwin} {Kreyszig})},
	volume = {21},
	url = {https://doi.org/10.1137/1021075},
	doi = {10.1137/1021075},
	number = {3},
	journal = {SIAM Review},
	author = {Sopka, J.},
	year = {1979},
	pages = {412--413}
}

@article{trotter_product_1959,
	title = {On the {Product} of {Semi}-{Groups} of {Operators}},
	volume = {10},
	issn = {00029939, 10886826},
	url = {http://www.jstor.org/stable/2033649},
	number = {4},
	journal = {Proceedings of the American Mathematical Society},
	author = {Trotter, H. F.},
	year = {1959},
	pages = {545--551}
}

@article{suzuki_fractal_1990,
	title = {Fractal decomposition of exponential operators with applications to many-body theories and {Monte} {Carlo} simulations},
	volume = {146},
	issn = {0375-9601},
	url = {http://www.sciencedirect.com/science/article/pii/037596019090962N},
	doi = {https://doi.org/10.1016/0375-9601(90)90962-N},
	abstract = {A new systematic scheme of decomposition of exponential operators is presented, namely exp [x(A+B)]=Sm(x)+O(xm+1) for any positive integer m, where Sm(x)=et1Aet2Bet3Aet4B…etMA. A general scheme of construction of tj is given explicitly. The decomposition exp[x(A+B)]=[Sm(x/n)]n+O(xm+1/nm) yields a new efficient approach to quantum Monte Carlo simulations.},
	number = {6},
	journal = {Physics Letters A},
	author = {Suzuki, Masuo},
	year = {1990},
	pages = {319 -- 323}
}

@article{bremner_achieving_2017,
	title = {Achieving quantum supremacy with sparse and noisy commuting quantum computations},
	volume = {1},
	issn = {2521-327X},
	url = {https://doi.org/10.22331/q-2017-04-25-8},
	doi = {10.22331/q-2017-04-25-8},
	journal = {Quantum},
	author = {Bremner, Michael J. and Montanaro, Ashley and Shepherd, Dan J.},
	month = apr,
	year = {2017},
	pages = {8}
}

@article{kerenidis_quantum_2016,
	title = {Quantum {Recommendation} {Systems}},
	url = {http://arxiv.org/abs/1603.08675},
	abstract = {A recommendation system uses the past purchases or ratings of \$n\$ products by a group of \$m\$ users, in order to provide personalized recommendations to individual users. The information is modeled as an \$m {\textbackslash}textbackslashtimes n\$ preference matrix which is assumed to have a good rank-\$k\$ approximation, for a small constant \$k\$. In this work, we present a quantum algorithm for recommendation systems that has running time \$O({\textbackslash}textbackslashtext\{poly\}(k){\textbackslash}textbackslashtext\{polylog\}(mn))\$. All known classical algorithms for recommendation systems that work through reconstructing an approximation of the preference matrix run in time polynomial in the matrix dimension. Our algorithm provides good recommendations by sampling efficiently from an approximation of the preference matrix, without reconstructing the entire matrix. For this, we design an efficient quantum procedure to project a given vector onto the row space of a given matrix. This is the first algorithm for recommendation systems that runs in time polylogarithmic in the dimensions of the matrix and provides an example of a quantum machine learning algorithm for a real world application.},
	urldate = {2018-07-23},
	journal = {arXiv:1603.08675 [quant-ph]},
	author = {Kerenidis, Iordanis and Prakash, Anupam},
	month = mar,
	year = {2016},
	keywords = {Quantum Physics, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval},
	annote = {arXiv: 1603.08675},
	annote = {Comment: 22 pages},
	file = {arXiv\:1603.08675 PDF:/home/brian/Zotero/storage/I7SGJBT5/Kerenidis and Prakash - 2016 - Quantum Recommendation Systems.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/36B6DZTX/1603.html:text/html}
}

@article{tang_quantum-inspired_2018-1,
	title = {A quantum-inspired classical algorithm for recommendation systems},
	url = {http://arxiv.org/abs/1807.04271},
	abstract = {A recommendation system suggests products to users based on data about user preferences. It is typically modeled by a problem of completing an \$m{\textbackslash}textbackslashtimes n\$ matrix of small rank \$k\$. We give the first classical algorithm to produce a recommendation in \$O({\textbackslash}textbackslashtext\{poly\}(k){\textbackslash}textbackslashtext\{polylog\}(m,n))\$ time, which is an exponential improvement on previous algorithms that run in time linear in \$m\$ and \$n\$. Our strategy is inspired by a quantum algorithm by Kerenidis and Prakash: like the quantum algorithm, instead of reconstructing a user's full list of preferences, we only seek a randomized sample from the user's preferences. Our main result is an algorithm that samples high-weight entries from a low-rank approximation of the input matrix in time independent of \$m\$ and \$n\$, given natural sampling assumptions on that input matrix. As a consequence, we show that Kerenidis and Prakash's quantum machine learning (QML) algorithm, one of the strongest candidates for provably exponential speedups in QML, does not in fact give an exponential speedup over classical algorithms.},
	urldate = {2018-07-23},
	journal = {arXiv:1807.04271 [quant-ph]},
	author = {Tang, Ewin},
	month = jul,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval},
	annote = {arXiv: 1807.04271},
	annote = {Comment: 35 pages},
	file = {arXiv\:1807.04271 PDF:/home/brian/Zotero/storage/8GTTYKHB/Tang - 2018 - A quantum-inspired classical algorithm for recomme.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/MJNXCIL9/1807.html:text/html}
}

@article{han_threshold_1997,
	title = {Threshold {Computation} and {Cryptographic} {Security}},
	volume = {26},
	url = {https://doi.org/10.1137/S0097539792240467},
	doi = {10.1137/S0097539792240467},
	number = {1},
	journal = {SIAM Journal on Computing},
	author = {Han, Y. and Hemaspaandra, L. and Thierauf, T.},
	year = {1997},
	pages = {59--78}
}

@article{aaronson_quantum_2005,
	title = {Quantum computing, postselection, and probabilistic polynomial-time},
	volume = {461},
	url = {http://rspa.royalsocietypublishing.org/content/461/2063/3473.abstract},
	doi = {10.1098/rspa.2005.1546},
	abstract = {I study the class of problems efficiently solvable by a quantum computer, given the ability to ‘postselect’ on the outcomes of measurements. I prove that this class coincides with a classical complexity class called PP, or probabilistic polynomial-time. Using this result, I show that several simple changes to the axioms of quantum mechanics would let us solve PP-complete problems efficiently. The result also implies, as an easy corollary, a celebrated theorem of Beigel, Reingold and Spielman that PP is closed under intersection, as well as a generalization of that theorem due to Fortnow and Reingold. This illustrates that quantum computing can yield new and simpler proofs of major results about classical computation.},
	number = {2063},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
	author = {Aaronson, Scott},
	month = nov,
	year = {2005},
	pages = {3473}
}

@article{toda_pp_1991,
	title = {{PP} is as {Hard} as the {Polynomial}-{Time} {Hierarchy}},
	volume = {20},
	url = {https://doi.org/10.1137/0220053},
	doi = {10.1137/0220053},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Toda, S.},
	year = {1991},
	pages = {865--877}
}

@article{stockmeyer_polynomial-time_1976,
	title = {The polynomial-time hierarchy},
	volume = {3},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/030439757690061X},
	doi = {https://doi.org/10.1016/0304-3975(76)90061-X},
	abstract = {The polynomial-time hierarchy is that subrecursive analog of the Kleene arithmetical hierarchy in which deterministic (nondeterministic) polynomial time plays the role of recursive (recursively enumerable) time. Known properties of the polynomial-time hierarchy are summarized. A word problem which is complete in the second stage of the hierarchy is exhibited. In the analogy between the polynomial-time hierarchy and the arithmetical hierarchy, the first order theory of equality plays the role of elementary arithmetic (as the ω-jump of the hierarchy). The problem of deciding validity in the theory of equality is shown to be complete in polynomial-space, and close upper and lower bounds on the space complexity of this problem are established.},
	number = {1},
	journal = {Theoretical Computer Science},
	author = {Stockmeyer, Larry J.},
	year = {1976},
	pages = {1 -- 22}
}

@book{noauthor_pyquil:_2018,
	title = {pyquil: {A} {Python} library for quantum programming using {Quil}},
	copyright = {Apache-2.0},
	shorttitle = {pyquil},
	url = {https://github.com/rigetticomputing/pyquil},
	urldate = {2018-07-17},
	publisher = {Rigetti Computing},
	month = jul,
	year = {2018},
	keywords = {forest, quantum, quantum-computing, quil, rigetti-forest},
	annote = {original-date: 2017-01-09T21:30:22Z}
}

@article{deutsch_quantum_1985,
	title = {Quantum theory, the {Church}–{Turing} principle and the universal quantum computer},
	volume = {400},
	url = {http://rspa.royalsocietypublishing.org/content/400/1818/97.abstract},
	doi = {10.1098/rspa.1985.0070},
	abstract = {It is argued that underlying the Church–Turing hypothesis there is an implicit physical assertion. Here, this assertion is presented explicitly as a physical principle: ‘every finitely realizible physical system can be perfectly simulated by a universal model computing machine operating by finite means’. Classical physics and the universal Turing machine, because the former is continuous and the latter discrete, do not obey the principle, at least in the strong form above. A class of model computing machines that is the quantum generalization of the class of Turing machines is described, and it is shown that quantum theory and the 'universal quantum computer’ are compatible with the principle. Computing machines resembling the universal quantum computer could, in principle, be built and would have many remarkable properties not reproducible by any Turing machine. These do not include the computation of non-recursive functions, but they do include ‘quantum parallelism’, a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it. The intuitive explanation of these properties places an intolerable strain on all interpretations of quantum theory other than Everett’s. Some of the numerous connections between the quantum theory of computation and the rest of physics are explored. Quantum complexity theory allows a physically more reasonable definition of the ‘complexity’ or ‘knowledge’ in a physical system than does classical complexity theory.},
	number = {1818},
	journal = {Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences},
	author = {Deutsch, D.},
	month = jul,
	year = {1985},
	pages = {97}
}

@article{farhi_quantum_2000,
	title = {Quantum {Computation} by {Adiabatic} {Evolution}},
	url = {http://arxiv.org/abs/quant-ph/0001106},
	abstract = {We give a quantum algorithm for solving instances of the satisfiability problem, based on adiabatic evolution. The evolution of the quantum state is governed by a time-dependent Hamiltonian that interpolates between an initial Hamiltonian, whose ground state is easy to construct, and a final Hamiltonian, whose ground state encodes the satisfying assignment. To ensure that the system evolves to the desired final ground state, the evolution time must be big enough. The time required depends on the minimum energy difference between the two lowest states of the interpolating Hamiltonian. We are unable to estimate this gap in general. We give some special symmetric cases of the satisfiability problem where the symmetry allows us to estimate the gap and we show that, in these cases, our algorithm runs in polynomial time.},
	urldate = {2018-07-17},
	journal = {arXiv:quant-ph/0001106},
	author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam and Sipser, Michael},
	month = jan,
	year = {2000},
	keywords = {Quantum Physics},
	annote = {arXiv: quant-ph/0001106},
	annote = {Comment: 24 pages, 12 figures, LaTeX, amssymb,amsmath, BoxedEPS packages; email to farhi@mit.edu},
	file = {arXiv\:quant-ph/0001106 PDF:/home/brian/Zotero/storage/Q9QED8LT/Farhi et al. - 2000 - Quantum Computation by Adiabatic Evolution.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/UFLURAII/0001106.html:text/html}
}

@article{fitzsimons_unconditionally_2017,
	title = {Unconditionally verifiable blind quantum computation},
	volume = {96},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.96.012303},
	doi = {10.1103/PhysRevA.96.012303},
	number = {1},
	journal = {Phys. Rev. A},
	author = {Fitzsimons, Joseph F. and Kashefi, Elham},
	month = jul,
	year = {2017},
	pages = {012303}
}

@book{noauthor_qcircuit:_2018,
	title = {qcircuit: {A} quantum circuit drawing application},
	copyright = {GPL-2.0},
	shorttitle = {qcircuit},
	url = {https://github.com/CQuIC/qcircuit},
	urldate = {2018-07-17},
	publisher = {CQuIC},
	month = jul,
	year = {2018},
	annote = {original-date: 2014-06-27T22:20:40Z}
}

@article{bravyi_complexity_2006,
	title = {The {Complexity} of {Stoquastic} {Local} {Hamiltonian} {Problems}},
	url = {http://arxiv.org/abs/quant-ph/0606140},
	abstract = {We study the complexity of the Local Hamiltonian Problem (denoted as LH-MIN) in the special case when a Hamiltonian obeys conditions of the Perron-Frobenius theorem: all off-diagonal matrix elements in the standard basis are real and non-positive. We will call such Hamiltonians, which are common in the natural world, stoquastic. An equivalent characterization of stoquastic Hamiltonians is that they have an entry-wise non-negative Gibbs density matrix for any temperature. We prove that LH-MIN for stoquastic Hamiltonians belongs to the complexity class AM – a probabilistic version of NP with two rounds of communication between the prover and the verifier. We also show that 2-local stoquastic LH-MIN is hard for the class MA. With the additional promise of having a polynomial spectral gap, we show that stoquastic LH-MIN belongs to the class POSTBPP=BPPpath – a generalization of BPP in which a post-selective readout is allowed. This last result also shows that any problem solved by adiabatic quantum computation using stoquastic Hamiltonians lies in PostBPP.},
	urldate = {2018-07-13},
	journal = {arXiv:quant-ph/0606140},
	author = {Bravyi, Sergey and DiVincenzo, David P. and Oliveira, Roberto I. and Terhal, Barbara M.},
	month = jun,
	year = {2006},
	keywords = {Quantum Physics},
	annote = {arXiv: quant-ph/0606140},
	annote = {Comment: 21 pages Latex, 1 figure. v2 contains several small corrections. v3 has more small corrections},
	file = {arXiv\:quant-ph/0606140 PDF:/home/brian/Zotero/storage/Q28DCEEY/Bravyi et al. - 2006 - The Complexity of Stoquastic Local Hamiltonian Pro.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/K6U96Q58/0606140.html:text/html}
}

@article{wang_divergence_2005,
	title = {Divergence estimation of continuous distributions based on data-dependent partitions},
	volume = {51},
	issn = {0018-9448},
	doi = {10.1109/TIT.2005.853314},
	abstract = {We present a universal estimator of the divergence D(P/spl par/Q) for two arbitrary continuous distributions P and Q satisfying certain regularity conditions. This algorithm, which observes independent and identically distributed (i.i.d.) samples from both P and Q, is based on the estimation of the Radon-Nikodym derivative dP/dQ via a data-dependent partition of the observation space. Strong convergence of this estimator is proved with an empirically equivalent segmentation of the space. This basic estimator is further improved by adaptive partitioning schemes and by bias correction. The application of the algorithms to data with memory is also investigated. In the simulations, we compare our estimators with the direct plug-in estimator and estimators based on other partitioning approaches. Experimental results show that our methods achieve the best convergence performance in most of the tested cases.},
	number = {9},
	journal = {IEEE Transactions on Information Theory},
	author = {Wang, Qing and Kulkarni, S. R. and Verdu, S.},
	month = sep,
	year = {2005},
	keywords = {adaptive partitioning schemes, arbitrary continuous distribution, bias correction, Bias correction, Convergence, data-dependent partition, Density measurement, direct plug-in estimator, divergence, Entropy, Extraterrestrial measurements, information measures, information theory, Information theory, Mutual information, Partitioning algorithms, Pattern recognition, probability, Radon-Nikodym derivative, Radon–Nikodym derivative, Random variables, stationary and ergodic data, Testing, universal divergence estimator, universal estimation of information measures},
	pages = {3064--3074}
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.csail.mit.edu/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	urldate = {2018-07-09},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	month = mar,
	year = {2012},
	pages = {723−773},
	file = {Fulltext PDF:/home/brian/Zotero/storage/E39NLLXE/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf:application/pdf}
}

@article{borgwardt_integrating_2006,
	title = {Integrating structured biological data by {Kernel} {Maximum} {Mean} {Discrepancy}},
	volume = {22},
	issn = {1367-4811},
	doi = {10.1093/bioinformatics/btl242},
	abstract = {MOTIVATION: Many problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic. The Maximum Mean Discrepancy (MMD) can take advantage of the kernel trick, which allows us to apply it not only to vectors, but strings, sequences, graphs, and other common structured data types arising in molecular biology. RESULTS: We study the practical feasibility of an MMD-based test on three central data integration tasks: Testing cross-platform comparability of microarray data, cancer diagnosis, and data-content based schema matching for two different protein function classification schemas. In all of these experiments, including high-dimensional ones, MMD is very accurate in finding samples that were generated from the same distribution, and outperforms its best competitors. CONCLUSIONS: We have defined a novel statistical test of whether two samples are from the same distribution, compatible with both multivariate and structured data, that is fast, easy to implement, and works well, as confirmed by our experiments. AVAILABILITY: http://www.dbs.ifi.lmu.de/{\textbackslash}textasciitildeborgward/MMD.},
	language = {eng},
	number = {14},
	journal = {Bioinformatics (Oxford, England)},
	author = {Borgwardt, Karsten M. and Gretton, Arthur and Rasch, Malte J. and Kriegel, Hans-Peter and Schölkopf, Bernhard and Smola, Alex J.},
	month = jul,
	year = {2006},
	pmid = {16873512},
	keywords = {Algorithms, Computational Biology, Computer Simulation, Information Storage and Retrieval, Sample Size, Statistical Distributions, Systems Integration, Biological, Data Interpretation, Databases, Factual, Models, Statistical},
	pages = {e49--57}
}

@inproceedings{kearns_learnability_1994,
	address = {New York, NY, USA},
	series = {{STOC} '94},
	title = {On the {Learnability} of {Discrete} {Distributions}},
	isbn = {0-89791-663-8},
	url = {http://doi.acm.org/10.1145/195058.195155},
	doi = {10.1145/195058.195155},
	booktitle = {Proceedings of the {Twenty}-sixth {Annual} {ACM} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Kearns, Michael and Mansour, Yishay and Ron, Dana and Rubinfeld, Ronitt and Schapire, Robert E. and Sellie, Linda},
	year = {1994},
	pages = {273--282},
	annote = {event-place: Montreal, Quebec, Canada}
}

@article{ron_property_2008,
	title = {Property {Testing}: {A} {Learning} {Theory} {Perspective}},
	volume = {1},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000004},
	doi = {10.1561/2200000004},
	number = {3},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ron, Dana},
	year = {2008},
	pages = {307--402}
}

@book{jeanfeydy_mmd_2018,
	title = {{MMD}, {Hausdorff} and {Sinkhorn} divergences scaled up to 1,000,000 samples.: jeanfeydy/global-divergences},
	copyright = {MIT},
	shorttitle = {{MMD}, {Hausdorff} and {Sinkhorn} divergences scaled up to 1,000,000 samples.},
	url = {https://github.com/jeanfeydy/global-divergences},
	urldate = {2019-02-06},
	author = {{jeanfeydy}},
	month = nov,
	year = {2018},
	annote = {original-date: 2018-08-15T09:29:01Z}
}

@book{briancoyle_implementation_2019,
	title = {Implementation of {Quantum} {Ising} {Born} {Machine}.},
	copyright = {MIT},
	url = {https://github.com/BrianCoyle/IsingBornMachine},
	urldate = {2019-02-06},
	author = {{BrianCoyle}},
	month = feb,
	year = {2019},
	annote = {original-date: 2018-07-10T19:39:22Z}
}

@article{arunachalam_quantum_2019,
	title = {Quantum hardness of learning shallow classical circuits},
	url = {http://arxiv.org/abs/1903.02840},
	abstract = {In this paper we study the quantum learnability of constant-depth classical circuits under the uniform distribution and in the distribution-independent framework of PAC learning. In order to attain our results, we establish connections between quantum learning and quantum-secure cryptosystems. We then achieve the following results. 1) Hardness of learning AC\$ˆ0\$ and TC\$ˆ0\$ under the uniform distribution. Our first result concerns the concept class TC\$ˆ0\$ (resp. AC\$ˆ0\$), the class of constant-depth and polynomial-sized circuits with unbounded fan-in majority gates (resp. AND, OR, NOT gates). We show that if there exists no quantum polynomial-time (resp. sub-exponential time) algorithm to solve the Learning with Errors (LWE) problem, then there exists no polynomial-time quantum learning algorithm for TC\$ˆ0\$ (resp. AC\$ˆ0\$) under the uniform distribution (even with access to quantum membership queries). The main technique in this result uses explicit pseudo-random generators that are believed to be quantum-secure to construct concept classes that are hard to learn quantumly under the uniform distribution. 2) Hardness of learning TC\$ˆ0\_2\$ in the PAC setting. Our second result shows that if there exists no quantum polynomial time algorithm for the LWE problem, then there exists no polynomial time quantum PAC learning algorithm for the class TC\$ˆ0\_2\$, i.e., depth-2 TC\$ˆ0\$ circuits. The main technique in this result is to establish a connection between the quantum security of public-key cryptosystems and the learnability of a concept class that consists of decryption functions of the cryptosystem. This gives a strong conditional negative answer to one of the "Ten Semi-Grand Challenges for Quantum Computing Theory" raised by Aaronson [Aar05], who asked if AC\$ˆ0\$ and TC\$ˆ0\$ can be PAC-learned in quantum polynomial time.},
	urldate = {2019-03-19},
	journal = {arXiv:1903.02840 [quant-ph]},
	author = {Arunachalam, Srinivasan and Grilo, Alex B. and Sundaram, Aarthi},
	month = mar,
	year = {2019},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Computational Complexity},
	annote = {arXiv: 1903.02840},
	annote = {Comment: 44 pages},
	file = {arXiv\:1903.02840 PDF:/home/brian/Zotero/storage/TTHKKPZJ/Arunachalam et al. - 2019 - Quantum hardness of learning shallow classical cir.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/PYYZJZZQ/1903.html:text/html}
}

@article{arunachalam_survey_2017,
	title = {A {Survey} of {Quantum} {Learning} {Theory}},
	url = {http://arxiv.org/abs/1701.06806},
	abstract = {This paper surveys quantum learning theory: the theoretical aspects of machine learning using quantum computers. We describe the main results known for three models of learning: exact learning from membership queries, and Probably Approximately Correct (PAC) and agnostic learning from classical or quantum examples.},
	urldate = {2019-03-19},
	journal = {arXiv:1701.06806 [quant-ph]},
	author = {Arunachalam, Srinivasan and de Wolf, Ronald},
	month = jan,
	year = {2017},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Computational Complexity},
	annote = {arXiv: 1701.06806},
	annote = {Comment: 26 pages LaTeX. v2: many small changes to improve the presentation. This version will appear as Complexity Theory Column in SIGACT News in June 2017. v3: fixed a small ambiguity in the definition of gamma(C) and updated a reference},
	file = {arXiv\:1701.06806 PDF:/home/brian/Zotero/storage/U7IV4EQW/Arunachalam and de Wolf - 2017 - A Survey of Quantum Learning Theory.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/VIU8Y9PJ/1701.html:text/html}
}

@article{grant_initialization_2019,
	title = {An initialization strategy for addressing barren plateaus in parametrized quantum circuits},
	url = {http://arxiv.org/abs/1903.05076},
	abstract = {In this technical note we propose a theoretically motivated and empirically validated initialization strategy which can resolve the barren plateau problem for practical applications. The proposed strategy allows for efficient training of parametrized quantum circuits. The technique involves randomly selecting some of the initial parameter values, then choosing the remaining values so that the final circuit is a sequence of shallow unitary blocks that each evaluates to the identity. Initializing in this way limits the effective depth of the circuits used to calculate the first parameter update so that they cannot be stuck in a barren plateau at the start of training. We show empirically that circuits initialized using this strategy can be trained using a gradient based method.},
	urldate = {2019-03-26},
	journal = {arXiv:1903.05076 [quant-ph]},
	author = {Grant, Edward and Wossnig, Leonard and Ostaszewski, Mateusz and Benedetti, Marcello},
	month = mar,
	year = {2019},
	keywords = {Quantum Physics},
	annote = {arXiv: 1903.05076},
	file = {arXiv\:1903.05076 PDF:/home/brian/Zotero/storage/ARNU8ZF7/Grant et al. - 2019 - An initialization strategy for addressing barren p.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/T5VA6YDR/1903.html:text/html}
}

@book{schuld_supervised_2018,
	series = {Quantum {Science} and {Technology}},
	title = {Supervised {Learning} with {Quantum} {Computers}},
	isbn = {978-3-319-96423-2},
	url = {https://www.springer.com/us/book/9783319964232},
	abstract = {Quantum machine learning investigates how quantum computers can be used for data-driven prediction and decision making. The books summarises and conceptualises ideas of this relatively young discipline for an audience of computer scientists and physicists from a graduate level upwards. It aims at providing a starting point for those new to the field, showcasing a toy example of a quantum machine learning algorithm and providing a detailed introduction of the two parent disciplines. For more advanced readers, the book discusses topics such as data encoding into quantum states, quantum algorithms and routines for inference and optimisation, as well as the construction and analysis of genuine “quantum learning models”. A special focus lies on supervised learning, and applications for near-term quantum devices.},
	language = {en},
	urldate = {2019-03-29},
	publisher = {Springer International Publishing},
	author = {Schuld, Maria and Petruccione, Francesco},
	year = {2018},
	file = {Snapshot:/home/brian/Zotero/storage/YG6TCUDM/9783319964232.html:text/html}
}

@article{havlicek_supervised_2019,
	title = {Supervised learning with quantum-enhanced feature spaces},
	volume = {567},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-019-0980-2},
	doi = {10.1038/s41586-019-0980-2},
	abstract = {Machine learning and quantum computing are two technologies that each have the potential to alter how computation is performed to address previously untenable problems. Kernel methods for machine learning are ubiquitous in pattern recognition, with support vector machines (SVMs) being the best known method for classification problems. However, there are limitations to the successful solution to such classification problems when the feature space becomes large, and the kernel functions become computationally expensive to estimate. A core element in the computational speed-ups enabled by quantum algorithms is the exploitation of an exponentially large quantum state space through controllable entanglement and interference. Here we propose and experimentally implement two quantum algorithms on a superconducting processor. A key component in both methods is the use of the quantum state space as feature space. The use of a quantum-enhanced feature space that is only efficiently accessible on a quantum computer provides a possible path to quantum advantage. The algorithms solve a problem of supervised learning: the construction of a classifier. One method, the quantum variational classifier, uses a variational quantum circuit1,2 to classify the data in a way similar to the method of conventional SVMs. The other method, a quantum kernel estimator, estimates the kernel function on the quantum computer and optimizes a classical SVM. The two methods provide tools for exploring the applications of noisy intermediate-scale quantum computers3 to machine learning.},
	number = {7747},
	journal = {Nature},
	author = {Havlíček, Vojtěch and Córcoles, Antonio D. and Temme, Kristan and Harrow, Aram W. and Kandala, Abhinav and Chow, Jerry M. and Gambetta, Jay M.},
	month = mar,
	year = {2019},
	pages = {209--212}
}

@article{killoran_continuous-variable_2018,
	title = {Continuous-variable quantum neural networks},
	url = {http://arxiv.org/abs/1806.06871},
	abstract = {We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks.},
	urldate = {2019-04-01},
	journal = {arXiv:1806.06871 [quant-ph]},
	author = {Killoran, Nathan and Bromley, Thomas R. and Arrazola, Juan Miguel and Schuld, Maria and Quesada, Nicolás and Lloyd, Seth},
	month = jun,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1806.06871},
	file = {arXiv\:1806.06871 PDF:/home/brian/Zotero/storage/D5X4MCHY/Killoran et al. - 2018 - Continuous-variable quantum neural networks.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/Q4VPPGBP/1806.html:text/html}
}

@article{khatri_quantum-assisted_2018,
	title = {Quantum-assisted quantum compiling},
	url = {http://arxiv.org/abs/1807.00800},
	abstract = {Compiling quantum algorithms for near-term quantum computers (accounting for connectivity and native gate alphabets) is a major challenge that has received significant attention both by industry and academia. Avoiding the exponential overhead of classical simulation of quantum dynamics will allow compilation of larger algorithms, and a strategy for this is to evaluate an algorithm's cost on a quantum computer. To this end, we propose quantum-assisted quantum compiling (QAQC). In QAQC, we use the overlap between a target unitary \$U\$ and a trainable unitary \$V\$ as the cost function to be evaluated on the quantum computer. More precisely, to ensure that QAQC scales well with problem size, our cost function involves not only the global overlap \$\{{\textbackslash}textbackslashrm Tr\} (Vˆ{\textbackslash}textbackslashdagger U)\$ but also the local overlaps with respect to individual qubits. We introduce novel short-depth quantum circuits to quantify the terms in our cost function, and we present both gradient-free and gradient-based approaches to minimizing this function. As a demonstration of QAQC, we compile various one-qubit gates on IBM's and Rigetti's quantum computers into their respective native gate alphabets. Future applications of QAQC include algorithm depth compression, black-box compiling, noise mitigation, and benchmarking.},
	urldate = {2019-04-01},
	journal = {arXiv:1807.00800 [quant-ph]},
	author = {Khatri, Sumeet and LaRose, Ryan and Poremba, Alexander and Cincio, Lukasz and Sornborger, Andrew T. and Coles, Patrick J.},
	month = jul,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1807.00800},
	annote = {Comment: 12 + 7 pages, 9 figures, Improved cost function for better scaling},
	file = {arXiv\:1807.00800 PDF:/home/brian/Zotero/storage/JR5YKBRC/Khatri et al. - 2018 - Quantum-assisted quantum compiling.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/HJEHTAYK/1807.html:text/html}
}

@article{jones_quantum_2018,
	title = {Quantum compilation and circuit optimisation via energy dissipation},
	url = {http://arxiv.org/abs/1811.03147},
	abstract = {We describe a method for automatically recompiling a quantum circuit A into a target circuit B, with the goal that both circuits have the same action on a specific input i.e. A{\textbackslash}textbarin{\textbackslash}textgreater = B{\textbackslash}textbarin{\textbackslash}textgreater. This is of particular relevance to hybrid, NISQ-era algorithms for dynamical simulation or eigensolving. The user initially specifies B as a blank template: a layout of parameterised unitary gates configured to the identity. The compilation then proceeds using quantum hardware to perform an isomorphic energy-minimisation task, and optionally a gate elimination phase to compress the circuit. We use a recently introduced imaginary-time technique derived from McLachlan's variational principle. If the template for B is too shallow for perfect recompilation then the method will result in an approximate solution. As a demonstration we successfully recompile a 7-qubit circuit involving 186 gates of multiple types into an alternative form with a different topology, a far lower two-qubit gate count, and a smaller family of gate types. We note that a classical simulation of the process can be useful to optimise circuits for today's prototypes, and more generally the method may enable `blind' compilation i.e. harnessing a device whose response to control parameters is deterministic but unknown.},
	urldate = {2019-04-01},
	journal = {arXiv:1811.03147 [quant-ph]},
	author = {Jones, Tyson and Benjamin, Simon C.},
	month = nov,
	year = {2018},
	keywords = {Quantum Physics},
	annote = {arXiv: 1811.03147},
	annote = {Comment: 13 pages, 10 figures; fixed table formats, elaborated on applications and Trotter method in supplementary},
	file = {arXiv\:1811.03147 PDF:/home/brian/Zotero/storage/XZMSVCJ8/Jones and Benjamin - 2018 - Quantum compilation and circuit optimisation via e.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/HTHDGMF4/1811.html:text/html}
}

@article{rocchetto_learning_2018,
	title = {Learning hard quantum distributions with variational autoencoders},
	volume = {4},
	issn = {2056-6387},
	url = {https://doi.org/10.1038/s41534-018-0077-z},
	doi = {10.1038/s41534-018-0077-z},
	abstract = {The exact description of many-body quantum systems represents one of the major challenges in modern physics, because it requires an amount of computational resources that scales exponentially with the size of the system. Simulating the evolution of a state, or even storing its description, rapidly becomes intractable for exact classical algorithms. Recently, machine learning techniques, in the form of restricted Boltzmann machines, have been proposed as a way to efficiently represent certain quantum states with applications in state tomography and ground state estimation. Here, we introduce a practically usable deep architecture for representing and sampling from probability distributions of quantum states. Our representation is based on variational auto-encoders, a type of generative model in the form of a neural network. We show that this model is able to learn efficient representations of states that are easy to simulate classically and can compress states that are not classically tractable. Specifically, we consider the learnability of a class of quantum states introduced by Fefferman and Umans. Such states are provably hard to sample for classical computers, but not for quantum ones, under plausible computational complexity assumptions. The good level of compression achieved for hard states suggests these methods can be suitable for characterizing states of the size expected in first generation quantum hardware.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Rocchetto, Andrea and Grant, Edward and Strelchuk, Sergii and Carleo, Giuseppe and Severini, Simone},
	month = jun,
	year = {2018},
	pages = {28}
}

@book{coyle_project_2018,
	title = {Project / {Dissertation} {Submission} {Index}},
	url = {https://project-archive.inf.ed.ac.uk/msc/2018-outstanding.html},
	abstract = {In this work, we propose a generative quantum machine learning algorithm, which we conjecture is not possible to simulate efficiently by any classical means. We call the algorithm the Ising Born Machine as it generates statistics according to the Born rule of Quantum Mechanics and involves several possible quantum circuit classes which all derive from an Ising Hamiltonian. We recall the currently known proofs of the classical hardness to simulate these circuit classes up to multiplicative error, and make some slight modification such that they are compatible and more useful for the Ising Born Machine construction. Further, we invoke the use of kernel methods as part of the algorithm, and incorporate a kernel function which also claims to demonstrate a quantum classical result, thereby strengthening our claim. Finally, we also present numerical simulations using Rigetti’s Forest quantum simulation platform to investigate and test the ability of the model to learn a simple toy data distribution, with two cost functions, the Kullback Leibler Divergence and the Maximum Mean Discrepancy (MMD).},
	urldate = {2019-04-18},
	author = {Coyle, , Brian},
	month = aug,
	year = {2018},
	file = {Project / Dissertation Submission Index:/home/brian/Zotero/storage/65PXEJRP/2018-outstanding.html:text/html}
}

@article{liu_differentiable_2018,
	title = {Differentiable learning of quantum circuit {Born} machines},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.98.062324},
	doi = {10.1103/PhysRevA.98.062324},
	abstract = {Quantum circuit Born machines are generative models which represent the probability distribution of classical dataset as quantum pure states. Computational complexity considerations of the quantum sampling problem suggest that the quantum circuits exhibit stronger expressibility compared to classical neural networks. One can efficiently draw samples from the quantum circuits via projective measurements on qubits. However, similar to the leading implicit generative models in deep learning, such as the generative adversarial networks, the quantum circuits cannot provide the likelihood of the generated samples, which poses a challenge to the training. We devise an efficient gradient-based learning algorithm for the quantum circuit Born machine by minimizing the kerneled maximum mean discrepancy loss. We simulated generative modeling of the Bars-and-Stripes dataset and Gaussian mixture distributions using deep quantum circuits. Our experiments show the importance of circuit depth and gradient-based optimization algorithm. The proposed learning algorithm is runnable on near-term quantum device and can exhibit quantum advantages for generative modeling.},
	number = {6},
	journal = {Phys. Rev. A},
	author = {Liu, Jin-Guo and Wang, Lei},
	month = dec,
	year = {2018},
	pages = {062324}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2019-05-20},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1406.2661},
	file = {arXiv\:1406.2661 PDF:/home/brian/Zotero/storage/BZXLEB8P/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/XVMB9EPZ/1406.html:text/html}
}

@article{maron_automatic_1961,
	title = {Automatic {Indexing}: {An} {Experimental} {Inquiry}},
	volume = {8},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/321075.321084},
	doi = {10.1145/321075.321084},
	number = {3},
	journal = {J. ACM},
	author = {Maron, M. E.},
	month = jul,
	year = {1961},
	pages = {404--417}
}

@article{zhao_quantum-assisted_2019,
	title = {Quantum-assisted {Gaussian} process regression},
	volume = {99},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.99.052331},
	doi = {10.1103/PhysRevA.99.052331},
	number = {5},
	journal = {Phys. Rev. A},
	author = {Zhao, Zhikuan and Fitzsimons, Jack K. and Fitzsimons, Joseph F.},
	month = may,
	year = {2019},
	pages = {052331}
}

@article{dudley_speed_1969,
	title = {The {Speed} of {Mean} {Glivenko}-{Cantelli} {Convergence}},
	volume = {40},
	url = {https://doi.org/10.1214/aoms/1177697802},
	doi = {10.1214/aoms/1177697802},
	number = {1},
	journal = {The Annals of Mathematical Statistics},
	author = {Dudley, R. M.},
	year = {1969},
	pages = {40--50}
}

@article{lloyd_quantum_2018,
	title = {Quantum {Generative} {Adversarial} {Learning}},
	volume = {121},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.040502},
	doi = {10.1103/PhysRevLett.121.040502},
	number = {4},
	journal = {Phys. Rev. Lett.},
	author = {Lloyd, Seth and Weedbrook, Christian},
	month = jul,
	year = {2018},
	pages = {040502}
}

@article{kubler_quantum_2019,
	title = {Quantum {Mean} {Embedding} of {Probability} {Distributions}},
	url = {http://arxiv.org/abs/1905.13526},
	abstract = {The kernel mean embedding of probability distributions is commonly used in machine learning as an injective mapping from distributions to functions in an infinite dimensional Hilbert space. It allows us, for example, to define a distance measure between probability distributions, called maximum mean discrepancy (MMD). In this work, we propose to represent probability distributions in a pure quantum state of a system that is described by an infinite dimensional Hilbert space. This enables us to work with an explicit representation of the mean embedding, whereas classically one can only work implicitly with an infinite dimensional Hilbert space through the use of the kernel trick. We show how this explicit representation can speed up methods that rely on inner products of mean embeddings and discuss the theoretical and experimental challenges that need to be solved in order to achieve these speedups.},
	urldate = {2019-07-30},
	journal = {arXiv:1905.13526 [quant-ph]},
	author = {Kübler, Jonas M. and Muandet, Krikamol and Schölkopf, Bernhard},
	month = may,
	year = {2019},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {arXiv: 1905.13526},
	annote = {Comment: 7 pages, 2 figures},
	file = {arXiv\:1905.13526 PDF:/home/brian/Zotero/storage/FWWTL2RA/Kübler et al. - 2019 - Quantum Mean Embedding of Probability Distribution.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/9G68V2FA/1905.html:text/html}
}

@article{morales_variational_2018,
	title = {Variational learning of {Grover}'s quantum search algorithm},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.98.062333},
	doi = {10.1103/PhysRevA.98.062333},
	number = {6},
	journal = {Phys. Rev. A},
	author = {Morales, Mauro E. S. and Tlyachev, Timur and Biamonte, Jacob},
	month = dec,
	year = {2018},
	pages = {062333}
}

@article{biamonte_universal_2019,
	title = {Universal {Variational} {Quantum} {Computation}},
	url = {http://arxiv.org/abs/1903.04500},
	abstract = {Variational quantum algorithms dominate contemporary gate-based quantum enhanced optimization [1], eigenvalue estimation [2] and machine learning [3]. Here we establish the quantum computational universality of variational quantum computation by developing two constructions which prepare states with high 2-norm overlap with the outputs of quantum circuits. The fleeting resource is the number of expected values which must be iteratively minimized using a classical-to-quantum feedback loop. The first approach is efficient in the number of expected values for \$n\$-qubit circuits containing \${\textbackslash}mathcal\{O\}({\textbackslash}text\{poly\} {\textbackslash}ln n)\$ non-Clifford gates---the number of expected values has no dependence on Clifford gates appearing in the simulated circuit. The second approach adapts the Kitaev-Feynman clock construction yielding \${\textbackslash}sim4{\textbackslash}cdot n + 9{\textbackslash}cdot (T + 1)\$ expected values while introducing not more than \$T\$ slack qubits, for a quantum circuit partitioned into \$T\$ Hermitian blocks (gates). The variational model is hence universal and necessitates (i) state-preparation by a control sequence followed by (ii) measurements in one basis and (iii) gradient-free or gradient-based minimization of a polynomially bounded number of expected values.},
	urldate = {2019-07-31},
	journal = {arXiv:1903.04500 [quant-ph]},
	author = {Biamonte, Jacob},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.04500},
	keywords = {Quantum Physics},
	annote = {Comment: RevTeX, 5 pages, comments welcome},
	file = {arXiv\:1903.04500 PDF:/home/brian/Zotero/storage/WX7JPPMA/Biamonte - 2019 - Universal Variational Quantum Computation.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/YA6WG979/1903.html:text/html}
}

@article{perez-salinas_data_2019,
	title = {Data re-uploading for a universal quantum classifier},
	url = {http://arxiv.org/abs/1907.02085},
	abstract = {A single qubit provides sufficient computational capabilities to construct a universal quantum classifier. This fact may be surprising since a single qubit only offers a simple superposition of two states and single qubit gates only make a rotation in the Bloch sphere. The key ingredient to circumvent these limitations is to allow for multiple data re-uploading. A quantum circuit can then be organized as a series of data re-uploading and single-qubit processing units. Furthermore, both data re-uploading and measurements can accommodate multiple dimensions in the input and multiple categories in the output, so as to conform a universal quantum classifier. The extension of this idea to several qubits enhances the efficiency of the strategy as entanglement expands the superpositions carried along the classification. Extensive benchmarking on different examples of the single- and multi- qubit quantum classifier validates its ability to describe and classify complex data.},
	urldate = {2019-07-31},
	journal = {arXiv:1907.02085 [quant-ph]},
	author = {Pérez-Salinas, Adrián and Cervera-Lierta, Alba and Gil-Fuster, Elies and Latorre, José I.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.02085},
	keywords = {Quantum Physics},
	annote = {Comment: 17 pages, 9 figures},
	file = {arXiv\:1907.02085 PDF:/home/brian/Zotero/storage/SAEXE3W7/Pérez-Salinas et al. - 2019 - Data re-uploading for a universal quantum classifi.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/7AHMYJLN/1907.html:text/html}
}

@article{cincio_learning_2018,
	title = {Learning the quantum algorithm for state overlap},
	volume = {20},
	issn = {1367-2630},
	url = {http://arxiv.org/abs/1803.04114},
	doi = {10.1088/1367-2630/aae94a},
	abstract = {Short-depth algorithms are crucial for reducing computational error on near-term quantum computers, for which decoherence and gate infidelity remain important issues. Here we present a machine-learning approach for discovering such algorithms. We apply our method to a ubiquitous primitive: computing the overlap \$\{{\textbackslash}rm Tr\}({\textbackslash}rho{\textbackslash}sigma)\$ between two quantum states \${\textbackslash}rho\$ and \${\textbackslash}sigma\$. The standard algorithm for this task, known as the Swap Test, is used in many applications such as quantum support vector machines, and, when specialized to \${\textbackslash}rho = {\textbackslash}sigma\$, quantifies the Renyi entanglement. Here, we find algorithms that have shorter depths than the Swap Test, including one that has a constant depth (independent of problem size). Furthermore, we apply our approach to the hardware-specific connectivity and gate sets used by Rigetti's and IBM's quantum computers and demonstrate that the shorter algorithms that we derive significantly reduce the error - compared to the Swap Test - on these computers.},
	number = {11},
	urldate = {2019-07-31},
	journal = {New Journal of Physics},
	author = {Cincio, Lukasz and Subaşı, Yiğit and Sornborger, Andrew T. and Coles, Patrick J.},
	month = nov,
	year = {2018},
	note = {arXiv: 1803.04114},
	keywords = {Quantum Physics},
	pages = {113022},
	annote = {Comment: 12 pages, 11 figures},
	file = {arXiv\:1803.04114 PDF:/home/brian/Zotero/storage/NJSUKKFU/Cincio et al. - 2018 - Learning the quantum algorithm for state overlap.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/AK2BU8CW/1803.html:text/html}
}

@article{cerezo_variational_2019,
	title = {Variational {Quantum} {Fidelity} {Estimation}},
	url = {http://arxiv.org/abs/1906.09253},
	abstract = {Computing quantum state fidelity will be important to verify and characterize states prepared on a quantum computer. In this work, we propose novel lower and upper bounds for the fidelity \$F({\textbackslash}rho,{\textbackslash}sigma)\$ based on the "truncated fidelity" \$F({\textbackslash}rho\_m, {\textbackslash}sigma)\$, which is evaluated for a state \${\textbackslash}rho\_m\$ obtained by projecting \${\textbackslash}rho\$ onto its \$m\$-largest eigenvalues. Our bounds can be refined, i.e., they tighten monotonically with \$m\$. To compute our bounds, we introduce a hybrid quantum-classical algorithm, called Variational Quantum Fidelity Estimation, that involves three steps: (1) variationally diagonalize \${\textbackslash}rho\$, (2) compute matrix elements of \${\textbackslash}sigma\$ in the eigenbasis of \${\textbackslash}rho\$, and (3) combine these matrix elements to compute our bounds. Our algorithm is aimed at the case where \${\textbackslash}sigma\$ is arbitrary and \${\textbackslash}rho\$ is low rank, which we call low-rank fidelity estimation, and we prove that a classical algorithm cannot efficiently solve this problem. Finally, we demonstrate that our bounds can detect quantum phase transitions and are often tighter than previously known computable bounds for realistic situations.},
	urldate = {2019-07-31},
	journal = {arXiv:1906.09253 [quant-ph]},
	author = {Cerezo, M. and Poremba, Alexander and Cincio, Lukasz and Coles, Patrick J.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.09253},
	keywords = {Quantum Physics},
	annote = {Comment: 6 + 8 pages, 4 figures},
	file = {arXiv\:1906.09253 PDF:/home/brian/Zotero/storage/EGJGQEY8/Cerezo et al. - 2019 - Variational Quantum Fidelity Estimation.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/I5DG3HN4/1906.html:text/html}
}

@article{larose_variational_2019,
	title = {Variational {Quantum} {State} {Diagonalization}},
	volume = {5},
	issn = {2056-6387},
	url = {http://arxiv.org/abs/1810.10506},
	doi = {10.1038/s41534-019-0167-6},
	abstract = {Variational hybrid quantum-classical algorithms are promising candidates for near-term implementation on quantum computers. In these algorithms, a quantum computer evaluates the cost of a gate sequence (with speedup over classical cost evaluation), and a classical computer uses this information to adjust the parameters of the gate sequence. Here we present such an algorithm for quantum state diagonalization. State diagonalization has applications in condensed matter physics (e.g., entanglement spectroscopy) as well as in machine learning (e.g., principal component analysis). For a quantum state \${\textbackslash}rho\$ and gate sequence \$U\$, our cost function quantifies how far \$ U{\textbackslash}rho U{\textasciicircum}\{{\textbackslash}dagger\}\$ is from being diagonal. We introduce novel short-depth quantum circuits to quantify our cost. Minimizing this cost returns a gate sequence that approximately diagonalizes \${\textbackslash}rho\$. One can then read out approximations of the largest eigenvalues, and the associated eigenvectors, of \${\textbackslash}rho\$. As a proof-of-principle, we implement our algorithm on Rigetti's quantum computer to diagonalize one-qubit states and on a simulator to find the entanglement spectrum of the Heisenberg model ground state.},
	number = {1},
	urldate = {2019-07-31},
	journal = {npj Quantum Information},
	author = {LaRose, Ryan and Tikku, Arkin and O'Neel-Judy, Étude and Cincio, Lukasz and Coles, Patrick J.},
	month = dec,
	year = {2019},
	note = {arXiv: 1810.10506},
	keywords = {Quantum Physics},
	pages = {57},
	annote = {Comment: 12+9 pages, added larger scale implementations and additional details on optimization methods, ansatz, and cost operational meaning},
	file = {arXiv\:1810.10506 PDF:/home/brian/Zotero/storage/7EWZFYS5/LaRose et al. - 2019 - Variational Quantum State Diagonalization.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/4IXLACF5/1810.html:text/html}
}

@article{romero_quantum_2017,
	title = {Quantum autoencoders for efficient compression of quantum data},
	volume = {2},
	issn = {2058-9565},
	url = {http://arxiv.org/abs/1612.02806},
	doi = {10.1088/2058-9565/aa8072},
	abstract = {Classical autoencoders are neural networks that can learn efficient low dimensional representations of data in higher dimensional space. The task of an autoencoder is, given an input \$x\$, is to map \$x\$ to a lower dimensional point \$y\$ such that \$x\$ can likely be recovered from \$y\$. The structure of the underlying autoencoder network can be chosen to represent the data on a smaller dimension, effectively compressing the input. Inspired by this idea, we introduce the model of a quantum autoencoder to perform similar tasks on quantum data. The quantum autoencoder is trained to compress a particular dataset of quantum states, where a classical compression algorithm cannot be employed. The parameters of the quantum autoencoder are trained using classical optimization algorithms. We show an example of a simple programmable circuit that can be trained as an efficient autoencoder. We apply our model in the context of quantum simulation to compress ground states of the Hubbard model and molecular Hamiltonians.},
	number = {4},
	urldate = {2019-07-31},
	journal = {Quantum Science and Technology},
	author = {Romero, Jonathan and Olson, Jonathan P. and Aspuru-Guzik, Alan},
	month = dec,
	year = {2017},
	note = {arXiv: 1612.02806},
	keywords = {Quantum Physics},
	pages = {045001},
	annote = {Comment: 10 pages, 9 figures},
	file = {arXiv\:1612.02806 PDF:/home/brian/Zotero/storage/UUIVBPTE/Romero et al. - 2017 - Quantum autoencoders for efficient compression of .pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/F3RPTRNX/1612.html:text/html}
}

@article{jasek_experimental_2019,
	title = {Experimental hybrid quantum-classical reinforcement learning by boson sampling: how to train a quantum cloner},
	shorttitle = {Experimental hybrid quantum-classical reinforcement learning by boson sampling},
	url = {http://arxiv.org/abs/1906.05540},
	abstract = {We report on experimental implementation of a machine-learned quantum gate driven by a classical control. The gate learns optimal phase-covariant cloning in a reinforcement learning scenario having fidelity of the clones as reward. In our experiment, the gate learns to achieve nearly optimal cloning fidelity allowed for this particular class of states. This makes it a proof of present-day feasibility and practical applicability of the hybrid machine learning approach combining quantum information processing with classical control. Moreover, our experiment can be directly generalized to larger interferometers where the computational cost of classical computer is much lower than the cost of boson sampling.},
	urldate = {2019-07-31},
	journal = {arXiv:1906.05540 [quant-ph]},
	author = {Jašek, Jan and Jiráková, Kateřina and Bartkiewicz, Karol and Černoch, Antonín and Fürst, Tomáš and Lemr, Karel},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05540},
	keywords = {Quantum Physics},
	annote = {Comment: 7 pages, 6 figures},
	file = {arXiv\:1906.05540 PDF:/home/brian/Zotero/storage/GEKKVBBX/Jašek et al. - 2019 - Experimental hybrid quantum-classical reinforcemen.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/VIQ9N5JM/1906.html:text/html}
}

@article{benedetti_parameterized_2019,
	title = {Parameterized quantum circuits as machine learning models},
	url = {http://arxiv.org/abs/1906.07682},
	abstract = {Hybrid quantum-classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be thought of as machine learning models with remarkable expressive power. This Review presents components of these models and discusses their application to a variety of data-driven tasks such as supervised learning and generative modeling. With experimental demonstrations carried out on actual quantum hardware, and with software actively being developed, this rapidly growing field could become one of the first instances of quantum computing that addresses real world problems.},
	urldate = {2019-07-31},
	journal = {arXiv:1906.07682 [quant-ph]},
	author = {Benedetti, Marcello and Lloyd, Erika and Sack, Stefan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07682},
	keywords = {Quantum Physics, Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 8 figures;},
	file = {arXiv\:1906.07682 PDF:/home/brian/Zotero/storage/K9XEMT7D/Benedetti et al. - 2019 - Parameterized quantum circuits as machine learning.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/YJ6WFYUL/1906.html:text/html}
}

@article{mcclean_theory_2016,
	title = {The theory of variational hybrid quantum-classical algorithms},
	volume = {18},
	issn = {1367-2630},
	url = {https://doi.org/10.1088%2F1367-2630%2F18%2F2%2F023023},
	doi = {10.1088/1367-2630/18/2/023023},
	abstract = {Many quantum algorithms have daunting resource requirements when compared to what is available today. To address this discrepancy, a quantum-classical hybrid optimization scheme known as ‘the quantum variational eigensolver’ was developed (Peruzzo et al 2014 Nat. Commun. 5 4213) with the philosophy that even minimal quantum resources could be made useful when used in conjunction with classical routines. In this work we extend the general theory of this algorithm and suggest algorithmic improvements for practical implementations. Specifically, we develop a variational adiabatic ansatz and explore unitary coupled cluster where we establish a connection from second order unitary coupled cluster to universal gate sets through a relaxation of exponential operator splitting. We introduce the concept of quantum variational error suppression that allows some errors to be suppressed naturally in this algorithm on a pre-threshold quantum device. Additionally, we analyze truncation and correlated sampling in Hamiltonian averaging as ways to reduce the cost of this procedure. Finally, we show how the use of modern derivative free optimization techniques can offer dramatic computational savings of up to three orders of magnitude over previously used optimization techniques.},
	language = {en},
	number = {2},
	urldate = {2019-07-31},
	journal = {New Journal of Physics},
	author = {McClean, Jarrod R. and Romero, Jonathan and Babbush, Ryan and Aspuru-Guzik, Alán},
	month = feb,
	year = {2016},
	pages = {023023},
	file = {IOP Full Text PDF:/home/brian/Zotero/storage/TR9ULV3U/McClean et al. - 2016 - The theory of variational hybrid quantum-classical.pdf:application/pdf}
}

@article{mitarai_quantum_2018,
	title = {Quantum circuit learning},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.98.032309},
	doi = {10.1103/PhysRevA.98.032309},
	number = {3},
	journal = {Phys. Rev. A},
	author = {Mitarai, K. and Negoro, M. and Kitagawa, M. and Fujii, K.},
	month = sep,
	year = {2018},
	pages = {032309}
}

@article{goldreich_property_1998,
	title = {Property {Testing} and {Its} {Connection} to {Learning} and {Approximation}},
	volume = {45},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/285055.285060},
	doi = {10.1145/285055.285060},
	number = {4},
	journal = {J. ACM},
	author = {Goldreich, Oded and Goldwasser, Shari and Ron, Dana},
	month = jul,
	year = {1998},
	keywords = {approximation algorithms, computational learning theory, graph algorithms},
	pages = {653--750}
}

@article{smith_practical_2016,
	title = {A {Practical} {Quantum} {Instruction} {Set} {Architecture}},
	url = {http://arxiv.org/abs/1608.03355},
	abstract = {We introduce an abstract machine architecture for classical/quantum computations---including compilation---along with a quantum instruction language called Quil for explicitly writing these computations. With this formalism, we discuss concrete implementations of the machine and non-trivial algorithms targeting them. The introduction of this machine dovetails with ongoing development of quantum computing technology, and makes possible portable descriptions of recent classical/quantum algorithms.},
	urldate = {2019-08-12},
	journal = {arXiv:1608.03355 [quant-ph]},
	author = {Smith, Robert S. and Curtis, Michael J. and Zeng, William J.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.03355},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Programming Languages, Quantum Physics},
	annote = {Comment: 15 pages, 4 figures},
	file = {arXiv\:1608.03355 PDF:/home/brian/Zotero/storage/KFGQEMCY/Smith et al. - 2016 - A Practical Quantum Instruction Set Architecture.pdf:application/pdf;arXiv.org Snapshot:/home/brian/Zotero/storage/XIYKKN8X/1608.html:text/html}
}